[{"uri":"https://hoangworthy.github.io/AWS-Worklog/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “Discover Agentic AI – Amazon QuickSuite Workshop” Event Objectives Demystify Agentic AI: Define the shift from passive Generative AI to autonomous Agentic AI. Showcase Amazon QuickSuite: Provide the first live demonstration of this solution in Vietnam. Enable Adoption: Lower financial barriers through the AWS LIFT Program ($80,000 USD credit). Facilitate Hands-on Learning: Offer a practical environment to build AI concepts with expert guidance. Speakers Vivien Nguyen – Territory Manager, AWS Tung Cao – Solution Architect, AWS Cloud Kinetics Team – Strategic Implementation Partners Key Highlights The Paradigm Shift: From Generative to Agentic The workshop established a clear distinction between previous AI iterations and the current wave.\nGenerative AI: Focuses on creating content (text, images, code) based on prompts. Agentic AI: Focuses on autonomy and action. These systems can perceive their environment, reason through complex workflows, and execute tasks independently without constant human intervention. Unveiling Amazon QuickSuite This event marked the first live demonstration of Amazon QuickSuite in Vietnam.\nUnified Ecosystem: It seamlessly integrates data visualization (QuickSight) with generative capabilities (Quick Suite Q). Speed and Agility: The \u0026ldquo;Quick\u0026rdquo; branding emphasizes rapid deployment, allowing businesses to move from concept to implementation in a short timeframe. Data-Centricity: The suite is designed to process high volumes of data, which is a prerequisite for intelligent agents to make informed decisions. Strategic Partnership \u0026amp; Support Collaboration with Cloud Kinetics: The event highlighted that while AWS provides the platform, partners like Cloud Kinetics are essential for architectural guidance and \u0026ldquo;last-mile\u0026rdquo; implementation. Dual-Layer Support: Attendees benefited from both platform experts (AWS) and consulting partners, de-risking the technical adoption process. Financial Enablement: AWS LIFT Program $80,000 USD Credit: A significant financial incentive was introduced to support new customers and SMBs. Risk Reduction: This funding allows companies to experiment with high-performance computing and R\u0026amp;D without the immediate burden of infrastructure costs. Key Takeaways Design Mindset Focus on Autonomy: When designing for Agentic AI, the goal is to build systems that act on behalf of the user, not just assist them. Real-World Application: Move beyond \u0026ldquo;hype\u0026rdquo; by identifying specific operational bottlenecks where an autonomous agent can add value (e.g., automated reporting, supply chain adjustments). Technical Architecture The Ecosystem Approach: Effective agents require a connected network of tools. QuickSuite acts as the connective tissue between data sources and action logic. Infrastructure Readiness: Creating an AWS account and setting up the correct environment is the critical first step to accessing these advanced capabilities. Strategic Implementation Information Asymmetry: Early adopters of QuickSuite gain a competitive advantage by utilizing tools that the broader market has not yet mastered. Cost Management: Leveraging programs like LIFT is crucial for accelerating time-to-market while managing cash flow. Applying to Work Explore QuickSuite: Investigate how QuickSight and Quick Suite Q can be integrated into current data analytics workflows to create \u0026ldquo;Analyst Agents.\u0026rdquo; Leverage Financial Incentives: Apply for the AWS LIFT Program to secure credits for upcoming R\u0026amp;D projects. Identify Use Cases: Audit internal business operations to find repetitive, multi-step tasks that are suitable for autonomous execution by Agentic AI. Collaborate with Partners: Engage with solution partners like Cloud Kinetics for complex architectural needs rather than building entirely in-house. Event Experience Attending the “Discover Agentic AI” workshop at the Bitexco Financial Tower was a highly professional and insightful experience. It provided a clear roadmap for the future of enterprise AI. Key experiences included:\nExclusive Access and Innovation Being part of the first-ever live demonstration of Amazon QuickSuite in Vietnam provided a sense of being at the forefront of technology. The venue at the AWS Vietnam Office underscored the commitment of AWS to the local market and provided a high-quality professional environment. High-Value Hands-on Learning The 90-minute workshop session was instrumental. Unlike passive seminars, actually building concepts with Quick Sight + Quick Suite Q helped solidify the theoretical knowledge. Having AWS technical experts available for over-the-shoulder mentorship allowed for immediate troubleshooting and deeper technical discussions. Networking and Ecosystem The extended lunch break and networking sessions offered valuable time to discuss strategies with peers and industry experts. Understanding the role of the Cloud Kinetics partnership helped clarify how to bridge the gap between abstract platform capabilities and specific business solutions. Lessons Learned Agentic AI is the future of operations: The shift from \u0026ldquo;chatting with AI\u0026rdquo; to \u0026ldquo;AI doing the work\u0026rdquo; is inevitable and transformative. Speed is critical: The tools are designed for rapid deployment (\u0026ldquo;QuickSuite\u0026rdquo;), suggesting that agility is the new metric for success. Funding accelerates innovation: The LIFT program changes the conversation from \u0026ldquo;Can we afford this?\u0026rdquo; to \u0026ldquo;How fast can we start?\u0026rdquo; Overall, the workshop successfully demystified the complex concept of Agentic AI and provided the concrete tools, funding, and expertise required to start building immediately. It was a strategic intervention that empowered attendees to transform their business operations.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Dang Minh Hoang\nPhone Number: 0979091813\nEmail: Hoangsusp@gmail.com\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Day Vietnam - AI Edition 2025” Event Name: Vietnam Cloud Day 2025\nDate: September 18, 2025\nLocation: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in the Event: Participant\nEvent Objectives The AWS Cloud Day Vietnam - AI Edition 2025 is positioned as a pivotal gathering for the Vietnamese technology and business communities. The event\u0026rsquo;s primary objective is to catalyze the digital transformation of Vietnam\u0026rsquo;s economy by leveraging the symbiotic power of Cloud Computing and Artificial Intelligence (AI).\nThe overarching goals of the event can be categorized into four strategic pillars:\nDemocratizing Generative AI for Enterprise: Moving Generative AI (GenAI) beyond hype to practical implementation. The goal is to demonstrate how businesses can transform generic AI solutions into sophisticated, context-aware programs, emphasizing a \u0026ldquo;comprehensive data strategy\u0026rdquo; as the key differentiator. Dissolving Business-IT Boundaries: Particularly within the Financial Services sector, bridging the gap between operational business goals and IT. The objective is to showcase cloud technology as a driver of business value, enabling \u0026ldquo;Ecosystem Banking\u0026rdquo; and \u0026ldquo;Embedded Finance\u0026rdquo; models. Accelerating Industry-Specific Modernization: Providing tailored roadmaps for diverse industries (Retail, Energy, Telco, Public Sector). Highlighting success stories like Xanh SM, Honda Vietnam, and Masterise Group to prove that modernization requires specific pathways, whether through migration or building cloud-native applications. Fortifying Security and Resilience: Instilling a \u0026ldquo;security by design\u0026rdquo; mindset. The aim is to teach attendees how to integrate security best practices throughout the entire application lifecycle—from development to production. Speakers The event features a comprehensive roster of 24 speakers, ranging from high-ranking government officials to C-level executives and technical experts.\nSpeaker Name Title Organization H.E. Pham Duc Long Deputy Minister of Science and Technology Ministry of Science and Technology H.E. Marc E. Knapper U.S. Ambassador to Vietnam U.S. Embassy in Vietnam Jaime Valles VP, General Manager Asia Pacific \u0026amp; Japan AWS Jeff Johnson Managing Director ASEAN AWS Dr. Jens Lottner Chief Executive Officer Techcombank Dieter Botha CEO TymeX Trang Phung CEO U2U Network Vu Van Co-founder \u0026amp; CEO ELSA Corp Nguyen Hoa Binh Chairman Nexttech Group Gia Hieu Dinh Chief Information Officer F88 Nguyen Hong Phuong Huy Head of Cloud Infrastructure \u0026amp; Cyber Security Masterise Group Nguyen Vu Hoang Head of Technologies VTV Digital Ha Anh Van Head: IT Solutions Department Honda Vietnam Nguyen Tuan Huy Director of Digital Transformation Mobifone Minh Hoang Chief Data Officer Techcom Securities Vincent Nguyen Managing Director Nam Long Commercial Property Seunghoon Chae General Director MegazoneCloud Vietnam Uy Tran Co-Founder \u0026amp; COO Katalon Thai Huy Chuong Head of Application Development Bao Viet Holdings Tran Dinh Khiem Digital Bank Director Techcombank Christopher Bennett Chief Technology Officer TymeX Selma Belhadjamor Principal Data Scientist Onebyzero Ngo Manh Ha Co-CEO, CTO TechX Corp Nguyen Thanh Binh Head of DevOps Renova Cloud Key Highlights The Strategic Convergence: Policy and Leadership Government Endorsement: Opening addresses by H.E. Pham Duc Long and H.E. Marc E. Knapper signal strong bilateral support for Vietnam\u0026rsquo;s digital infrastructure. Leadership Panels: Moderated by Jeff Johnson, leaders like Vu Van (ELSA Corp) and Nguyen Hoa Binh (Nexttech Group) discussed how \u0026ldquo;People and Culture\u0026rdquo; drive cloud adoption. Track 1: Financial Services (FSI) – The New Banking Paradigm Innovation in Banking \u0026amp; Insurance: Techcombank and Bao Viet Holdings discussed the shift toward \u0026ldquo;Ecosystem Banking.\u0026rdquo; XGenAI Implementation: Ngo Manh Ha (TechX) presented XGenAI, demonstrating how local partners build on AWS to deliver superior customer experiences in the financial sector. Track 2: Cross-Industry Modernization Migration Success Stories: Ha Anh Van shared Honda Vietnam\u0026rsquo;s blueprint for SAP migration, moving from \u0026ldquo;lifting and shifting\u0026rdquo; to enhancing business value. Digital Media \u0026amp; Telco: VTV Digital and Mobifone shared their \u0026ldquo;From Vision to Value\u0026rdquo; digital transformation journeys. Operational Excellence: MegazoneCloud emphasized using AI to streamline processes and optimize costs post-migration. Business Agility: Masterise Group presented their strategy of migrating hundreds of VMware workloads to AWS. Track 3 \u0026amp; 4: Data, AI, and DevOps Data Strategy: Experts from Onebyzero and Techcom Securities underscored that \u0026ldquo;data serves as the critical differentiator\u0026rdquo; for Generative AI. DevOps Revolution: Katalon and Renova Cloud explored integrating GenAI into the DevOps lifecycle to automate testing and code generation. Key Takeaways Design Mindset Business-Led Technology: Shift mindset from IT-centric to business-centric (e.g., F88 improving financial access, Nam Long optimizing property management). Resilience as a Standard: Continuous resilience requires deliberate design utilizing cloud-native technologies. Technical Architecture The Data-AI Dependency: Generative AI cannot function without a Data Strategy. Output quality is proportional to data input quality. Hybrid Pathways: Refactoring to microservices/serverless (e.g., TymeX). Replatforming (e.g., Masterise Group moving VMware to AWS). Modernization Strategy Beyond Migration: The strategy is \u0026ldquo;Migrate to Operate\u0026rdquo;—focusing on continuous innovation after the move. Ecosystem Integration: For FSI, the strategy is Open Banking (exposing services via APIs). Applying to Work Audit Data Readiness: Conduct a data audit to ensure a \u0026ldquo;comprehensive data strategy\u0026rdquo; before initiating GenAI projects. Pilot GenAI in DevOps: Experiment with automated code generation and testing to measure delivery velocity. Review Legacy Workloads: Analyze Honda Vietnam and Masterise Group sessions to align SAP/VMware roadmaps with best practices. Implement \u0026ldquo;Security at Scale\u0026rdquo;: Integrate security tools throughout the application lifecycle. Event Experience Attending the AWS Cloud Day Vietnam - AI Edition 2025 offered a comprehensive immersion into the future of Vietnam\u0026rsquo;s digital economy. The event was a strategic forum linking national policy, business transformation, and technical implementation.\nLearning from skilled speakers: A balanced view of strategy versus execution was provided by speakers like Dr. Jens Lottner (Techcombank) and technical experts from TechX and Masterise. Hands-on technical exposure: Deep technical insights were gained, particularly regarding \u0026ldquo;Generative AI into the DevOps Lifecycle,\u0026rdquo; showing the shift from manual coding to AI-assisted development. Networking and discussions: The event fostered dialogue between government officials, banking executives, and technical architects, highlighting that transformation involves culture as much as code. Lessons Learned Data is the critical differentiator: Without high-quality data, GenAI is just a novelty. Modernization is continuous: It requires a shift to \u0026ldquo;smarter operations.\u0026rdquo; Security is everyone\u0026rsquo;s job: It must be integrated from the first line of code. Conclusion: Overall, the event effectively demonstrated that the convergence of AWS cloud infrastructure and Generative AI is the primary engine for Vietnam\u0026rsquo;s next phase of economic growth, offering a clear, actionable roadmap for organizations ready to modernize.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"1. High-Level Architecture The IELTS BandUp platform is built on a robust, highly available architecture on AWS. The system is designed to handle user traffic securely while providing low-latency access to study materials and AI-powered features.\n2. Core AWS Services To achieve the goals of scalability, security, and high availability, we utilize the following key AWS services:\nNetworking \u0026amp; Content Delivery Amazon VPC (Virtual Private Cloud): The foundational network layer. We utilize a custom VPC with isolated Public and Private subnets to strictly control traffic flow. NAT Gateway: Allows instances in private subnets (like our Backend containers) to access the internet for updates or external API calls without being exposed to incoming public traffic. Application Load Balancer (ALB): Distributes incoming application traffic across multiple targets (containers) in different Availability Zones, ensuring fault tolerance. Amazon Route 53: A scalable Domain Name System (DNS) web service used for domain registration and traffic routing. Compute \u0026amp; Containers Amazon ECS (Elastic Container Service) on Fargate: A serverless compute engine for containers. We use Fargate to run both our Next.js Frontend and Spring Boot Backend, removing the need to provision or manage servers. Amazon ECR (Elastic Container Registry): A fully managed container registry where we store, manage, and deploy our Docker container images. Database \u0026amp; Storage Amazon RDS (Relational Database Service): We use PostgreSQL in a Multi-AZ deployment (Primary and Standby) to ensure data durability and disaster recovery for user profiles and test data. Amazon ElastiCache (Redis): Acts as an in-memory data store to cache frequent queries and manage user sessions, significantly improving application performance. Amazon S3 (Simple Storage Service): Stores static assets, media files (audio for listening tests), and user-generated content securely. AI \u0026amp; Serverless Integration To power the intelligent features of BandUp (Writing/Speaking Feedback, Flashcard Generation), we use a Serverless approach:\nAmazon Bedrock \u0026amp; Google Gemini API: The core Generative AI models used to analyze user inputs and generate personalized study feedback. AWS Lambda: Serverless compute functions that orchestrate the AI workflow, connecting the application to AI models. Amazon SQS (Simple Queue Service): Decouples the backend from the AI processing layer, allowing requests to be queued and processed asynchronously to prevent system overload. Amazon API Gateway: Acts as the \u0026ldquo;front door\u0026rdquo; for the AI services, managing RESTful API calls securely. DevOps \u0026amp; CI/CD AWS CodePipeline: Automates the release pipelines for fast and reliable application and infrastructure updates. AWS CodeBuild: Compiles source code, runs tests, and produces software packages (Docker images) ready to deploy. Security AWS WAF (Web Application Firewall): Protects the web application from common web exploits. AWS Secrets Manager: Securely stores and manages sensitive credentials (database passwords, API keys) throughout their lifecycle. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.4-setup-fe/5.4.1-docker/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize our Next.js Frontend application and push the Docker image to Amazon Elastic Container Registry (ECR). This image will later be used by ECS Fargate to launch the application.\n1. Prepare Dockerfile We use a multi-stage Dockerfile optimized for Bun (a fast JavaScript runtime) and Next.js. This configuration reduces the final image size and improves security.\nBase Image: oven/bun:1.1.26 Builder: Compiles the Next.js application. Runner: A lightweight production environment exposing port 3000. 2. Build Docker Image Run the following command in your project root to build the image. We tag it as band-up-frontend.\ndocker build -t band-up-frontend . The build process will install dependencies using bun install and compile the project.\n3. Verify Local Image Once the build is complete, verify that the image exists locally.\ndocker image ls You should see band-up-frontend with the latest tag.\nTest Locally: You can try running the container locally to ensure it starts up correctly before pushing to AWS.\ndocker run -p 3000:3000 band-up-frontend:latest 4. Push to Amazon ECR Now we need to upload this image to AWS.\nStep 1: Create Repository\nGo to Amazon ECR \u0026gt; Repositories. Click Create repository. Visibility settings: Private. Repository name: band-up-frontend. Click Create repository. Step 2: Push Image Use the AWS CLI to authenticate and push the image. Replace [AWS_ACCOUNT_ID] and [REGION] with your details.\nLogin to ECR:\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com Tag the Image:\ndocker tag band-up-frontend:latest [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Push the Image:\ndocker push [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Once the push is complete, your image is hosted on AWS ECR and ready for deployment.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.5-setup-be/5.5.1-ecr/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize the Spring Boot Backend and push the optimized Docker image to Amazon ECR.\n1. Dockerfile Strategy For the Backend, we utilize a Multi-Stage Build strategy with eclipse-temurin:21 (Java 21). This ensures a small and secure final image.\nStage 1 (Deps): Resolves and downloads Maven dependencies. Stage 2 (Package): Builds the application and extracts the Spring Boot Layered Jar. This splits the application into layers (dependencies, spring-boot-loader, application code), allowing Docker to cache unchanged layers (like dependencies) effectively. Stage 3 (Final): Copies the extracted layers into a lightweight JRE image. It also creates a non-privileged user appuser for security. 2. Build Docker Image Run the build command in the backend project root. We tag the image as band-up-backend.\ndocker build -t band-up-backend . Docker will execute the stages defined above.\n3. Create ECR Repository We need a repository to store this image.\nNavigate to Amazon ECR \u0026gt; Create repository. Repository name: band-up-backend (Ensure this matches your push command). Visibility: Private. Image tag mutability: Mutable. Click Create repository. 4. Push Image to ECR Once the image is built and the repository is ready, proceed to push.\nStep 1: Tag the Image Tag the local image with a version number (e.g., v1.0.0).\ndocker tag band-up-backend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 Step 2: Push to ECR Upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 5. Verify Navigate to the Amazon ECR Console and select the bandup-backend repository. You should see the image tagged v1.0.0.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.3-network/5.3.1-vpc/","title":"VPC, Subnets &amp; Routing","tags":[],"description":"","content":"In this step, we establish the isolated network environment for IELTS BandUp. We will create a Virtual Private Cloud (VPC), partition it into subnets across multiple Availability Zones, and configure routing for internet access.\n1. Create the VPC First, we need a private network space.\nNavigate to the VPC Dashboard. Click Create VPC. Choose VPC only. Name tag: band-up-vpc IPv4 CIDR block: 10.0.0.0/16 (This provides 65,536 IP addresses, sufficient for future scaling). Leave other settings as default and click Create VPC. 2. Create Subnets Next, we divide the VPC into smaller networks (Subnets) distributed across two Availability Zones (AZs) for High Availability. We will follow this IP schema:\nSubnet Name Type CIDR Block Availability Zone public-subnet-1 Public 10.0.0.0/24 ap-southeast-1a public-subnet-2 Public 10.0.1.0/24 ap-southeast-1b private-app-subnet-1 Private 10.0.2.0/24 ap-southeast-1a private-app-subnet-2 Private 10.0.3.0/24 ap-southeast-1b private-database-subnet-1 Database 10.0.4.0/24 ap-southeast-1a private-database-subnet-2 Database 10.0.5.0/24 ap-southeast-1b Steps:\nGo to Subnets \u0026gt; Create subnet. Select the VPC ID: band-up-vpc. Enter the Subnet name, Availability Zone, and IPv4 CIDR block for each subnet according to the table above. Repeat the process until all 6 subnets are created. 3. Create Internet Gateway (IGW) By default, a VPC is closed to the internet. To allow resources in our Public Subnets to communicate with the outside world, we need an Internet Gateway.\nGo to Internet gateways \u0026gt; Create internet gateway. Name tag: band-up-igw. Click Create internet gateway. After creation, click Actions \u0026gt; Attach to VPC. Select band-up-vpc and click Attach internet gateway. 4. Configure Route Tables Finally, we need to direct traffic from our Public Subnets to the Internet Gateway.\nGo to Route tables \u0026gt; Create route table. Name: public-route-table. VPC: band-up-vpc. Click Create route table. Add Route to Internet:\nSelect the newly created public-route-table. Go to the Routes tab \u0026gt; Edit routes. Add a new route: Destination: 0.0.0.0/0 (All traffic). Target: Select Internet Gateway -\u0026gt; band-up-igw. Click Save changes. Associate Subnets:\nGo to the Subnet associations tab \u0026gt; Edit subnet associations. Select only the Public Subnets (public-subnet-1 and public-subnet-2). Click Save associations. 5. Verify Configuration To verify that the network architecture is correctly established, navigate back to your VPC Dashboard, select band-up-vpc, and view the Resource map tab. You should see a clear structure linking your Public Subnets to the Route Table and the Internet Gateway.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives Connect and get acquainted with members of First Cloud Journey (FCJ). Understand the organization and basic AWS services. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Attend the FCJ kick-off session.\n- Learn about the FCJ organization.\n- Form a team for collaborative project work. 06/09/2025 06/09/2025 3 - Create an AWS account.\n- Study cloud computing concepts.\n- Draw a sample architecture using draw.io. 09/09/2025 09/09/2025 REFER HERE 4 - Explore the objectives of the First Cloud Journey program and the AWS website.\n- Perform initial setup on the AWS account:\n+ Create a budget.\n+ Create user groups.\n+ Enable two-factor authentication (2FA).\n- Learn about the AWS Support Center and how to submit support requests. 10/09/2025 10/09/2025 REFER HERE 5 - Create a VPC and configure its settings.\n- Create and configure Subnets (including a public Subnet with auto-assigned public IPs).\n- Set up an Internet Gateway and attach it to the VPC.\n- Create and configure a Route Table, connecting it to the Internet Gateway.\n- Configure Subnet Associations.\n- Create Security Groups (public and private). 11/09/2025 14/09/2025 REFER HERE Week 1 Achievements Successfully participated in the FCJ kick-off session and connected with team members. Gained an understanding of the FCJ organization and its objectives. Formed a team to collaborate on projects. Created an AWS account and completed initial setup tasks: Set up a budget to monitor costs. Created user groups for access management. Enabled two-factor authentication (2FA) for enhanced security. Studied basic cloud computing concepts and successfully drew a sample architecture using draw.io. Learned how to navigate the AWS Support Center and submit support requests. Successfully completed VPC-related tasks: Created and configured a VPC. Set up Subnets, including a public Subnet with auto-assigned public IPs. Created and attached an Internet Gateway to the VPC. Configured a Route Table and connected it to the Internet Gateway. Established Subnet Associations. Created public and private Security Groups for secure resource access. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This page documents the entire Worklog carried out throughout the First Cloud Journey (FCJ) internship program at AWS. This document details the process of learning, implementing the Bandup IELTS project, troubleshooting system errors, and participating in specialized events over 12 weeks (approximately 3 months).\nDuring these 12 weeks, I transitioned from familiarizing myself with core Cloud concepts to building and optimizing a complete AI-powered Serverless application on AWS, completing 50+ AWS Skill Builder courses along the way.\nSummary of Work by Week: Week Focus Area Week 1 Familiarization with FCJ, AWS account creation, basic Cloud concepts, and foundational network setup (VPC, Subnets, Internet Gateway). Week 2 Mastered Amazon EC2 and VPC fundamentals, completed AWS Skill Builder courses on IAM, Budgets, EC2, and attended Cloud Day event for AI/Data insights. Week 3 Resolved AWS account issues, configured Hybrid DNS with Route 53 Resolver and VPC Peering, learned CloudFormation and Cloud9 for IaC development. Week 4 Mastered AWS Transit Gateway for centralized network management, deep dive into EC2 Auto Scaling, Lightsail, and Migration services (DMS, VM Import/Export). Week 5 Analyzed and optimized AWS costs, designed Serverless infrastructure architecture, learned RDS, DynamoDB, ElastiCache, and set up AWS Toolkit for VS Code. Week 6 Mastered AWS Storage services (S3, Glacier, Storage Gateway), enhanced Python skills, finalized project architecture, and attended DevSecOps \u0026amp; Amazon Q Developer Webinar. Week 7 Comprehensive review and knowledge consolidation of core AWS services (Compute, Storage, Networking, Database, Security) in preparation for the mid-term exam. Week 8 Completed mid-term exam, began implementing foundational CRUD functionalities, researched serverless architecture (Lambda, API Gateway, DynamoDB), and set up development environment. Week 9 Transitioned to AWS SAM, refactored CRUD functionalities, integrated Docker for build environment, and successfully deployed project to AWS overcoming local debugging challenges. Week 10 Debugged CORS and template validation errors, integrated Frontend/Backend, completed Read/Delete functions, resolved Cognito authentication issues, and attended AWS Cloud Mastery Series #1. Week 11 Implemented Multi-Stack architecture for optimization, fixed the persistent CORS error, and began integrating AI Services (Lambda, Bedrock). Week 12 Finalized AI-powered Lambda functions, integrated Gemini API for IELTS evaluation, completed RAG pipeline for flashcard generation, and attended the final AWS Cloud Mastery Series. AWS Skill Builder Learning Path (Weeks 2-5) Category Courses Completed Networking VPC, Route 53, VPC Peering, Transit Gateway, Networking Workshop Compute EC2, EC2 Auto Scaling, Lightsail, Lightsail Containers Security IAM, IAM Roles for EC2 Database RDS, DynamoDB, ElastiCache Migration VM Import/Export, DMS, SCT, Elastic Disaster Recovery DevOps CloudFormation, Cloud9, AWS CLI, AWS Toolkit for VS Code Cost Management AWS Budgets, Cost Explorer, Service Quotas, Right-Sizing Architecture Building Highly Available Web Applications AWS Skill Builder Learning Path (Weeks 6-10) Category Courses Completed Storage Static Website Hosting with S3, AWS Backup, CloudFront Reliability Data Protection with AWS Backup Development AWS Toolkit for VS Code, Serverless patterns Learning Progression Weeks 1-5: Foundation \u0026amp; Exploration\nCore AWS services (EC2, S3, VPC, IAM) Networking fundamentals (VPC, Route 53, Transit Gateway) Cost optimization and architecture design Infrastructure as Code (CloudFormation, Cloud9) Weeks 6-7: Consolidation \u0026amp; Assessment\nStorage services mastery (S3, Glacier, Storage Gateway) Disaster recovery and backup strategies Comprehensive exam preparation Mid-term exam completion Weeks 8-10: Implementation \u0026amp; Deployment\nServerless architecture implementation (Lambda, API Gateway, DynamoDB) AWS SAM framework adoption Docker integration for consistent builds Frontend-Backend integration Production deployment and debugging Weeks 11-12: Advanced Features \u0026amp; AI Integration\nMulti-stack architecture optimization AI services integration (Bedrock, Gemini API) RAG pipeline implementation Final project completion "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/5.6.1-api-gateway/","title":"API Gateway","tags":[],"description":"","content":"Overview Create Amazon API Gateway as the entry point for AI service requests.\nCreate REST API Navigate to API Gateway → Create API → REST API Setting Value API name ielts-ai-api API type REST API Endpoint type Regional Create Resources and Methods Endpoints:\nMethod Path Description POST /writing/evaluate Submit writing sample POST /speaking/evaluate Submit audio recording POST /flashcards/generate Generate flashcard POST /upload/audio Upload audio POST /upload/document Upload document Configure SQS Integration For each POST endpoint, configure SQS integration:\nSelect method → Integration Request Integration type: AWS Service AWS Service: SQS HTTP method: POST Action: SendMessage Execution role: API Gateway role with SQS permissions Request Mapping Template:\nAction=SendMessage\u0026amp;MessageBody=$util.urlEncode($input.body)\u0026amp;QueueUrl=$util.urlEncode(\u0026#39;https://sqs.ap-southeast-1.amazonaws.com/{account}/ielts-writing-queue\u0026#39;) Enable CORS Select resource → Enable CORS Access-Control-Allow-Origin: * (or specific domain) Access-Control-Allow-Methods: POST, GET, OPTIONS Deploy API Actions → Deploy API Stage name: dev Note the invoke URL: https://{api-id}.execute-api.ap-southeast-1.amazonaws.com/dev AWS CLI Commands # Create REST API API_ID=$(aws apigateway create-rest-api \\ --name ielts-ai-api \\ --endpoint-configuration types=REGIONAL \\ --query \u0026#39;id\u0026#39; --output text) # Get root resource ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#39;items[?path==`/`].id\u0026#39; --output text) # Create /ai resource AI_RESOURCE=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part ai \\ --query \u0026#39;id\u0026#39; --output text) # Create /ai/writing-assessment resource aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AI_RESOURCE \\ --path-part writing-assessment Next Steps Proceed to SQS Queues.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerate benefits claims processing with Amazon Bedrock Data Automation In the benefits administration industry, claims processing is a vital operational pillar that makes sure employees and beneficiaries receive timely benefits, such as health, dental, or disability payments, while controlling costs and adhering to regulations like HIPAA and ERISA. Businesses aim to optimize the workflow—covering claim submission, validation, adjudication, payment, and appeals—to enhance employee satisfaction, strengthen provider relationships, and mitigate financial risks. The process includes specific steps like claim submission (through portals or paper), data validation (verifying eligibility and accuracy), adjudication (assessing coverage against plan rules), payment or denial (including check processing for reimbursements), and appeal handling. Efficient claims processing supports competitive benefits offerings, which is crucial for talent retention and employer branding, but requires balancing speed, accuracy, and cost in a highly regulated environment.\nDespite its importance, claims processing faces significant challenges in many organizations. Most notably, the reliance on legacy systems and manual processes results in frustratingly slow resolution times, high error rates, and increased administrative costs. Incomplete or inaccurate claim submissions—such as those with missing diagnosis codes or eligibility mismatches—frequently lead to denials and rework, creating frustration for both employees and healthcare providers. Additionally, fraud, waste, and abuse continue to inflate costs, yet detecting these issues without delaying legitimate claims remains challenging. Complex regulatory requirements demand constant system updates, and poor integration between systems—such as Human Resource Information Systems (HRIS) and other downstream systems—severely limits scalability. These issues drive up operational expenses, erode trust in benefits programs, and overburden customer service teams, particularly during appeals processes or peak claims periods.\nGenerative AI can help address these challenges. With Amazon Bedrock Data Automation, you can automate generation of useful insights from unstructured multimodal content such as documents, images, audio, and video. Amazon Bedrock Data Automation can be used in benefits claims process to automate document processing by extracting and classifying documents from claims packets, policy applications, and supporting documents with industry-leading accuracy, reducing manual errors and accelerating resolution times. Amazon Bedrock Data Automation natural language processing capabilities interpret unstructured data, such as provider notes, supporting compliance with plan rules and regulations. By automating repetitive tasks and providing insights, Amazon Bedrock Data Automation helps reduce administrative burdens, enhance experiences for both employees and providers, and support compliance in a cost-effective manner. Furthermore, its scalable architecture enables seamless integration with existing systems, improving data flow across HRIS, claims systems, and provider networks, and advanced analytics help detect fraud patterns to optimize cost control.\nIn this post, we examine the typical benefit claims processing workflow and identify where generative AI-powered automation can deliver the greatest impact.\nBenefit claims processing When an employee or beneficiary pays out of pocket for an expense covered under their health benefits, they submit a claim for reimbursement. This process requires several supporting documents, including doctor’s prescriptions and proof of payment, which might include check images, receipts, or electronic payment confirmations.\nThe claims processing workflow involves several critical steps:\nDocument intake and processing – The system receives and categorizes submitted documentation, including: Medical records and prescriptions Proof of payment documentation Supporting forms and eligibility verification Payment verification processing – For check-based reimbursements, the system must complete the following steps: Extract information from check images, including the account number and routing number contained in the MICR line Verify payee and payer names against the information provided during the claim submission process Confirm payment amounts match the claimed expenses Flag discrepancies for human review Adjudication and reimbursement – When verification is complete, the system performs several actions: Determine eligibility based on plan rules and coverage limits Calculate appropriate reimbursement amounts Initiate payment processing through direct deposit or check issuance Provide notification to the claimant regarding the status of their reimbursement In this post, we walk through a real-world scenario to make the complexity of this multi-step process clearer. The following example demonstrates how Amazon Bedrock Data Automation can streamline the claims processing workflow, from initial submission to final reimbursement.\nSolution overview Let’s consider a scenario where a benefit plan participant seeks treatment and pays out of pocket for the doctor’s fee using a check. They then buy the medications prescribed by the doctor at the pharmacy store. Later, they log in to their benefit provider’s portal and submit a claim along with the image of the check and payment receipt for the medications.\nThis solution uses Amazon Bedrock Data Automation to automate the two most critical and time-consuming aspects of this workflow: document intake and payment verification processing. The following diagram illustrates the benefits claims processing architecture.\nThe end-to-end process works through four integrated stages: ingestion, extraction, validation, and integration.Quy trình tổng thể hoạt động qua bốn giai đoạn tích hợp: nhập dữ liệu, trích xuất dữ liệu, kiểm tra, và tích hợp.\nIngestion When a beneficiary uploads supporting documents (check image and pharmacy receipt) through the company’s benefit claims portal, these documents are securely saved in an Amazon Simple Storage Service (Amazon S3) bucket, triggering the automated claims processing pipeline.\nExtraction After documents are ingested, the system immediately begins with intelligent data extraction:\nThe S3 object upload triggers an AWS Lambda function, which invokes the Amazon Bedrock Data Automation project. Amazon Bedrock Data Automation uses blueprints for file processing and extraction. Blueprints are artifacts used to configure file processing business logic by specifying a list of field names for data extraction, along with their desired data formats (string, number, or Boolean) and natural language context for data normalization and validation rules. Amazon Bedrock Data Automation provides a catalog of sample blueprints out of the box. You can create a custom blueprint for your unique document types that aren’t predefined in the catalog. This solution uses two blueprints designed for different document types, as shown in the following screenshot:\nThe catalog blueprint US-Bank-Check for check processing.\nThe custom blueprint benefit-claims-pharmacy-receipt-blueprint for pharmacy-specific receipts.\nUS-Bank-Check is a catalog blueprint provided out of the box by Amazon Bedrock Data Automation. The custom blueprint benefit-claims-pharmacy-receipt-blueprint is created using an AWS CloudFormation template to handle pharmacy receipt processing, addressing a specific document type that wasn’t available in the standard blueprint catalog. The benefit administrator wants to look for vendor-specific information such as name, address, and phone details for benefits claims processing. The custom blueprint schema contains natural language explanation of those fields, such as VendorName, VendorAddress, VendorPhone, and additional fields, explaining what the field represents, expected data types, and inference type for each extracted field (explained in Creating Blueprints for Extraction), as shown in the following screenshot.\nThe two blueprints are added to the Amazon Bedrock Data Automation project. An Amazon Bedrock Data Automation project is a grouping of both standard and custom blueprints that you can use to process different types of files (like documents, audio, and images) using specific configuration settings, where you can control what kind of information you want to extract from each file type. When the project is invoked asynchronously, it automatically applies the appropriate blueprint, extracts information such as confidence scores and bounding box details for each field, and saves results in a separate S3 bucket. This intelligent classification alleviates the need for you to write complex document classification logic.\nThe following screenshot illustrates the document classification by the standard catalog blueprint US-Bank-Check.\nThe following screenshot shows the document classification by the custom blueprint benefit-claims-pharmacy-receipt-blueprint.\nValidation With the data extracted, the system moves to the validation and decision-making process using the business rules specific to each document type.\nThe business rules are documented in standard operating procedure documents (AnyCompany Benefit Checks Standard Operating procedure.docx and AnyCompany Benefit Claims Standard Operating procedure.docx) and uploaded to an S3 bucket. Then the system creates a knowledge base for Amazon Bedrock with the S3 bucket as the source, as shown in the following screenshot.\nWhen the extracted Amazon Bedrock Data Automation results are saved to the configured S3 bucket, a Lambda function is triggered automatically. Based on the business rules retrieved from the knowledge base for the specific document type and the extracted Amazon Bedrock Data Automation output, an Amazon Nova Lite large langue model (LLM) makes the automated approve/deny decision for claims.\nThe following screenshot shows the benefit claim adjudication automated decision for US-Bank-Check.\nThe following screenshot shows the benefit claim adjudication automated decision for benefit-claims-pharmacy-receipt-blueprint.\nIntegration The system seamlessly integrates with existing business processes.\nWhen validation is complete, an event is pushed to Amazon EventBridge, which triggers a Lambda function for downstream integration. In this implementation, we use an Amazon DynamoDB table and Amazon Simple Notification Service (Amazon SNS) email for downstream integration. A DynamoDB table is created as part of the deployment stack, which is used to populate details including document classification, extracted data, and automated decision. An email notification is sent for both check and receipts after the final decision is made by the system. The following screenshot shows an example email for pharmacy receipt approval.\nThis flexible architecture helps you integrate with your existing applications through internal APIs or events to update claim status or trigger additional workflows when validation fails.\nReducing manual effort through intelligent business rules management Beyond automating document processing, this solution addresses a common operational challenge: Traditionally, customers must write and maintain code for handling business rules around claims adjudication and processing. Every business rule change requires development effort and code updates, slowing time-to-market and increasing maintenance overhead.\nOur approach converts business rules and standard operating procedures (SOPs) into knowledge bases using Amazon Bedrock Knowledge Bases, which you can use for automated decision-making. This approach can dramatically reduce time-to-market when business rules change, because updates can be made through knowledge management rather than code deployment.\nIn the following sections, we walk you through the steps to deploy the solution to your own AWS account.\nPrerequisites To implement the solution provided in this post, you must have the following:\nAn AWS account Access to Amazon Titan Text Embeddings V2 and Amazon Nova Lite foundation models (FMs) enabled in Amazon Bedrock This solution uses Python 3.13 with Boto3 1.38. or later version, and the AWS Serverless Application Model Command Line Interface (AWS SAM CLI) version 1.138.0. We assume that you have installed these in your local machine already. If not, refer to the following instructions:\nPython 3.13 installation Install the AWS SAM CLI Set up code in your local machine To set up the code, clone the GitHub repository. After you have cloned the repository to your local machine, the project folder structure will look like the following code, as mentioned in the README file:\nDeploy the solution in your account The sample code comes with a CloudFormation template that creates necessary resources. To deploy the solution in your account, follow the deployment instructions in the README file.\nClean up Deploying this solution in your account will incur costs. Follow the cleanup instructions in the README file to avoid charges when you are done.\nConclusion Benefits administration companies can significantly enhance their operations by automating claims processing using the solution outlined in this post. This strategic approach directly addresses the industry’s core challenges and can deliver several key advantages:\nEnhanced processing efficiency through accelerated claims resolution times, reduced manual error rates, and higher straight-through processing rates that minimize the frustrating delays and manual rework plaguing legacy systems Streamlined document integration and fraud detection capabilities, where adding new supporting documents becomes seamless through new Amazon Bedrock Data Automation blueprints, while AI-powered analytics identify suspicious patterns without delaying legitimate claims, avoiding traditional months-long development cycles and reducing costly fraud, waste, and abuse\nAgile business rule management that enables rapid adaptation to changing HIPAA and ERISA requirements and modification of business rules, significantly reducing administrative costs and time-to-market while improving scalability and integration with existing HRIS and claims, ultimately enhancing employee satisfaction, strengthening provider relationships, and supporting competitive benefits offerings that are crucial for talent retention and employer branding\nTo get started with this solution, refer to the GitHub repo. For more information about Amazon Bedrock Data Automation, refer to Transform unstructured data into meaningful insights using Amazon Bedrock Data Automation and try the Document Processing Using Amazon Bedrock Data Automation workshop.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Enabling AI adoption at scale through enterprise risk management framework – Part 2 In Part 1 of this series, we explored the fundamental risks and governance considerations. In this part, we examine practical strategies for adapting your enterprise risk management framework (ERMF) to harness generative AI’s power while maintaining robust controls.\nThis part covers:\nAdapting your ERMF for the cloud Adapting your ERMF for generative AI Sustainable Risk Management By the end of this post, you’ll have a roadmap for scaling generative AI adoption securely and responsibly.\nAdapting your ERMF for the cloud Before diving into generative AI-specific controls, it’s crucial to understand the fundamental infrastructure that enables these technologies. Cloud computing is the foundational infrastructure that has made generative AI possible and accessible at scale. The development and deployment of large language models and other generative AI systems require massive computational resources, vast amounts of data storage, and sophisticated distributed processing capabilities that cloud systems can efficiently provide.\nCloud technology differs from on-premises IT solutions, and the relationship between financial institutions and cloud service providers is also different from the relationship with a traditional outsourcing provider.\nThese differences change the nature of many risks that financial institutions face and how they manage them. However, if cloud technology is implemented in the right way, it can reduce risk and provide tools to help Chief Risk Officers (CROs) to manage risk too.\nYou can read more about how your ERMF needs to change for large scale cloud adoption in Is your Enterprise Risk Management Framework ready for the Cloud?\nAdapting your ERMF for generative AI Organizations adopting generative AI can use their enterprise risk management framework to realize business value while maintaining appropriate controls. This approach allows you to build on existing risk management practices while addressing generative AI’s unique characteristics.\nFor a structured approach to cloud-enabled AI transformation, the AWS Cloud Adoption Framework for AI, ML, and generative AI (AWS CAF for AI) provides detailed implementation guidance aligned with enterprise risk management principles. For a detailed user guide, see AWS User Guide to Governance, Risk and Compliance for Responsible AI Adoption within Financial Services Industries, available in AWS Artifact using your AWS sign in. AWS Artifact provides AWS security and compliance reports, helping organizations maintain compliance through best practices.\nWhen it comes to model management and the AI system lifecycle, customers can consult ISO42001 AI Management, Section A6. This section encompasses capturing the objective and processes for the responsible design and development of AI systems, including criteria and requirements for each stage of the AI system life cycle. This guidance can help organizations verify that their model management practices align with industry standards for responsible AI development.\nFrom a business leader’s perspective, incorporating generative AI considerations into your ERMF helps establish documented good practices, implement effective controls, and maintain transparency about usage across the enterprise. This enables both responsible innovation and prudent risk management. Here’s how organizations are approaching this:\nGenerative AI policy and governance foundations in ERMF In the field of generative AI, organizations establish both guardrails for innovation and clear accountability for risk management. The three lines of defense model provides the structure for implementing these foundational elements:\nAcceptable use framework for your organization: Clear direction on appropriate generative AI use helps organizations manage risks while enabling innovation. The range of use cases for generative AI is large and likely to expand over the years, making it essential to have clear guidance on what applications are permitted and under what conditions. As organizations explore these opportunities, their framework can evolve with their experience and maturity. Risk accountability: The generative AI lifecycle—from use case selection through implementation and ongoing monitoring—requires clear ownership across business and control functions. While organizations can establish specific generative AI oversight mechanisms, these should integrate with existing governance structures. Risk reporting and accountability for generative AI initiatives should flow through established enterprise risk committees and governance boards, helping to facilitate consistent risk management across the organization rather than creating isolated pockets of oversight. Implementation approach for generative AI: Putting principles into practice Building on the three lines of defense model discussed earlier, organizations can adapt their risk management practices to address the unique characteristics of generative AI while using industry best practices and frameworks. This often involves evolving existing controls and introducing new ones specific to generative AI. AWS services have built-in capabilities that support these enhanced governance, risk management, and compliance requirements, helping organizations to implement controlled and responsible generative AI solutions. This includes, for example, Amazon Bedrock Guardrails, among many others.\nBuilding on the risk areas we outlined earlier, we now explore how organizations can implement controls for each of these areas. For each, we describe the principle and the practical implementation considerations. While organizations might prioritize these areas differently based on their use cases and risk appetite, together they provide a framework for responsible generative AI adoption through ERMF.\nWhile we explore high-level control principles that follow, technical teams can review the AWS Well-Architected Framework – Generative AI Lens for detailed architectural guidance that supports these governance objectives.\nFairness Generative AI systems can deliver equitable outcomes across different stakeholder groups, helping organizations build trust and meet expectations. Organizations can support this by setting up clear fairness metrics for specific use cases, regularly assessing training data for bias, and closely monitoring performance across different groups. For high-stakes applications, additional checks can help facilitate fair treatment across diverse populations.\nAmazon Bedrock Guardrails provides configurable safeguards to help maintain fair and unbiased outputs, with customizable thresholds to match different use case requirements. Amazon Bedrock provides comprehensive model evaluation tools including model cards with detailed bias metrics, to assess bias across demographic groups. Amazon Bedrock includes built-in prompt datasets like the Bias in Open-ended Language Generation Dataset (BOLD), which automatically evaluates fairness across key areas such as profession, gender, race, and various ideologies. These capabilities integrate with Amazon SageMaker Clarify for comprehensive bias detection and mitigation, supported by built-in bias metrics and reporting.\nExplainability Generative AI systems can provide understanding of their decision-making processes, supporting accountability and effective oversight. Explainability is essential for all generative AI systems—whether using custom-built or pre-built models, particularly for complex models like transformer networks.\nOrganizations can implement practical controls by establishing clear explainability thresholds based on use case risk levels. This remains an active industry challenge, with ongoing research and evolving approaches. For critical business applications, tailoring explanations to different stakeholders while maintaining accuracy can improve understanding and trust.\nAmazon Bedrock provides tools that help identify which factors influenced the generative AI’s decisions, while maintaining detailed records of system inputs and outputs. For complex workflows, Chain-of-Thought (CoT) reasoning traces are available through Amazon Bedrock Agents, showing the step-by-step logic behind each decision. Organizations can monitor how responses are generated in real time. For Retrieval-Augmented Generation (RAG) applications, which optimize AI outputs by referencing specific knowledge bases, Amazon Bedrock Knowledge Bases automatically includes references and links to source materials used in generating responses.\nPrivacy and security Generative AI systems benefit from strong privacy and security measures to protect sensitive information and help prevent unauthorized access or data exposure. These systems can potentially generate content or unintentionally reveal confidential data, which organizations can proactively manage.\nOrganizations can set up multi-layered protection strategies, including access controls, content filtering, and data privacy safeguards. This can involve creating company-wide standards for prompt engineering to help prevent harmful outputs, using techniques like RAG to control information sources, and using automated systems to detect and protect personal information. Regular testing and validation, especially to comply with regulations like GDPR, can be part of the development and deployment process.\nAmazon Bedrock implements multiple security layers including private endpoints with Amazon Virtual Private Cloud (Amazon VPC) support, fine-grained AWS Identity and Access Management (IAM) access control, and end-to-end encryption. Importantly, it maintains no persistent storage of prompt or completion data and helps preserve model provider isolation.\nAmazon Bedrock Guardrails provides sensitive information filters that can detect and protect personally identifiable information (PII) through automated input rejection, response redaction, and configurable regex patterns, supporting various use cases while maintaining data privacy. Organizations like Genesys demonstrate these capabilities at scale, maintaining GDPR compliance while processing 1.5 billion monthly customer interactions through Amazon Bedrock.\nFor detailed security considerations, see Generative AI Security Scoping Matrix, which provides a comprehensive framework for assessing and addressing generative AI security risks.\nSafety Generative AI systems can be designed and operated with safeguards to avoid harm to individuals, and communities. This includes addressing risks of generating dangerous, illegal, or abusive content, and helping to prevent system misuse.\nOrganizations can implement specific safety measures through predeployment content filtering, real-time safety boundaries with prompt constraints, and output classification systems to detect and block dangerous content. Context-aware content moderation considers the specific application domain, while automated detection can identify potential safety violations before content generation. Ongoing monitoring and updating of these controls help address evolving capabilities and potential risks of generative AI systems.\nAmazon Bedrock Guardrails delivers industry-leading safety protections across text and images, blocking up to 85 percent more harmful content on top of native protections provided by foundation models (FMs). Additional safety controls include token limits to avoid excessive responses, rate limiting against misuse, and moderation endpoints for content screening.\nFor full practical implementation guidance on building safety controls, see Build safe and responsible generative AI applications with guardrails.\nControllability Organizations can maintain appropriate control over generative AI systems to make sure that they work as intended and can be adjusted or stopped if issues arise. This helps manage risks and maintain system reliability.\nA multi-layered approach to control includes implementing technical safeguards and operational processes. Organizations can control model behaviour by adjusting parameters such as temperature (controlling output randomness), and sampling methods like top-k or top-p (managing output diversity). Clear operational boundaries define the system’s scope of action, while human-in-the-loop validation provides oversight for critical applications.\nFor effective control, organizations can establish parameter thresholds tailored to different use cases, implement rapid adjustment mechanisms, and create clear escalation procedures. Amazon Bedrock enhances control through customizable agent prompts and reasoning techniques, and the ability to break complex tasks into smaller, manageable components. Organizations can choose between structured workflows or flexible agent-based approaches. Regular comparison of outputs against established benchmarks helps maintain system reliability.\nThis balanced approach supports creative AI outputs while helping to facilitate consistent performance within defined quality limits. This helps prevent service degradation and business disruption while minimizing inefficiencies.\nControl capabilities are further enhanced through Amazon CloudWatch monitoring integration and robust knowledge base version control. The capabilities of Amazon Bedrock, including LLM-as-a-judge features, help organizations assess and optimize their generative AI applications efficiently.\nVeracity and robustness Generative AI systems can produce reliable and accurate outputs, even when faced with unexpected or challenging inputs. This helps maintain trust and helps maintain the system’s usefulness across various applications.\nOrganizations can implement a combination of technical and procedural controls to enhance both system robustness and output reliability. This includes establishing clear parameter thresholds for different use cases, implementing human-in-the-loop validation for critical applications, and regularly comparing outputs against established ground truths. The framework specifies when and how these controls are applied based on the use case criticality and required level of accuracy.\nAmazon Bedrock Guardrails improves veracity by helping to prevent factual errors through automated reasoning checks that deliver up to 99 percent accuracy in detecting correct responses from models, using mathematical logic and formal verification techniques. This capability supports processing of large documents up to 80,000 tokens and includes automated scenario generation for comprehensive testing.\nAmazon Bedrock also includes sophisticated input sanitization features and supports adversarial testing through AWS testing tools integration.\nGovernance Effective governance of generative AI systems helps manage risks, maintain accountability, and align AI use with organizational values and regulations. This covers the entire AI lifecycle, from development to deployment and ongoing operation.\nOrganizations can create clear governance structures, including defined roles for AI oversight, regular risk assessments, and ways to engage with stakeholders. This involves integrating AI governance into existing risk management practices and making sure of compliance with relevant laws and standards. Because AI technology is evolving rapidly, regular reviews and updates to governance practices are essential to address new capabilities, emerging risks, and changing regulatory requirements. This includes providing appropriate training and skill development for system users.\nAWS has achieved of ISO/IEC 42001 certification, demonstrating our commitment to systematic governance approaches in AI implementation. Governance features in Amazon Bedrock include comprehensive model provenance tracking, detailed AWS CloudTrail audit logging, and streamlined model deployment approval workflows integrated with AWS Organizations. AWS Audit Manager provides pre-built frameworks to assess generative AI implementation against best practices.\nTransparency Generative AI systems can operate transparently, helping stakeholders understand system capabilities, limitations, and the context of AI-generated outputs. This builds trust and enables informed decision-making by users and affected parties.\nOrganizations can implement specific transparency measures including comprehensive model documentation detailing intended use cases, known limitations, and performance boundaries. Clear AI disclosure practices should describe when and how AI is being used and what data is being processed. Regular performance reporting can include accuracy rates, error patterns, and bias assessments.\nFor customer-facing applications, transparency includes providing clear indicators of AI-generated content, documenting how decisions are made, and establishing processes for users to question or challenge outputs. Maintaining detailed version histories of model updates and changes in system behavior helps track the evolution of AI capabilities and their impacts over time.\nFrom the AWS side of the Shared Responsibility Model, transparency is supported through AWS AI Service Cards and detailed documentation of model characteristics. Amazon Bedrock enhances this with comprehensive logging and monitoring capabilities to track model behavior and performance metrics.\nUnified risk management These eight areas are interconnected and mutually reinforcing within the enterprise risk management framework. While organizations might prioritize them differently based on their use cases and risk appetite, together they provide a comprehensive approach to responsible generative AI adoption. For detailed technical guidance, standards, and compliance requirements, see the AWS guidance documents in Resources for technical implementation, at the end of this blog post, that support implementation across these areas.\nAI risk management in practice: Building organizational capability Successful implementation of generative AI systems involves integrating risk management practices across the organization. This includes establishing processes for measuring outcomes and risks and preparing the organization to adapt as technology evolves. Effective risk management depends on building appropriate knowledge and skills at all levels of the organization.\nOrganizations can create clear pathways from proof of concept to production by aligning with the three lines of defense model. The ERMF provides broad parameters for reliability, safety, and privacy, which business units can adapt for their specific use cases.\nTo build and maintain lasting capability for both current and future generative AI adoption, organizations can focus on:\nDeveloping incident response plans for AI-specific scenarios Building expertise through training and certification programs Regular review and updates of risk management practices These elements, when woven into the organization’s operating fabric, create sustainable practices that evolve with advancing technology and emerging risks. Sustainable risk management: Making your ERMF generative AI-ready Governance, risk, and compliance (GRC) leaders, Chief Risk Officers (CROs), and Chief Internal Auditors (CIAs) can provide sustained executive sponsorship for generative AI adoption. Long-term capability building extends beyond technology and innovation hubs to encompass business and control functions. Clear direction from leadership helps organizations balance generative AI opportunities with appropriate risk management.\nOrganizations benefit from viewing generative AI as a transformative capability that touches many functions rather than as isolated initiatives. This approach supports sustainable integration of enterprise-wide governance approaches for generative AI, avoiding the limitations of short-term projects with restricted scope and impact.\nOrganizations can successfully implement generative AI while maintaining their risk management obligations through controlled, well-defined use cases. TP ICAP’s Parameta division demonstrates this approach in their regulatory compliance implementation. By focusing initially on a highly regulated area, maintaining clear governance controls, and making sure there was human oversight in the compliance review process, they established a framework for responsible AI adoption. This led to creating dedicated oversight roles for AI initiatives, strengthening their governance structure for future AI implementations.\nSimilarly, Rocket Mortgage’s implementation of AWS services for their AI tool Rocket Logic – Synopsis demonstrates how organizations can use Amazon Bedrock for responsible AI integration at scale. This approach enabled them to maintain stringent data security and compliance measures while saving 40,000 team hours annually through automated processes.\nAction checklist for sustainable generative AI implementation:\nERMF foundations: Assess and enhance your risk framework’s readiness for generative AI, including acceptable use guidelines and clear accountabilities Technical controls: Begin with core controls such as Amazon Bedrock Guardrails and expand based on specific use cases and risk profiles Organizational capability: Develop broad expertise through training and oversight mechanisms across business and control functions Monitoring and measurement: Create dashboards for key risk indicators and maintain regular reviews Integration strategy: Align generative AI controls with existing processes and organizational strategy Conclusion This two-part series has explored the critical importance of integrating generative AI governance into enterprise risk management frameworks. In Part 1, we introduced the unique risks and governance considerations associated with generative AI adoption. Part 2 has provided a comprehensive guide for adapting your ERMF to address these challenges effectively.\nWe’ve outlined practical strategies for scaling generative AI adoption securely and responsibly, covering key areas such as fairness, explainability, privacy and security, safety, controllability, veracity and robustness, governance, and transparency. By implementing these strategies and following the action checklist provided, organizations can build sustainable practices that evolve with advancing technology and emerging risks.\nOrganizations that integrate generative AI governance into their ERMF as described in this post are better positioned to accelerate innovation and operational efficiency while protecting against key risks such as data exposure, model hallucinations, and regulatory non-compliance. This balanced approach enables organizations to capture the transformative potential of generative AI while maintaining the robust controls essential for financial services institutions.\nFor foundational concepts and risk considerations, see Part 1.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock PropHero is a leading property wealth management service that democratizes access to intelligent property investment advice through big data, AI, and machine learning (ML). For the Spanish and Australian consumer base, PropHero needed an AI-powered advisory system that could engage customers in accurate property investment discussions. The goal was to provide personalized investment insights and to guide and assist users at every stage of their investment journey: from understanding the process, gaining visibility into timelines, securely uploading documents, to tracking progress in real time.\nPropHero collaborated with the AWS Generative AI Innovation Center to implement an intelligent property investment advisor using AWS generative AI services with continuous evaluation. The solution helps users engage in natural language conversations about property investment strategies and receive personalized recommendations based on PropHero’s comprehensive market knowledge.\nIn this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement.\nThe challenge: Making property investment knowledge more accessible The area of property investment presents numerous challenges for both novice and experienced investors. Information asymmetry creates barriers where comprehensive market data remains expensive or inaccessible. Traditional investment processes are manual, time-consuming, and require extensive market knowledge to navigate effectively. For the Spanish and Australian consumers specifically, we needed to build a solution that could provide accurate, contextually relevant property investment advice in Spanish while handling complex, multi-turn conversations about investment strategies. The system needed to maintain high accuracy while delivering responses at scale, continuously learning and improving from customer interactions. Most importantly, it needed to assist users across every phase of their journey, from initial onboarding through to final settlement, ensuring comprehensive support throughout the entire investment process.\nSolution overview We built a complete end-to-end solution using AWS generative AI services, architected around a multi-agent AI advisor with integrated continuous evaluation. The system provides seamless data flow from ingestion through intelligent advisory conversations with real-time quality monitoring. The following diagram illustrates this architecture.\nThe solution architecture consists of four virtual layers, each serving specific functions in the overall system design.\nData foundation layer The data foundation provides the storage and retrieval infrastructure for system components:\nAmazon DynamoDB – Fast storage for conversation history, evaluation metrics, and user interaction data Amazon Relational Database (Amazon RDS) for PostgreSQL – A PostgreSQL database storing LangFuse observability data, including large language model (LLM) traces and latency metrics Amazon Simple Storage Service (Amazon S3) – A central data lake storing Spanish FAQ documents, property investment guides, and conversation datasets Multi-agent AI layer The AI processing layer encompasses the core intelligence components that power the conversational experience:\nAmazon Bedrock – Foundation models (FMs) such as LLMs and rerankers powering specialized agents Amazon Bedrock Knowledge Bases – Semantic search engine with semantic chunking for FAQ-style content LangGraph – Orchestration of multi-agent workflows and conversation state management AWS Lambda – Serverless functions executing multi-agent logic and retrival of user information for richer context Continuous evaluation layer The evaluation infrastructure facilitates continuous quality monitoring and improvement through these components:\nAmazon CloudWatch – Real-time monitoring of quality metrics with automated alerting and threshold management Amazon EventBridge – Real-time event triggers for conversation completion and quality assessment AWS Lambda – Automated evaluation functions measuring context relevance, response groundedness, and goal accuracy Amazon QuickSight – Interactive dashboards and analytics for monitoring the respective metrics Application and integration layer The integration layer provides secure interfaces for external communication:\nAmazon API Gateway – Secure API endpoints for conversational interface and evaluation webhooks Multi-agent AI advisor architecture The intelligent advisor uses a multi-agent system orchestrated through LangGraph, which sits in a single Lambda function, where each agent is optimized for specific tasks. The following diagram shows the communication flow among the various agents within the Lambda function.\nAgent composition and model selection Our model selection strategy involved extensive testing to match each component’s computational requirements with the most cost-effective Amazon Bedrock model. We evaluated factors including response quality, latency requirements, and cost per token to determine optimal model assignments for each agent type.Each component in the system uses the most appropriate model for its designated function, as outlined in the following table.\nEnd-to-end conversation flow The conversation processing follows a structured workflow that facilitates accurate responses while maintaining quality standards:\nUser queries enter through API Gateway and are routed to the router agent. The router agent determines the appropriate specialized agent based on query analysis. User information is retrieved at the start for richer context and knowledge-intensive queries trigger the retriever to access the Amazon Bedrock knowledge base. Specialized agents process queries with retrieved user information and relevant context from the knowledge base. The response agent formats and generates the final user-facing response with the appropriate tone. Parallel evaluation processes assess context relevance, response groundedness, and goal accuracy. Conversation data is stored in DynamoDB for analysis and improvement. Amazon Bedrock Knowledge Bases configuration The knowledge base uses several AWS services for optimal performance:\nData source – S3 bucket Chunking strategy – Semantic chunking optimized for FAQ-style content Embedding model – Cohere Embed Multilingual v3 for Spanish understanding Reranker – Cohere Rerank 3.5 for reranking retrieved Spanish content Vector database – Amazon OpenSearch Serverless for scalable semantic search Sample conversation experience The system handles natural Spanish conversations seamlessly. For example:\nUsuario: “Hola, ¿qué es PropHero exactamente?”\nAsistente: “¡Hola! PropHero es una plataforma que te permite crear, optimizar y crecer tu patrimonio inmobiliario delegando todo el proceso en nuestro equipo de profesionales…”\nUsuario: “¿Y cómo funciona en la práctica?”\nAsistente: “Es muy sencillo. Comenzamos con una sesión con tu Property Coach para definir tu estrategia de inversión personalizada…”\nIntegrated continuous evaluation system The continuous evaluation system is integrated into the architecture, operating as a core component. This approach supports quality monitoring in real-time alongside conversation processing.\nEvaluation metrics We used the following metrics from the Ragas library for evaluation:\nContext Relevance (0–1) – Measures the relevance of retrieved context to user queries, evaluating RAG system effectiveness Response Groundedness (0–1) – Makes sure responses are factually accurate and derived from PropHero’s official information Agent Goal Accuracy (0–1) – Binary measure of whether responses successfully address user investment goals Real-time evaluation workflow The evaluation system operates seamlessly within the conversation architecture:\nAmazon DynamoDB Streams triggers – Conversation data written to DynamoDB automatically triggers a Lambda function for evaluation through Amazon DynamoDB Streams Parallel processing – Lambda functions execute evaluation logic in parallel with response delivery Multi-dimensional assessment – Each conversation is evaluated across three key dimensions simultaneously Intelligent scoring with LLM-as-a-judge – Anthropic’s Claude 3.5 Haiku provides consistent evaluation as an LLM judge, offering standardized assessment criteria across conversations. Monitoring and analytics – CloudWatch captures metrics from the evaluation process, and QuickSight provides dashboards for trend analysis The following diagram provides an overview of the Lambda function responsible for continuous evaluation. Implementation insights and best practices Our development journey involved a 6-week iterative process with PropHero’s technical team. We conducted testing across different model combinations and evaluated chunking strategies using real customer FAQ data. This journey revealed several architectural optimizations that enhanced system performance, achieved significant cost reductions, and improved user experience.\nModel selection strategy Our approach to model selection demonstrates the importance of matching model capabilities to specific tasks. By using Amazon Nova Lite for simpler tasks and Amazon Nova Pro for complex reasoning, the solution achieves optimal cost-performance balance while maintaining high accuracy standards.\nChunking and retrieval optimization Semantic chunking proved superior to hierarchical and fixed chunking approaches for FAQ-style content. The Cohere Rerank 3.5 model enabled the system to use fewer chunks (10 vs. 20) while maintaining accuracy, reducing latency and cost.\nMultilingual capabilities The system effectively handles Spanish and English queries by using FMs that support Spanish language on Amazon Bedrock.\nBusiness impact The PropHero AI advisor delivered measurable business value:\nEnhanced customer engagement – A 90% goal accuracy rate makes sure customers receive relevant, actionable property investment advice. Over 50% of our users (and over 70% of paid users) are actively using the AI advisor. Operational efficiency – Automated responses to common questions reduced customer service workload by 30%, freeing staff to focus on complex customer needs. Scalable growth – The serverless architecture automatically scales to handle increasing customer demand without manual intervention. Cost optimization – Strategic model selection achieved high performance while reducing AI costs by 60% compared to using premium models throughout. Consumer base expansion – Successful Spanish language support enabled PropHero’s expansion into the Spanish consumer base with localized expertise. Conclusion The PropHero AI advisor demonstrates how AWS generative AI services can be used to create intelligent, context-aware conversational agents that deliver real business value. By combining a modular agent architecture with a robust evaluation system, PropHero has created a solution that enhances customer engagement while providing accurate and relevant responses.The comprehensive evaluation pipeline has been particularly valuable, providing clear metrics for measuring conversation quality and guiding ongoing improvements. This approach makes sure the AI advisor will continue to evolve and improve over time.For more information about building multi-agent AI advisors with continuous evaluation, refer to the following resources:\nRetrieve data and generate AI responses with Amazon Bedrock Knowledge Bases – With Amazon Bedrock Knowledge Bases, you can implement semantic search with chunking strategies LangGraph – LangGraph can help you build multi-agent workflows Ragas – Ragas offers comprehensive LLM evaluation metrics, including context relevance, groundedness, and goal accuracy used in this implementation To learn more about the Generative AI Innovation Center, get in touch with your account team.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.7-cicd-pipeline/5.7.1-create-gwe/","title":"Connect GitLab repo &amp; create CodeBuild project","tags":[],"description":"","content":"Objective Configure two CodeBuild projects (frontend and backend) and a trigger from GitLab Release events that starts CodePipeline. CodePipeline invokes CodeBuild using the repository’s existing frontend-buildspec.yml and backend-buildspec.yml, and then deploys to ECS.\nAWS Resources CodeBuild projects: Frontend: Source = CodePipeline; Buildspec = frontend-buildspec.yml Backend: Source = CodePipeline; Buildspec = backend-buildspec.yml CodePipeline (later step) consuming artifacts and deploying to ECS Create CodeBuild Projects \u0026amp; Connect GitLab Repository In the creating new CodeBuild Project configuration section, select Default project. In the Source section, choose GitLab and Band-Up repository. Leave default configurations for Environment. Specify Band-Up own buildspec for frontend/backend like the following image: Submit and create the other frontend/backend CodeBuild project likewise. Summary You now have two CodeBuild projects (frontend and backend) ready to be invoked by CodePipeline. A GitLab Release event can trigger CodePipeline, which then runs each project with its corresponding frontend-buildspec.yml and backend-buildspec.yml. In subsequent steps, CodePipeline will take the build artifacts and deploy to ECS.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives Complete Module 2, mastering Amazon EC2 and VPC fundamentals. Prepare and configure essential resources for launching EC2 instances. Explore Amazon Route 53 and DNS management concepts. Participate in Cloud Day event for AI and Data insights. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study VPC architecture and networking components in depth.\n- Learn AWS architecture design principles from Mentor Gia Hung\u0026rsquo;s lectures.\n- Complete AWS Skill Builder: Networking Essentials with Amazon VPC. 15/09/2025 16/09/2025 AWS VPC Documentation 3 - Create VPC resources to prepare for EC2 instance deployment.\n- Launch EC2 instances using configured resources.\n- Study Security Groups and Network ACLs.\n- Complete: Compute Essentials with Amazon EC2. 16/09/2025 17/09/2025 AWS EC2 Documentation Introduction to Amazon EC2 Deploying FCJ Management Application with Auto Scaling Group 4 - Resolve AWS account authentication issues by submitting verification documents.\n- Complete: Creating Your First AWS Account and Getting Help with AWS Support. 17/09/2025 20/09/2025 AWS Support Request Support with AWS Support 5 - Attend Cloud Day event.\n- Gain insights into AI and Data trends.\n- Network with prominent mentors in the AWS community. 18/09/2025 18/09/2025 Cloud Day Event AWS Skill Builder Courses Completed Course Category Status Creating Your First AWS Account Getting Started ✅ Managing Costs with AWS Budgets Cost Management ✅ Getting Help with AWS Support Support ✅ Access Management with AWS IAM Security ✅ Networking Essentials with Amazon VPC Networking ✅ Compute Essentials with Amazon EC2 Compute ✅ Instance Profiling with IAM Roles for EC2 Security ✅ Week 2 Achievements Technical Skills Acquired:\nGained comprehensive understanding of VPC architecture and EC2 fundamentals Mastered the process of preparing resources for EC2 instance deployment: Created and configured Subnets for network segmentation Set up Internet Gateway for external connectivity Configured Route Tables to manage traffic routing Implemented Security Groups to control inbound/outbound traffic Understood IAM roles and instance profiles for secure EC2 access Learned AWS cost management strategies using AWS Budgets Cloud Day Event Highlights:\nParticipated in networking sessions with AWS mentors and industry professionals Gained valuable insights into AI and Data market trends Understood the future potential and market demand for AI technologies Received commemorative gift from event organizers Key Takeaways:\nVPC is the foundation for all AWS networking - understanding it is critical Security Groups act as virtual firewalls at the instance level IAM roles eliminate the need for hardcoded credentials in EC2 instances AWS Budgets help prevent unexpected costs through proactive monitoring "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.3-network/5.3.2-alb/","title":"Application Load Balancer (ALB)","tags":[],"description":"","content":"The Application Load Balancer (ALB) serves as the entry point for all incoming traffic to our platform. It distributes requests to the appropriate containers (Frontend or Backend) and handles SSL termination.\n1. Create Security Group for ALB Before creating the load balancer, we need a firewall rule that allows public access.\nGo to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Security group name: alb-sg. Description: Allow http and https traffic. VPC: Select band-up-vpc. Inbound rules: Add the following rules to allow traffic from anywhere: Type: HTTP | Port: 80 | Source: Anywhere-IPv4 (0.0.0.0/0). Type: HTTPS | Port: 443 | Source: Anywhere-IPv4 (0.0.0.0/0). Click Create security group. 2. Create Target Group The ALB needs to know where to route the traffic. We will create a Target Group for our Frontend service first.\nGo to EC2 Dashboard \u0026gt; Target groups \u0026gt; Create target group. Choose a target type: Select IP addresses (Required for ECS Fargate). Target group name: target-bandup-fe. Protocol: HTTP. Port: 3000 (Our Next.js frontend runs on port 3000). VPC: Select band-up-vpc. Click Next. Register targets: Since we haven\u0026rsquo;t deployed the ECS tasks yet, skip this step and click Create target group. 3. Create Application Load Balancer Now, we aggregate everything into the Load Balancer.\nStep 1: Basic Configuration\nGo to Load Balancers \u0026gt; Create load balancer. Select Application Load Balancer and click Create. Load balancer name: bandup-public-alb. Scheme: Internet-facing (To allow public access). IP address type: IPv4. Step 2: Network Mapping\nVPC: Select band-up-vpc. Mappings: Select two Availability Zones (ap-southeast-1a and ap-southeast-1b). Subnets: IMPORTANT - Select the Public Subnets (public-subnet-1 and public-subnet-2) created in the previous section. Note: Ensure you do not select Private subnets, otherwise the ALB cannot be reached from the internet. Step 3: Security Groups \u0026amp; Listeners\nSecurity groups: Deselect the default one and choose alb-sg. Listeners and routing: Protocol: HTTP | Port: 80. Default action: Forward to target-bandup-fe. Click Create load balancer. Your ALB is now provisioning. Once active, it will be ready to route traffic to your frontend application.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.5-setup-be/5.5.2-rds/","title":"Create PostgreSQL RDS","tags":[],"description":"","content":"In this step, we provision an Amazon RDS for PostgreSQL instance. This will serve as the primary persistent data store for the IELTS BandUp platform. We will configure it for high availability and security within our VPC.\n1. Configure Security Groups Before creating the database, we need to define the firewall rules.\nStep 1.1: Create Backend Security Group This group is for the ECS Fargate tasks (the application layer) to control outbound traffic.\nName: ecs-backend-sg. Inbound: Allow port 8080 (Spring Boot default) from the ALB. Step 1.2: Create RDS Security Group This group is attached to the database itself.\nName: rds-sg. Inbound: Allow PostgreSQL traffic (Port 5432) only from the ecs-backend-sg created above (or the VPC CIDR for testing). This ensures only our application can talk to the database. 2. Create DB Subnet Group RDS needs to know which subnets it is allowed to use. We will group our private database subnets together.\nNavigate to Amazon RDS \u0026gt; Subnet groups \u0026gt; Create DB subnet group. Name: bandup-db-subnet-group. VPC: Select band-up-vpc. Add subnets: Select the Availability Zones and choose the dedicated private-database-subnet-1 and private-database-subnet-2. 3. Create the Database Now, we provision the PostgreSQL instance.\nNavigate to Databases \u0026gt; Create database. Choose a database creation method: Standard create. Engine options: PostgreSQL (Version 17.6 or latest). Availability and durability: Select Multi-AZ DB instance. This creates a primary DB and a synchronous standby replica in a different Availability Zone for automatic failover. Settings: DB instance identifier: bandup-db. Master username: postgres. Credential management: Self managed. Master password: Set a strong password (save this for later). Instance configuration: DB instance class: Burstable classes -\u0026gt; db.t4g.micro (Cost-effective for workshops/dev). Storage: gp3 (General Purpose SSD) with 20 GiB. Connectivity: Compute resource: Don\u0026rsquo;t connect to an EC2 compute resource. VPC: band-up-vpc. DB subnet group: bandup-db-subnet-group (Created in step 2). Public access: No (Critical for security). VPC security group: Select existing -\u0026gt; rds-sg. Database authentication: Password authentication. Monitoring: Enable Performance Insights (retention 7 days). Additional configuration: Initial database name: band_up (Important: Hibernate will look for this DB name). Backup: Enable automated backups. Encryption: Enable encryption. Click Create database. The provisioning process will take a few minutes. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"This workshop is designed for DevOps Engineers, Cloud Architects, and Full-stack Developers who aim to deploy a modern, AI-integrated application on AWS.\nTo successfully complete this workshop, participants are expected to possess the following knowledge, skills, and tools.\n1. Technical Knowledge Requirements AWS Fundamentals Console Navigation: Familiarity with the AWS Management Console. Core Services: Basic understanding of compute services like Amazon EC2 and AWS Fargate, networking concepts within Amazon VPC, and storage using Amazon S3. IAM \u0026amp; Security: Understanding of AWS Identity and Access Management (IAM), specifically Roles, Policies, and the principle of least privilege. Containerization \u0026amp; Orchestration Docker: Proficiency in creating Dockerfiles, building images, and running containers locally. Understanding of concepts like layers, exposing ports, and environment variables is essential. Refer to the Docker Documentation. ECS Concepts: Familiarity with Amazon ECS terminology including Task Definitions, Services, Clusters, and the operational differences between EC2 and Fargate launch types. DevOps \u0026amp; CI/CD Git: Proficiency in version control (commit, push, branching) to manage source code and trigger automated pipelines. CI/CD Flow: Understanding of Continuous Integration and Continuous Delivery principles using tools like AWS CodePipeline and AWS CodeBuild. Networking Basics Protocols: Understanding of HTTP/HTTPS, DNS resolution with Amazon Route 53, and load balancing concepts using Application Load Balancer. Network Security: Knowledge of IP addressing (CIDR blocks) and traffic control using Security Groups. 2. Environment Setup Before starting the workshop, ensure your local development environment is equipped with the following tools:\nAWS Account: An active AWS account with Administrator access to provision resources. IDE: A code editor such as Visual Studio Code or IntelliJ IDEA. Command Line Tools: AWS CLI (v2): Installed and configured with your account credentials. Installation Guide. Git: Installed for cloning repositories. Downloads. Docker Desktop: Running locally to inspect or build images if necessary. Get Docker. 3. Service Quotas \u0026amp; Costs Cost Alert: This workshop utilizes resources that are not covered by the AWS Free Tier, including:\nNAT Gateways (Hourly charge + Data processing fees) Application Load Balancers ECS Fargate Tasks (vCPU/Memory usage) Amazon RDS \u0026amp; ElastiCache Please ensure you clean up resources immediately after finishing the workshop to avoid unexpected charges. A cleanup guide is provided at the end of this documentation.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.4-setup-fe/5.4.2-ecr/","title":"Setup ECR &amp; IAM Role","tags":[],"description":"","content":"In this step, we prepare the AWS infrastructure required to store our container images. This involves verifying the initial state, creating a necessary IAM Role for ECR replication, and provisioning the repository.\n1. Verify ECR State First, verify the current state of the Private Registry. Initially, there are no repositories created.\n2. Create IAM Role for ECR We need to create a Service-Linked Role that allows Amazon ECR to perform replication actions across regions and accounts.\nNavigate to IAM \u0026gt; Roles \u0026gt; Create role. Select trusted entity: Choose AWS service. Service or use case: Select Elastic Container Registry from the list. Use case: Select Elastic Container Registry - Replication to allow ECR to replicate images. Add permissions: Confirm that the ECRReplicationServiceRolePolicy is attached. This managed policy grants the necessary permissions. Name, review, and create: The role name is automatically set to AWSServiceRoleForECRReplication. Review the configuration and create the role. Result: The role is successfully created and listed in the IAM Roles dashboard. 3. Create ECR Repository Now we create the repository to store the frontend image.\nNavigate to Amazon ECR \u0026gt; Create repository. General settings: Repository name: band-up-frontend. Visibility settings: Private. Image tag settings: Keep Mutable enabled to allow overwriting image tags. Result: The band-up-frontend repository is successfully created with AES-256 encryption enabled by default. 4. Configure CLI Access To push images from your local machine, you need programmatic access via the AWS CLI. We will generate an Access Key for your IAM User.\nNavigate to IAM Dashboard \u0026gt; Users \u0026gt; Select your user (e.g., NamDang). Open the Security credentials tab and click Create access key. Use case: Select Command Line Interface (CLI). Description tag: Enter a meaningful description (e.g., ECR Push Key) and click Create access key. Retrieve Keys: Important! Copy or download the Access Key ID and Secret Access Key immediately, as you cannot retrieve the Secret Key later. 5. Configure AWS CLI Open your terminal and configure the AWS CLI with the credentials you just generated.\naws configure Enter the following details when prompted:\nAWS Access Key ID: [Paste your key] AWS Secret Access Key: [Paste your secret] Default region name: ap-southeast-1 Default output format: json 6. Push Image to ECR Now that the CLI is configured, we can authenticate Docker and push our image.\nStep 1: Login to ECR Run the login command to authenticate your Docker client with the AWS registry.\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [Account-ID]https://www.google.com/search?q=.dkr.ecr.ap-southeast-1.amazonaws.com Output: Login Succeeded\nStep 2: Tag the Image We need to tag our local image band-up-frontend:latest with the full ECR repository URI and a version tag (e.g., v1.0.0).\ndocker tag band-up-frontend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 Step 3: Push the Image Execute the push command to upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 7. Final Verification Return to the Amazon ECR Console and open the band-up-frontend repository. You should see the image with the tag v1.0.0 listed successfully.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.7-cicd-pipeline/5.7.2-test-gwe/","title":"Create CodePipeline with GitLab tag trigger","tags":[],"description":"","content":"CodePipeline Design (Source -\u0026gt; Build → Deploy) Build (CodeBuild) Project: the project’s CodeBuild project (Source = CodePipeline) Environment variables: as needed for this project’s build Buildspec: use the repository’s existing buildspec.yml CodePipeline Set up guidelines On choosing pipeline creation option section, choose Build custom pipeline. In pipeline settings tab, specify the pipeline name and use default settings for the pipeline. Turn on webhook events and add filter type of tags. Make sure to set the tag patterns of \u0026ldquo;v*\u0026rdquo;. Add frontend/backend project using AWS CodeBuild in build stage. For the deploy stage, choose Amazon ECS as deploy provider. Specify its cluster and service to deploy. Also make sure to fill in the image definition file form like the image. Submit and then the pipeline is now created. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"IELTS Self-Learning Web System An Intelligent Platform for Independent IELTS Preparation 1. Executive Summary The IELTS Self-Learning Web System is a comprehensive online platform designed to support students in their independent IELTS preparation journey. The platform provides an all-in-one solution featuring user management, educational blogs, interactive study rooms, practice tests, and AI-powered flashcard systems. Leveraging modern web technologies and AI integration, the system offers personalized learning experiences, real-time collaboration features, and automated assessment tools to help students achieve their IELTS goals efficiently.\n2. Problem Statement What\u0026rsquo;s the Problem? Many IELTS learners face challenges in finding affordable, comprehensive, and interactive self-study platforms. Traditional learning methods lack real-time collaboration, personalized feedback, and integrated practice tools. Students often struggle with:\nLimited access to quality practice materials and mock tests Lack of immediate feedback on Speaking and Writing tasks Difficulty finding study partners and maintaining study discipline Fragmented resources across multiple platforms High costs of traditional IELTS preparation courses The Solution The IELTS Self-Learning Web System provides a unified platform with five core features:\nUser Management System: Multi-tier membership (Guest, Member, Premium, Admin) with social authentication (Google, Facebook), personal profiles, and messaging capabilities.\nEducational Blog Platform: Community-driven content with CRUD operations, genre/tag categorization, advanced filtering, commenting, reporting, and favoriting features.\nInteractive Study Rooms: Virtual study spaces with scheduling, voice/video calls, Pomodoro timers, background music, dictionary integration, and translation support for enhanced learning.\nIELTS Practice Tests: Comprehensive mock tests for Reading and Listening with automatic vocabulary extraction into flashcards, plus AI-powered assessment for Speaking and Writing tasks, and dictation exercises.\nQuizlet Flashcard System: Intelligent flashcard creation with multiple study modes, automatic vocabulary extraction from texts, AI-generated quizzes from uploaded materials, and sharing capabilities.\nBenefits and Return on Investment For Students: Cost-effective alternative to expensive courses, personalized learning paths, 24/7 access to study materials, AI-powered feedback, and collaborative learning environment. Educational Value: Develops self-discipline, provides comprehensive IELTS preparation, enables peer learning, and offers trackable progress. Technical Benefits: Scalable architecture, modern tech stack, AI integration for automated assessment, and potential for future feature expansion. Market Potential: Growing demand for online IELTS preparation, subscription-based revenue model (Guest → Member → Premium), and potential for partnerships with educational institutions. 3. Solution Architecture The platform employs a modern full-stack web architecture designed for scalability, real-time collaboration, and AI integration. The system consists of five major modules working together to provide a comprehensive IELTS learning experience. The infrastructure uses an active-passive Multi-AZ deployment on AWS ECS for high availability, where AZ-1 handles all active traffic and AZ-2 serves as a standby for automatic failover.\nSystem Architecture Overview: 3.1. Architecture Overview The IELTS Self-Learning Web System is built on AWS cloud infrastructure with a multi-layered architecture that ensures high availability, scalability, and security. The system follows a microservices-inspired approach with a monolithic backend for core services and serverless architecture for AI-powered features.\nKey Architecture Principles:\nHigh Availability: Active-passive Multi-AZ deployment with automatic failover Scalability: Horizontal scaling with ECS Auto Scaling and serverless Lambda functions Security: Multi-layer security with WAF, VPC isolation, and encryption at rest and in transit Performance: CDN distribution, caching layers, and optimized database queries Cost Efficiency: Serverless AI services, active-passive standby resources, and pay-per-use pricing 3.2. Network Architecture The network infrastructure is built within a Virtual Private Cloud (VPC) spanning two Availability Zones (AZ-1 and AZ-2) for redundancy and high availability.\nVPC Structure:\nPublic Subnets (AZ-1 \u0026amp; AZ-2): Application Load Balancer (ALB) endpoints NAT Gateways for outbound internet access Bastion hosts for secure access (optional) Private Subnets (AZ-1 \u0026amp; AZ-2): ECS Fargate tasks (Next.js frontend and Spring Boot backend) Amazon RDS PostgreSQL instances Amazon ElastiCache Redis clusters Internal service communication Network Components:\nRoute 53: DNS management and domain routing AWS Certificate Manager (ACM): SSL/TLS certificates for HTTPS AWS WAF: Web Application Firewall protecting against common web exploits Internet Gateway: Public internet access for public subnets NAT Gateways: Outbound internet access for private subnets Security Groups: Stateful firewall rules controlling traffic between components Network ACLs: Additional layer of subnet-level security Traffic Flow:\nUser requests → Route 53 → CloudFront CDN (for static assets) API requests → Route 53 → AWS WAF → Application Load Balancer ALB routes to ECS tasks in private subnets (AZ-1 active, AZ-2 standby) ECS tasks communicate with RDS, ElastiCache, and S3 via private subnets AI service requests → API Gateway → SQS → Lambda functions 3.3. Application Architecture The application layer consists of frontend and backend services deployed on Amazon ECS (Fargate) with an active-passive Multi-AZ configuration.\nFrontend Layer (Next.js):\nDeployment: Containerized Next.js application on ECS Fargate Location: Private subnets in AZ-1 (active) and AZ-2 (standby) Features: Server-side rendering (SSR) for SEO optimization Static asset delivery via CloudFront CDN Real-time WebRTC for study room video/voice calls Socket.io client for real-time messaging Responsive design for mobile and desktop Backend Layer (Spring Boot):\nDeployment: Monolithic Spring Boot REST API on ECS Fargate Location: Private subnets in AZ-1 (active) and AZ-2 (standby) Components: RESTful API endpoints for all modules Spring Security for authentication and authorization Spring WebSocket for real-time features JWT token management OAuth 2.0 integration (Google, Facebook) File upload handling for S3 integration Load Balancing \u0026amp; High Availability:\nApplication Load Balancer (ALB): Routes traffic to healthy ECS tasks in AZ-1 (active) Health checks every 30 seconds Automatic failover to AZ-2 if AZ-1 becomes unavailable SSL/TLS termination with ACM certificates Sticky sessions for WebSocket connections Container Orchestration:\nAmazon ECS (Fargate): No server management required Auto Scaling based on CPU/memory utilization Task definitions for Next.js and Spring Boot containers Service discovery and load balancing integration Container images stored in Amazon ECR 3.4. Data Architecture The data layer uses a combination of relational and NoSQL databases, along with caching for optimal performance.\nPrimary Database:\nAmazon RDS PostgreSQL (Multi-AZ): Primary Instance: db.t3.medium in AZ-1 (active) Standby Instance: db.t3.medium in AZ-2 (passive, synchronous replication) Data: Users, blogs, study sessions, practice tests, flashcards metadata Backup: Automated daily backups with 7-day retention High Availability: Automatic failover to standby in \u0026lt; 60 seconds Encryption: At rest (AES-256) and in transit (SSL/TLS) Caching Layer:\nAmazon ElastiCache (Redis): Session management and user authentication tokens Frequently accessed data caching (blog posts, user profiles) Real-time leaderboards and statistics Rate limiting and API throttling Cache invalidation strategies for data consistency Object Storage:\nAmazon S3: User-uploaded files (images, documents, audio recordings) Static assets (before CloudFront distribution) Backup storage for RDS snapshots Lifecycle policies for cost optimization (move to Glacier after 90 days) Versioning enabled for critical files NoSQL Database (AI Services):\nAmazon DynamoDB: AI assessment results (Writing and Speaking scores) Generated flashcards from RAG pipeline User progress tracking for AI features On-demand scaling for variable workloads Point-in-time recovery enabled Vector Store (RAG):\nAmazon OpenSearch Service (or Amazon Bedrock Knowledge Base): Document embeddings generated by Amazon Titan V2 Semantic search for relevant document chunks Vector similarity search for flashcard generation Index management and query optimization 3.5. Security Architecture Multi-layer security ensures data protection and system integrity at every level.\nNetwork Security:\nAWS WAF: Protection against SQL injection, XSS, DDoS attacks Security Groups: Restrictive firewall rules (least privilege principle) Network ACLs: Subnet-level traffic filtering VPC Isolation: Private subnets with no direct internet access VPN/Bastion Hosts: Secure administrative access (optional) Data Security:\nEncryption at Rest: RDS: AES-256 encryption S3: Server-side encryption (SSE-S3) DynamoDB: Encryption at rest enabled ElastiCache: Encryption in transit and at rest Encryption in Transit: HTTPS/TLS 1.2+ for all external communications SSL/TLS for database connections ACM certificates for all domains Identity \u0026amp; Access Management:\nAWS IAM: Role-based access control for AWS services ECS tasks use IAM roles (no hardcoded credentials) Lambda functions have minimal required permissions S3 bucket policies for access control Spring Security: Application-level authentication JWT tokens for stateless authentication OAuth 2.0 for social login (Google, Facebook) Role-based access control (Guest, Member, Premium, Admin) AWS Secrets Manager: Secure storage of API keys and credentials Database passwords Third-party API keys (Gemini, dictionary APIs) OAuth client secrets Application Security:\nInput Validation: All user inputs validated and sanitized SQL Injection Prevention: Parameterized queries with Spring Data JPA XSS Protection: Content Security Policy (CSP) headers CSRF Protection: Spring Security CSRF tokens Rate Limiting: API throttling with Redis Audit Logging: CloudWatch Logs for security events 3.6. CI/CD Pipeline Automated deployment pipeline ensures consistent and reliable releases.\nSource Control:\nGit Repository: Code versioning and collaboration Branch Strategy: Main branch for production, feature branches for development CI/CD Components:\nGitLab Webhook (or GitHub Actions): Triggers pipeline on code push AWS CodePipeline: Orchestrates the deployment workflow AWS CodeBuild: Builds and tests application code Frontend: npm install \u0026amp;\u0026amp; npm run build Backend: mvn clean package Docker image creation and tagging Amazon ECR: Container registry for Docker images Stores Next.js and Spring Boot container images Image versioning and lifecycle policies Deployment Flow:\nDeveloper pushes code to repository Webhook triggers CodePipeline CodeBuild compiles and tests code CodeBuild creates Docker images and pushes to ECR ECS service updates with new task definitions Rolling deployment: New tasks start, health checks pass, old tasks terminate CloudWatch monitors deployment success/failure Infrastructure as Code:\nAWS CloudFormation (or Terraform): Infrastructure provisioning VPC, subnets, security groups ECS clusters, services, task definitions RDS instances, ElastiCache clusters Lambda functions, API Gateway, SQS queues IAM roles and policies 3.7. Monitoring \u0026amp; Observability Comprehensive monitoring ensures system health and performance.\nApplication Monitoring:\nAmazon CloudWatch: Metrics: CPU, memory, network utilization for ECS tasks Logs: Application logs from Next.js and Spring Boot Alarms: Automated alerts for errors, high latency, resource exhaustion Dashboards: Custom dashboards for key metrics AWS X-Ray (optional): Distributed tracing for request flow analysis Infrastructure Monitoring:\nCloudWatch Metrics: RDS: CPU, memory, connection count, read/write latency ElastiCache: CPU, memory, cache hit ratio ALB: Request count, response time, error rates Lambda: Invocations, duration, errors, throttles SQS: Queue depth, message age, DLQ size API Gateway: Request count, latency, 4xx/5xx errors Logging:\nCloudWatch Logs: Application logs (structured JSON format) Access logs from ALB API Gateway execution logs Lambda function logs Log retention: 30 days (configurable) Alerting:\nCloudWatch Alarms: High error rate (\u0026gt; 5% 5xx errors) High latency (\u0026gt; 2 seconds p95) Low availability (\u0026lt; 99% uptime) Resource exhaustion (CPU \u0026gt; 80%, Memory \u0026gt; 85%) Database connection pool exhaustion SQS queue depth exceeding threshold SNS Notifications: Email/SMS alerts for critical alarms Performance Optimization:\nCloudFront CDN: Caching static assets at edge locations ElastiCache: Reducing database load with intelligent caching Database Query Optimization: Indexed queries, connection pooling Auto Scaling: ECS tasks scale based on CPU/memory metrics Technology Stack Frontend:\nNext.js 14+: Modern React framework for responsive web application TypeScript: Type-safe development TailwindCSS: Utility-first styling WebRTC: Real-time video/voice communication Socket.io Client: Real-time messaging and collaboration Backend:\nSpring Boot 3.x: Monolithic RESTful API architecture Java 17+: Backend programming language Spring Security: Authentication and authorization Spring WebSocket: Real-time communication JWT: Secure token-based authentication OAuth 2.0: Social login integration (Google, Facebook) Database:\nPostgreSQL 14+: Primary relational database for all data (users, tests, blogs, flashcards, study sessions) Amazon ElastiCache (Redis): Caching and session management Cloud Infrastructure (AWS):\nAmazon ECS (Elastic Container Service): Container orchestration with active-passive Multi-AZ deployment Application Load Balancer: Routes traffic to active AZ with automatic failover Amazon RDS for PostgreSQL: Multi-AZ deployment (active primary in AZ-1, passive standby in AZ-2) Amazon S3: Media and file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Monitoring and logging Amazon API Gateway: RESTful API endpoint for AI service requests Amazon SQS: Message queue for asynchronous AI processing with Dead Letter Queue (DLQ) AWS Lambda: Serverless functions for AI-powered assessments and flashcard generation Amazon DynamoDB: NoSQL database for storing AI assessment results and generated flashcards Amazon Bedrock: AI model service for embeddings (Titan V2) and content generation Amazon OpenSearch Service (optional): Vector store for RAG-based document embeddings and semantic search Third-party Services:\nGoogle Gemini Flash API (Free Tier): AI-powered Speaking/Writing assessment and smart query generation for RAG Amazon Bedrock (GPT-OSS): Alternative AI model for assessments and content generation Amazon Titan V2 Embeddings: Vector embeddings for document chunks in RAG-based flashcard generation Free Dictionary APIs: Word definitions and examples Open-source translation libraries: Context-aware translation (alternative to paid APIs) 3.9. Component Design The system consists of five major functional modules, each integrated with the core architecture described above. Each module leverages the shared infrastructure (ECS, RDS, ElastiCache, S3) while maintaining clear separation of concerns.\n1. User Management Module:\nMulti-tier authentication system (Guest, Member, Premium, Admin) Social OAuth integration (Google, Facebook) User profile management with learning statistics Real-time messaging system Password recovery and email verification 2. Blog Platform Module:\nCRUD operations for blog posts Genre and tag management system Advanced search and filtering (by tags, genres, date, popularity) Comment system with nested replies Content reporting and moderation Favorite/bookmark functionality SEO-optimized content delivery 3. Study Room Module:\nVirtual study room creation and management Study session scheduling with calendar integration WebRTC-based voice and video calls Pomodoro timer with customizable intervals Background music library with focus playlists Integrated dictionary with free dictionary APIs Translation support using open-source libraries Screen sharing for collaborative study Real-time participant management 4. IELTS Practice Test Module:\nTest bank management (Reading, Listening, Speaking, Writing) - stored in PostgreSQL Timed test simulations with auto-submission Automatic vocabulary extraction from Reading passages to flashcards AI-powered Speaking assessment (see Section 3.8, Lambda Function 2): Audio recordings uploaded to S3 Processed asynchronously via API Gateway → SQS → Lambda Function 2 Uses Amazon Transcribe for speech-to-text Evaluates pronunciation, fluency, coherence, lexical resource Results stored in DynamoDB and displayed to user AI-powered Writing assessment (see Section 3.8, Lambda Function 1): Writing samples submitted via API Gateway → SQS → Lambda Function 1 Evaluates grammar, vocabulary, task achievement, coherence Results stored in DynamoDB with detailed feedback Dictation exercises for Listening practice Detailed score reports and analytics (aggregated from DynamoDB) Progress tracking across test types (stored in PostgreSQL, cached in Redis) 5. Quizlet Flashcard Module:\nCRUD operations for flashcard sets (stored in PostgreSQL) Multiple study modes (flashcards, learn, test, match, write) Spaced repetition algorithm (SRS) for optimized learning Automatic vocabulary extraction from text passages AI-generated flashcards from uploaded documents using RAG (see Section 3.8) Documents uploaded to S3 Processed asynchronously via API Gateway → SQS → Lambda Function 3 Generated flashcards stored in DynamoDB and linked to user\u0026rsquo;s sets Collaborative flashcard sets with sharing Study statistics and mastery tracking (cached in Redis) Import/export functionality 3.8. AI Service Architecture (Serverless) The AI service is implemented as a fully serverless architecture using AWS services to handle AI-powered assessments and content generation. This architecture follows an asynchronous processing pattern for scalability, cost-efficiency, and reliability.\nArchitecture Components:\n1. API Gateway (Entry Point)\nPurpose: RESTful API endpoint for AI service requests Configuration: REST API with custom domain (optional) API keys for rate limiting and access control CORS configuration for frontend integration Request/response transformation Integration with SQS for asynchronous processing Endpoints: POST /ai/writing-assessment - Submit writing samples POST /ai/speaking-assessment - Submit audio recordings POST /ai/generate-flashcards - Upload documents for flashcard generation Security: IAM authentication or API keys 2. Amazon SQS (Message Queue)\nPurpose: Decouples API Gateway from Lambda functions for asynchronous processing Configuration: Standard queue for high throughput Message retention: 14 days Visibility timeout: 5 minutes (adjustable per function) Dead Letter Queue (DLQ) for failed messages after 3 retries Message Format: JSON payloads containing: User ID and request metadata File references (S3 URIs for documents/audio) Processing parameters Benefits: Handles traffic spikes without overwhelming Lambda Automatic retry mechanism for transient failures Cost-effective message queuing 3. AWS Lambda Functions (Processing Layer) Three specialized Lambda functions process different types of AI tasks, each optimized for its specific workload:\nLambda Function 1: Writing Assessment\nTrigger: SQS queue messages for writing assessments Runtime: Python 3.11 or Node.js 18.x Memory: 512 MB - 1 GB (configurable) Timeout: 5 minutes Processing Flow: Receives writing sample from SQS message Retrieves document from S3 if needed Calls Amazon Bedrock (GPT-OSS) or Google Gemini Flash API with prompt: Task description and IELTS band descriptors Writing sample text Evaluation criteria (grammar, vocabulary, task achievement, coherence) Parses AI response for structured assessment: Overall band score (0-9) Detailed scores per criterion Feedback comments and suggestions Stores results in DynamoDB with: User ID, timestamp, writing sample reference Assessment scores and feedback Processing metadata Sends notification (optional) via SNS or updates user via WebSocket Lambda Function 2: Speaking Assessment\nTrigger: SQS queue messages for speaking assessments Runtime: Python 3.11 or Node.js 18.x Memory: 1 GB - 2 GB (for audio processing) Timeout: 15 minutes (for longer audio files) Processing Flow: Receives audio file reference from SQS message Retrieves audio file from S3 Audio Transcription (if needed): Uses Amazon Transcribe for speech-to-text conversion Supports multiple languages and accents Generates transcript with timestamps Calls Amazon Bedrock (GPT-OSS) or Google Gemini Flash API with: Transcript text Audio metadata (duration, sample rate) IELTS speaking band descriptors Evaluation criteria (pronunciation, fluency, coherence, lexical resource) Parses AI response for structured assessment: Overall band score (0-9) Detailed scores per criterion Pronunciation analysis Fluency and coherence feedback Stores results in DynamoDB with: User ID, timestamp, audio file reference Transcript and assessment scores Detailed feedback Sends notification or updates user status Lambda Function 3: Flashcard Generation (RAG-based)\nTrigger: SQS queue messages for document uploads\nRuntime: Python 3.11 (for ML/AI libraries)\nMemory: 2 GB - 3 GB (for document processing)\nTimeout: 15 minutes (for large documents)\nProcessing Flow:\nStep 1: Document Processing \u0026amp; Embedding\nReceives document file reference from SQS Retrieves document from S3 (PDF, DOCX, TXT formats) Document Chunking: Splits document into semantic chunks (500-1000 tokens each) Preserves context and structure Handles tables, lists, and formatted content Vector Embedding Generation: Uses Amazon Titan V2 Embeddings model via Bedrock Generates 1024-dimensional vectors for each chunk Maintains chunk metadata (position, section, page number) Vector Storage: Stores embeddings in Amazon OpenSearch Service (vector index) Alternative: Amazon Bedrock Knowledge Base (managed service) Index structure: Document ID, chunk ID, embedding vector, metadata Step 2: Smart Query Generation\nUses Google Gemini Flash API to analyze document content Query Generation Process: Analyzes document structure and content Identifies key concepts, important facts, learning points Generates 10-20 intelligent, context-aware queries Queries designed to extract: Definitions and explanations Important dates, facts, and figures Concepts and relationships Examples and use cases Query Optimization: Ensures queries are specific and answerable Covers different difficulty levels Balances factual and conceptual questions Step 3: RAG-based Flashcard Generation\nRetrieval Phase: For each generated query, performs semantic search in vector store Retrieves top 3-5 most relevant document chunks Uses cosine similarity for vector matching Augmentation Phase: Combines query with retrieved chunks Adds context and metadata Formats prompt for flashcard generation Generation Phase: Sends query-context pairs to Google Gemini Flash or Amazon Bedrock Prompt includes: Query/question to answer Relevant document chunks Flashcard format requirements (question-answer pairs) Educational guidelines Model generates flashcards that are: Relevant to document content Contextually accurate Educationally valuable Properly formatted (question-answer pairs) Appropriate difficulty level Post-processing: Validates flashcard quality Removes duplicates Formats for storage Storage: Stores generated flashcards in DynamoDB Associates with user\u0026rsquo;s flashcard sets Links to original document Stores metadata (generation date, source chunks) 4. Amazon DynamoDB (Data Storage)\nPurpose: Stores AI assessment results and generated flashcards Table Design: WritingAssessments Table: Partition Key: userId (String) Sort Key: timestamp (Number) Attributes: writingSample, scores, feedback, metadata SpeakingAssessments Table: Partition Key: userId (String) Sort Key: timestamp (Number) Attributes: audioFile, transcript, scores, feedback, metadata GeneratedFlashcards Table: Partition Key: userId (String) Sort Key: flashcardId (String) Attributes: question, answer, sourceDocument, sourceChunks, metadata Features: On-demand scaling for variable workloads Point-in-time recovery enabled Encryption at rest TTL for automatic cleanup of old data 5. Amazon Bedrock (AI Model Service)\nModels Used: Amazon Titan V2 Embeddings: Vector embeddings for document chunks GPT-OSS (Open Source): Alternative model for assessments and generation Integration: Direct API calls from Lambda functions Cost: Pay-per-use pricing model 6. Amazon OpenSearch Service (Vector Store)\nPurpose: Semantic search and retrieval for RAG pipeline Configuration: Vector index for document embeddings KNN (k-nearest neighbors) search capability Index management and optimization Alternative: Amazon Bedrock Knowledge Base (fully managed) 7. Amazon CloudWatch (Monitoring)\nLambda Metrics: Invocations, duration, errors, throttles Memory utilization, concurrent executions SQS Metrics: Queue depth, message age DLQ message count API Gateway Metrics: Request count, latency, 4xx/5xx errors Alarms: Automated alerts for: High error rates Queue depth exceeding threshold Lambda timeouts API Gateway throttling Data Flow Summary:\nUser submits request → API Gateway (validates and queues) Request queued in SQS (decoupled processing) Lambda function triggered by SQS message Lambda processes request using AI services (Bedrock/Gemini) Results stored in DynamoDB User notified or results retrieved via API CloudWatch monitors entire flow Benefits of Serverless Architecture:\nScalability: Auto-scales with request volume Cost Efficiency: Pay only for actual usage Reliability: Built-in retry mechanisms and DLQ Maintenance: No server management required Performance: Low latency with optimized cold starts 4. Technical Implementation Implementation Phases\nPhase 1: Planning and Design (Weeks 1-2)\nRequirements gathering and user story definition System architecture design and AWS infrastructure planning Database schema design for PostgreSQL UI/UX wireframes and mockups API endpoint design for Spring Boot Third-party service evaluation and integration planning Multi-AZ architecture setup on AWS ECS Phase 2: Core Development (Weeks 3-6)\nSpring Boot application setup with monolithic architecture User authentication and authorization with Spring Security PostgreSQL database setup on Amazon RDS (Multi-AZ: active-passive) JWT and OAuth 2.0 integration (Google, Facebook) Next.js frontend setup with TypeScript Frontend component library creation Basic CRUD operations for all modules AWS ECS cluster configuration with active-passive failover Phase 3: Feature Development (Weeks 7-10)\nBlog platform with advanced filtering and search Study room creation with WebRTC integration Practice test module with automatic grading logic Flashcard system with spaced repetition algorithm Real-time messaging with Spring WebSocket Dictionary and Google Translate API integration Amazon S3 integration for file uploads ElastiCache Redis for session management and caching Phase 4: AI Integration \u0026amp; Deploying (Weeks 11-12)\nAI Service Infrastructure Setup: Amazon API Gateway configuration for AI endpoints Amazon SQS queue setup with Dead Letter Queue (DLQ) AWS Lambda function development (3 functions) Amazon DynamoDB table design for AI results storage Lambda Function 1: Writing Assessment Integration with Google Gemini Flash API or Amazon Bedrock Writing evaluation logic (grammar, vocabulary, task achievement, coherence) Result storage in DynamoDB Lambda Function 2: Speaking Assessment Audio transcription integration Integration with Google Gemini Flash API or Amazon Bedrock Speaking evaluation logic (pronunciation, fluency, coherence, lexical resource) Result storage in DynamoDB Lambda Function 3: RAG-based Flashcard Generation Document chunking algorithm implementation Amazon Titan V2 Embeddings integration for vector generation Vector store setup (Amazon OpenSearch Service or Bedrock Knowledge Base) Google Gemini integration for smart query generation RAG pipeline: Query generation → Document retrieval → Flashcard generation Flashcard storage in DynamoDB Automatic vocabulary extraction algorithms Comprehensive testing (unit, integration, end-to-end) Performance optimization and load testing Security audit and bug fixes Production deployment to AWS ECS with Multi-AZ Monitoring setup with CloudWatch for Lambda, SQS, and API Gateway Technical Requirements\nDevelopment Environment:\nJava 17+ for Spring Boot backend Node.js 18+ for Next.js frontend PostgreSQL 14+ for database Docker for local containerization Git for version control Maven for Java dependency management Frontend Requirements:\nNext.js 14+ with App Router and TypeScript WebRTC for real-time video/voice communication Socket.io client for real-time features Form validation (React Hook Form + Zod) Backend Requirements:\nSpring Boot 3.x with Java 17+ Spring Data JPA for database operations Spring Security for authentication/authorization Spring WebSocket for real-time features Spring Web for RESTful APIs JWT for token-based authentication OAuth 2.0 for social login Multipart file upload handling AWS Infrastructure:\nAmazon ECS: Fargate launch type for containerized applications Application Load Balancer: Routes traffic to active AZ with health checks Amazon RDS PostgreSQL: Multi-AZ active-passive deployment for high availability Amazon ElastiCache (Redis): Session and cache management Amazon S3: Media file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Logging and monitoring Amazon VPC: Network isolation with public/private subnets across 2 AZs AWS Certificate Manager: SSL/TLS certificates Amazon API Gateway: RESTful API for AI service endpoints Amazon SQS: Message queue for asynchronous AI processing with DLQ AWS Lambda: Serverless compute for AI functions (3 functions: Writing, Speaking, Flashcard Generation) Amazon DynamoDB: NoSQL database for AI assessment results and flashcards Amazon Bedrock: AI model service (Titan V2 Embeddings, GPT-OSS) Amazon OpenSearch Service (optional): Vector store for RAG document embeddings AI/ML Integration:\nGoogle Gemini Flash API (Free Tier): Writing assessment (grammar, vocabulary, task achievement) Speaking assessment (pronunciation, fluency, coherence) Smart query generation for RAG-based flashcard creation Amazon Bedrock (GPT-OSS): Alternative AI model for assessments Flashcard generation from query-context pairs Amazon Titan V2 Embeddings: Vector embeddings for document chunks Semantic search and retrieval for RAG RAG (Retrieval-Augmented Generation) Architecture: Document chunking and embedding generation Vector store for semantic search (OpenSearch Service or Bedrock Knowledge Base) Context-aware flashcard generation using smart queries + document chunks 5. Timeline \u0026amp; Milestones Project Timeline: 3 Months (12 Weeks)\nWeeks 1-2: Planning \u0026amp; Design\n✓ Requirements analysis and documentation ✓ AWS Multi-AZ architecture design ✓ PostgreSQL database schema design ✓ UI/UX mockups completion ✓ Spring Boot project structure setup Deliverable: Complete technical specification document and AWS infrastructure plan Weeks 3-6: Core Development\n✓ Spring Boot monolithic backend setup ✓ Spring Security implementation (JWT, OAuth 2.0) ✓ User management and role-based access control ✓ Amazon RDS PostgreSQL Multi-AZ deployment ✓ Next.js frontend framework and component library ✓ AWS ECS cluster setup with Application Load Balancer ✓ ElastiCache Redis configuration Deliverable: Working authentication system and AWS infrastructure Weeks 7-10: Feature Development\n✓ Blog platform with full CRUD functionality ✓ Study room creation and management ✓ WebRTC integration for video/voice calls Practice test module (Reading \u0026amp; Listening) Flashcard system with study modes Amazon S3 integration for media storage Free dictionary API and open-source translation integration Spring WebSocket for real-time features Deliverable: All five core features operational (without AI) Weeks 11-12: AI Integration \u0026amp; Deployment\n✓ Amazon API Gateway and SQS setup for AI service architecture ✓ AWS Lambda Function 1: Writing Assessment with Gemini/Bedrock integration ✓ AWS Lambda Function 2: Speaking Assessment with Gemini/Bedrock integration ✓ AWS Lambda Function 3: RAG-based Flashcard Generation Document chunking and Amazon Titan V2 Embeddings integration Google Gemini smart query generation RAG pipeline with vector retrieval and flashcard generation ✓ Amazon DynamoDB setup for AI results storage ✓ Amazon OpenSearch Service or Bedrock Knowledge Base for vector store ✓ Vocabulary extraction algorithms ✓ Comprehensive testing (unit, integration, E2E) ✓ Performance optimization and security audit ✓ Production deployment to AWS ECS Multi-AZ ✓ CloudWatch monitoring and alerting setup for Lambda, SQS, API Gateway ✓ Final bug fixes and documentation Deliverable: Production-ready application deployed on AWS with complete AI service architecture Key Milestones:\nWeek 2: Technical specification and AWS architecture approved Week 6: Core backend and infrastructure deployed Week 10: All features complete (beta version) Week 12: Production launch with AI integration Weekly Sprint Goals:\nSprint 1-2: Architecture and setup Sprint 3-4: Authentication and database Sprint 5-6: Core API and AWS deployment Sprint 7-8: Blog and study room features Sprint 9-10: Practice tests and flashcards Sprint 11: AI integration and testing Sprint 12: Production deployment and launch 6. Budget Estimation Development Costs (One-time)\nSoftware \u0026amp; Tools:\nDevelopment tools and licenses: $0 (using free/open-source tools) Design tools (Figma Free): $0 Project management tools: $0 (using free tier) Total Software: $0 Initial Setup:\nDomain registration: $1/year SSL certificate: $0 (AWS Certificate Manager - Free) Development servers: $0 (local development) Total Initial Setup: $1 Operating Costs (Monthly)\nInfrastructure \u0026amp; Hosting (AWS):\nAWS Infrastructure \u0026amp; Hosting Third-party Services:\nGoogle Gemini Flash API: Free tier Subtotal Services: $0/month Other Operating Costs:\nMonitoring \u0026amp; analytics: $10/month (integrated with CloudWatch) Backup storage (RDS automated backups): $5/month Domain \u0026amp; SSL renewal: $0.08/month (amortized from $1/year, SSL via AWS Certificate Manager - Free) Subtotal Other: $15.08/month Total Monthly Operating Costs: $103.66/month\nAnnual Budget Summary\nDevelopment Phase (3 Months):\nDevelopment costs: $1 (one-time, domain only) Operating costs (3 months): $310.98 ($103.66 × 3 months) Total Development Phase: $311.98 Year 1 (After Launch):\nDevelopment costs: $1 (one-time, domain only) Operating costs: $1,243.92 ($103.66 × 12 months) Total Year 1: $1,244.92 Year 2+ (Annual Recurring):\nOperating costs: $1,243.92/year Domain renewal: $1/year Total Annual: $1,244.92 Revenue Projections (Subscription Model)\nMembership Tiers:\nGuest: Free (limited features) Member: $5/month (basic features) Premium: $15/month (all features + AI assessments) Conservative Revenue Estimate (Year 1):\nMonth 6-12: Average 100 Premium + 200 Members Revenue: (100 × $15 + 200 × $5) × 7 months = $17,500 Operating costs (Year 1): $1,243.92 Net Profit Year 1: $16,256.08 Break-even: Month 1 after launch Optimistic Revenue Estimate (Year 2):\n500 Premium + 1,000 Members Monthly revenue: $12,500 Annual revenue: $150,000 Operating costs: $1,243.92 Net Profit Year 2: $148,756.08 Profit margin: ~97.6% after operating costs Cost Optimization Strategies:\nZero personnel costs (self-developed project) Active-passive Multi-AZ deployment reduces costs (standby resources only used during failover) Use AWS ECS Fargate Spot for development environments (70% cost savings) Implement caching with ElastiCache to reduce database queries Use CloudFront CDN to minimize data transfer costs Leverage free tier of Google Gemini Flash API for AI features Use AWS Lambda for cost-effective serverless AI processing (pay per request) Amazon SQS provides reliable message queuing with minimal cost Amazon Bedrock Titan V2 Embeddings offers competitive pricing for vector generation DynamoDB on-demand pricing scales with usage Utilize free dictionary APIs and open-source translation libraries Free development tools and design software (VS Code, Figma Free, etc.) Leverage AWS Free Tier during initial development RDS automated backups included (7-day retention) Use AWS Certificate Manager for free SSL certificates Implement S3 lifecycle policies to move old data to cheaper storage tiers No email service costs by deferring email features to later phase Standby ECS tasks in AZ-2 kept minimal until failover needed 7. Risk Assessment Risk Matrix High Priority Risks:\nAI API Cost Overruns\nImpact: Low | Probability: Low Description: Google Gemini Flash API free tier has usage limits that may be exceeded Mitigation: Implement usage quotas and rate limiting; monitor API usage through dashboard Contingency: Upgrade to paid tier if free limits are consistently exceeded, or implement queuing system Data Privacy \u0026amp; Security Breaches\nImpact: Critical | Probability: Low Description: User data exposure or unauthorized access Mitigation: Implement encryption, regular security audits, GDPR compliance Contingency: Incident response plan, insurance, legal consultation Scalability Issues\nImpact: High | Probability: Medium Description: System performance degradation with user growth Mitigation: AWS ECS Auto Scaling in active AZ, RDS Multi-AZ active-passive for high availability, ElastiCache for performance Contingency: Scale ECS tasks in active AZ, activate additional tasks in passive AZ if needed, upgrade RDS instance class Medium Priority Risks:\nThird-party Service Downtime\nImpact: Medium | Probability: Low Description: Dependency on external APIs (Gemini Flash free tier, free dictionary APIs) Mitigation: Graceful degradation, inform users when AI features are unavailable Contingency: Cached responses for dictionary lookups, manual grading option for tests during outages User Acquisition Challenges\nImpact: High | Probability: Medium Description: Difficulty attracting and retaining users Mitigation: Marketing strategy, SEO optimization, referral programs Contingency: Pivot features based on feedback, partnerships with schools Content Moderation Issues\nImpact: Medium | Probability: High Description: Inappropriate content in blogs, comments, or study rooms Mitigation: Automated content filtering, reporting system, moderator team Contingency: Community guidelines, user bans, legal disclaimer Low Priority Risks:\nTechnology Stack Obsolescence\nImpact: Low | Probability: Medium Description: Chosen technologies become outdated Mitigation: Regular dependency updates, modular architecture Contingency: Gradual migration plan, refactoring budget Competition from Established Platforms\nImpact: Medium | Probability: High Description: Competing with Duolingo, IELTS.org, etc. Mitigation: Unique features (study rooms, AI assessment), niche targeting Contingency: Differentiation strategy, feature innovation Mitigation Strategies Technical Mitigations:\nImplement comprehensive error handling and logging with CloudWatch Set up CloudWatch alarms for resource utilization and errors Regular automated backups with RDS Multi-AZ and point-in-time recovery Use CloudFront CDN for static assets to reduce ECS load Implement API rate limiting to prevent abuse Code reviews and automated testing in CI/CD pipeline (AWS CodePipeline/GitHub Actions) AWS WAF for application security and DDoS protection Business Mitigations:\nStart with freemium model to build user base Beta testing phase to identify critical issues Gradual feature rollout to manage costs Build community through social media and content marketing Establish partnerships with IELTS teachers and institutions Legal \u0026amp; Compliance Mitigations:\nTerms of Service and Privacy Policy GDPR and data protection compliance Content licensing agreements User consent for data processing Regular compliance audits Contingency Plans Technical Failures:\nDatabase failure: Automatic failover to RDS Multi-AZ standby instance in AZ-2 (passive) ECS task failure in AZ-1: Auto Scaling replaces unhealthy tasks; critical failure triggers AZ-2 activation API downtime: Serve cached content from ElastiCache and queue requests Security breach: AWS Security Hub immediate alerts, lockdown, and investigation Active-passive Multi-AZ ensures high availability with automatic failover to passive AZ-2 Business Failures:\nLow user adoption: Pivot to B2B model (schools, tutors) High churn rate: User interviews, feature improvements Revenue shortfall: Cost optimization, seek investment Legal Issues:\nCopyright claims: Content takedown procedure Privacy complaints: Data deletion and compliance review Terms violations: User suspension and investigation 8. Expected Outcomes Technical Improvements Platform Capabilities:\nFully functional web application with 5 integrated modules Real-time collaboration features (video/voice calls, messaging) Serverless AI service architecture with asynchronous processing AI-powered Writing assessment via Lambda Function 1 (grammar, vocabulary, task achievement, coherence) AI-powered Speaking assessment via Lambda Function 2 (pronunciation, fluency, coherence, lexical resource) RAG-based flashcard generation via Lambda Function 3 with smart query generation and document understanding Scalable Multi-AZ architecture on AWS ECS supporting 10,000+ concurrent users Mobile-responsive design for learning on-the-go Robust RESTful API built with Spring Boot monolith High availability with 99.9% uptime through active-passive Multi-AZ deployment Vector embeddings and semantic search for intelligent content retrieval Technical Achievements:\nModern full-stack web development with Next.js and Spring Boot Real-time communication implementation (WebRTC, Spring WebSocket) Serverless AI service architecture with API Gateway, SQS, and Lambda Three specialized Lambda functions for Writing/Speaking assessment and RAG-based flashcard generation RAG (Retrieval-Augmented Generation) implementation with Amazon Titan V2 Embeddings Vector store integration for semantic search and document retrieval Google Gemini Flash AI/ML integration for assessments and smart query generation Amazon Bedrock integration for alternative AI models and embeddings AWS cloud infrastructure and active-passive Multi-AZ architecture implementation PostgreSQL database design and optimization DynamoDB for AI results and flashcard storage Container orchestration with Amazon ECS Fargate Security best practices with Spring Security and AWS services DevOps practices with CloudWatch monitoring and automated deployments Educational Impact For Students:\nAccessible, affordable IELTS preparation platform Personalized learning paths and progress tracking Immediate feedback on practice tests Community-driven learning environment 24/7 access to study materials and practice tests Estimated 30-40% cost reduction compared to traditional courses Learning Outcomes:\nImproved IELTS scores through consistent practice Better time management with Pomodoro integration Enhanced vocabulary through flashcard system Intelligent flashcard generation from documents using RAG technology Speaking confidence through AI feedback with detailed pronunciation and fluency analysis Writing skills improvement with detailed grammar, vocabulary, and task achievement analysis Context-aware learning through RAG-based flashcard generation that understands document content Business Value Market Position:\nCompetitive alternative to expensive IELTS prep courses Unique combination of features (study rooms + AI + community) Scalable SaaS business model Potential for B2C and B2B markets (schools, tutoring centers) User Growth Targets:\nWeek 12 (Launch): 100 registered users (beta testers) Month 6: 500 registered users Month 12: 2,000 registered users Year 2: 10,000 registered users Premium conversion rate: 10-15% Revenue Potential:\nYear 1: $17,500 (after launch in Month 6) Year 2: $150,000+ (with 500 premium, 1,000 regular members) Year 3: $500,000+ (with market expansion and partnerships) Long-term Value Platform Evolution:\nFoundation for other language learning modules (TOEFL, SAT, etc.) Data collection for improved AI models Community-generated content library Potential for gamification and achievement systems Mobile app development based on web platform success Social Impact:\nDemocratizing IELTS preparation for students worldwide Reducing language learning barriers Building a supportive learning community Enabling peer-to-peer knowledge sharing Creating opportunities for educational content creators Portfolio \u0026amp; Career Benefits:\nComprehensive full-stack project with Spring Boot and Next.js for developer portfolios Real-world experience with AWS cloud services (ECS, RDS, S3, CloudFront, etc.) Active-passive Multi-AZ architecture and high-availability system design experience AI integration experience with Google Gemini Flash Understanding of educational technology (EdTech) Container orchestration and failover strategies Potential startup opportunity or acquisition target Success Metrics:\nUser engagement: Average 3+ sessions per week Test completion rate: 70%+ of started tests User retention: 60%+ monthly active users NPS Score: 50+ (indicating strong user satisfaction) Average IELTS score improvement: 0.5-1.0 band increase Project Planning "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/5.6.2-sqs-queues/","title":"SQS Queues","tags":[],"description":"","content":"Overview Create Amazon SQS queues for asynchronous AI processing with Dead Letter Queues for failed messages.\nCreate Writing Assessment Queue Setting Value Name ielts-ai-dev-writing-evaluation Type Standard Visibility timeout 5 minutes Message retention 14 days Dead-letter queue ielts-ai-dev-writing-evaluation-dlq Max receives 3 Create Speaking Assessment Queue Setting Value Name ielts-ai-dev-speaking-evaluation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-speaking-evaluation-dlq Create Flashcard Generation Queue Setting Value Name ielts-ai-dev-flashcard-generation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-flashcard-generation-dlq AWS CLI Commands # Create Dead Letter Queue aws sqs create-queue --queue-name ielts-writing-dlq # Create main queue with DLQ aws sqs create-queue \\ --queue-name ielts-writing-queue \\ --attributes \u0026#39;{ \u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;300\u0026#34;, \u0026#34;MessageRetentionPeriod\u0026#34;: \u0026#34;1209600\u0026#34;, \u0026#34;RedrivePolicy\u0026#34;: \u0026#34;{\\\u0026#34;deadLetterTargetArn\\\u0026#34;:\\\u0026#34;arn:aws:sqs:ap-southeast-1:{account}:ielts-writing-dlq\\\u0026#34;,\\\u0026#34;maxReceiveCount\\\u0026#34;:\\\u0026#34;3\\\u0026#34;}\u0026#34; }\u0026#39; # Repeat for speaking and flashcard queues aws sqs create-queue --queue-name ielts-speaking-dlq aws sqs create-queue --queue-name ielts-speaking-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; aws sqs create-queue --queue-name ielts-flashcard-dlq aws sqs create-queue --queue-name ielts-flashcard-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; Queue Summary Queue Visibility Timeout DLQ Max Receives ielts-writing-queue 5 min ielts-writing-dlq 3 ielts-speaking-queue 15 min ielts-speaking-dlq 3 ielts-flashcard-queue 15 min ielts-flashcard-dlq 3 Next Steps Proceed to Lambda Functions.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives Resolve AWS account issues and create a new account if necessary. Master Hybrid DNS configuration with Route 53 Resolver. Implement and understand VPC Peering for inter-VPC communication. Discuss project plans and finalize programming language with the team. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Access Management with AWS Identity and Access Management (IAM). 21/09/2025 23/09/2025 AWS Identity and Access Management (IAM) Access Control 3 - Complete Lab 10: Route 53 and Hybrid DNS configuration.\n- Launch virtual servers to implement and test DNS setup.\n- Complete: Hybrid DNS Management with Amazon Route 53. 24/09/2025 25/09/2025 FCJ Playlist 4 - Implement VPC Peering for private communication between VPCs.\n- Create necessary resources for VPC Peering configuration.\n- Clean up resources after completion.\n- Complete: Network Integration with VPC Peering. 25/09/2025 26/09/2025 AWS VPC Peering 5 - Attend team meeting to discuss project plans and programming language selection.\n- Set deadlines for team members to study chosen technology stack. 28/09/2025 28/09/2025 Team Meeting AWS Skill Builder Courses Completed Course Category Status Hybrid DNS Management with Amazon Route 53 Networking ✅ Network Integration with VPC Peering Networking ✅ Networking on AWS Workshop Networking ✅ Infrastructure as Code with AWS CloudFormation DevOps ✅ Cloud Development with AWS Cloud9 Development ✅ Static Website Hosting with Amazon S3 Storage ✅ Week 3 Achievements Technical Skills Acquired:\nRoute 53 and Hybrid DNS:\nSuccessfully configured Hybrid DNS infrastructure with Route 53 Resolver Created and configured Outbound Endpoints for DNS query forwarding Set up Route 53 Resolver rules for conditional DNS resolution Implemented Inbound Endpoints for on-premises to AWS DNS queries Successfully connected to RD Gateway Server during practical exercises VPC Peering:\nMastered VPC Peering concepts for private inter-VPC communication without traversing public internet Enabled Cross-Zone and Cross-Region DNS Resolution in VPC Peering: EC2 instances can now resolve DNS of instances in peered VPCs to private IP addresses Understood that without this feature, DNS queries return public IPs, routing traffic through internet Learned resource cleanup procedures to avoid unnecessary costs Infrastructure as Code:\nLearned to provision AWS resources using CloudFormation templates Understood declarative infrastructure management principles Explored AWS Cloud9 as a cloud-based development environment Team Collaboration:\nParticipated in team meeting to finalize project direction Selected programming language for the project Established deadlines for team members to study the chosen technology stack Continued learning journey with FCJ team support Key Takeaways:\nHybrid DNS enables seamless DNS resolution between on-premises and AWS environments VPC Peering is cost-effective for connecting VPCs but has limitations (no transitive peering) CloudFormation templates ensure consistent, repeatable infrastructure deployments AWS Cloud9 eliminates local development environment setup complexity "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.4-setup-fe/5.4.3-sg/","title":"Configure Security Group","tags":[],"description":"","content":"Our Frontend containers run in Private Subnets. To allow them to receive traffic, we must configure a Security Group that acts as a firewall.\nFor security best practices, we will only allow traffic from the Application Load Balancer (ALB) on port 3000. Direct access from the internet or other sources will be blocked.\n1. Create Security Group Navigate to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Basic details: Security group name: ecs-private-sg. Description: security group for ecs. VPC: Select band-up-vpc. 2. Configure Inbound Rules This is the most critical step. We need to allow the ALB to talk to our Next.js application.\nInbound rules: Click Add rule. Type: Custom TCP. Port range: 3000 (The port our Next.js app listens on). Source: Select Custom and choose the Security Group ID of your ALB (e.g., alb-sg). Note: By selecting the ALB\u0026rsquo;s Security Group ID instead of an IP range, we ensure that only traffic originating from our Load Balancer is accepted. Outbound rules: Leave the default settings (Allow all traffic) to enable the container to download packages or communicate with external APIs. Click Create security group. The Security Group is now ready to be attached to our ECS Tasks in the next step.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.5-setup-be/5.5.3-redis/","title":"Create ElastiCache (Redis/Valkey)","tags":[],"description":"","content":"In this step, we provision an in-memory data store to handle session management and caching for the backend. We will use Amazon ElastiCache with the Valkey engine (a high-performance, open-source fork of Redis supported by AWS).\n1. Configure Security Group First, create a Security Group to allow the backend to communicate with the cache cluster.\nNavigate to EC2 \u0026gt; Security Groups \u0026gt; Create security group. Name: redis-sg. Inbound rules: Allow Custom TCP traffic on port 6379 from the ecs-backend-sg. (Note: Ensure this is created before proceeding to the ElastiCache console).\n2. Create Subnet Group We need to define which subnets the cache nodes will reside in.\nNavigate to Amazon ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group. Name: bandup-cached-subnet-group. VPC: Select band-up-vpc. Subnets: Select private-database-subnet-1 and private-database-subnet-2 (Availability Zones ap-southeast-1a and 1b). 3. Create ElastiCache Cluster Now we provision the cache cluster.\nNavigate to ElastiCache \u0026gt; Caches \u0026gt; Create cache. Engine: Select Valkey - recommended (Compatible with Redis OSS). Deployment option: Select Node-based cluster (Gives more control over instance types). Creation method: Cluster cache. Cluster settings: Cluster mode: Disabled (Simple primary-replica structure is sufficient). Name: bandup-redis. Description: in memory db for bandup. Node configuration: Node type: cache.t3.micro (Cost-effective for testing). Number of replicas: 0 (Standalone node for this workshop). Connectivity: Network type: IPv4. Subnet groups: Select bandup-cached-subnet-group. Security \u0026amp; Encryption: Encryption at rest: Enabled (Default key). Encryption in transit: Enabled. Access control: No access control (We rely on Security Groups). Security groups: Select redis-sg created earlier. Backup: Enable automatic backups (Retention: 1 day). Click Create. The cluster status will change to Creating. Once Available, note down the Primary Endpoint (ending in ...cache.amazonaws.com:6379) to use in the backend configuration.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.3-network/5.3.3-iam/","title":"IAM Roles for ECS","tags":[],"description":"","content":"To allow Amazon ECS to manage your containers, it needs specific permissions. We must create an IAM Role that authorizes the ECS agent to pull container images from Amazon ECR and send logs to Amazon CloudWatch on your behalf.\nCreate ecsTaskExecutionRole Navigate to the IAM Dashboard. In the left navigation pane, choose Roles. Click Create role. Step 1: Trusted Entity\nTrusted entity type: Select AWS service. Service or use case: Choose Elastic Container Service. Select Elastic Container Service Task from the options below. Click Next. Step 2: Add Permissions\nIn the search bar, type AmazonECSTaskExecutionRolePolicy. Check the box next to the policy name AmazonECSTaskExecutionRolePolicy. Note: This managed policy grants permissions to pull images from ECR and upload logs to CloudWatch. Click Next. Step 3: Name and Review\nRole name: Enter ecsTaskExecutionRole. Review the configuration and click Create role. Once created, this role is ready to be assigned to our ECS Task Definitions in the upcoming sections.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.3-network/","title":"Network &amp; Security Infrastructure","tags":[],"description":"","content":"Overview In this section, we will establish the foundational network layer and security boundaries for IELTS BandUp.\nA robust network architecture is critical for protecting sensitive user data and ensuring high availability. Instead of using the default network settings, we will construct a custom Virtual Private Cloud (VPC) designed for a production-grade environment. This setup allows us to strictly control traffic flow between our application components (Frontend, Backend, Database) and the internet.\nFurthermore, we will configure VPC Endpoints to allow our private containers to communicate securely with AWS services (like ECR and S3) without traversing the public internet, enhancing both security and network performance.\nImplementation Steps We will break down the infrastructure setup into the following key tasks:\nVPC \u0026amp; Connectivity: Create the isolated network environment, partition it into Public and Private subnets, and configure Internet Gateways (IGW) for external connectivity. Load Balancing (ALB): Set up the Application Load Balancer and Target Groups to distribute incoming traffic efficiently to our future ECS tasks. IAM Security: Provision the ecsTaskExecutionRole to grant our Fargate containers the necessary permissions to pull images and push logs. VPC Endpoints: Establish private connections to AWS services (ECR, CloudWatch, S3) to secure internal traffic. Content VPC, Subnets \u0026amp; Routing Application Load Balancer (ALB) IAM Roles for ECS VPC Endpoints Setup "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/5.6.3-lambda-functions/","title":"Lambda Functions","tags":[],"description":"","content":"Overview The AI Service layer consists of four Lambda functions that power the IELTS learning platform. These functions process requests asynchronously via SQS queues and integrate with Google Gemini API and Amazon Bedrock for AI-powered evaluations.\nLambda Function 1: Writing Evaluator Evaluates IELTS Writing Task 1 and Task 2 essays using Gemini API with detailed band scoring.\nSetting Value Function name bandup-writing-evaluator Runtime Python 3.11 Memory 1024 MB Timeout 5 minutes Trigger SQS (bandup-writing-queue) AI Model Google Gemini 2.0 Flash Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any logger = logging.getLogger() logger.setLevel(logging.INFO) # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Writing essays using Gemini API.\u0026#34;\u0026#34;\u0026#34; # Parse SQS message or API Gateway request if is_sqs_event(event): request_data, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;, \u0026#39;writing\u0026#39;) else: request_data = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) # Get API key securely from Secrets Manager gemini_api_key = get_gemini_api_key() # Retrieved from AWS Secrets Manager gemini_client = GeminiClient(api_key=gemini_api_key) # Extract request parameters user_id = request_data.get(\u0026#39;user_id\u0026#39;) essay_content = request_data.get(\u0026#39;essay_content\u0026#39;) task_type = request_data.get(\u0026#39;task_type\u0026#39;, \u0026#39;TASK_2\u0026#39;) # Build evaluation prompt prompt = build_writing_prompt(essay_content, task_type) # Call Gemini API for evaluation response = gemini_client.generate_evaluation( prompt=prompt, feature=\u0026#39;writing_task2\u0026#39;, max_retries=3, timeout=60 ) # Parse and validate band scores evaluation = parse_gemini_response(response[\u0026#39;content\u0026#39;]) # Build result with IELTS criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;task_achievement_band\u0026#39;: evaluation[\u0026#39;task_achievement\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;coherence_band\u0026#39;: evaluation[\u0026#39;coherence_cohesion\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;feedback\u0026#39;: evaluation } # Save to DynamoDB dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_EVALUATIONS\u0026#39;)) table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: result[\u0026#39;session_id\u0026#39;], \u0026#39;user_id\u0026#39;: user_id, \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, **result }) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Gemini Prompt Template:\ndef build_writing_prompt(essay_content: str, task_type: str) -\u0026gt; str: return f\u0026#34;\u0026#34;\u0026#34;You are an experienced IELTS examiner. Evaluate this essay: Task Type: {task_type} ESSAY: {essay_content} Evaluate using IELTS band descriptors (1-9, 0.5 increments): 1. Task Achievement - Addresses all parts of task 2. Coherence and Cohesion - Logical organization 3. Lexical Resource - Vocabulary range and accuracy 4. Grammatical Range and Accuracy - Sentence structures RESPOND IN JSON FORMAT: {{ \u0026#34;overall_band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;task_achievement\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;coherence_cohesion\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;lexical_resource\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;grammatical_range_accuracy\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;quoted_examples\u0026#34;: [{{\u0026#34;quote\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;...\u0026#34;}}] }}\u0026#34;\u0026#34;\u0026#34; Lambda Function 2: Speaking Evaluator Evaluates IELTS Speaking using Gemini native audio processing - 72% cheaper and 2x faster than AWS Transcribe alternatives.\nSetting Value Function name bandup-speaking-evaluator Runtime Python 3.11 Memory 2048 MB Timeout 5 minutes Trigger SQS (bandup-speaking-queue) AI Model Gemini 2.5 Flash (Native Audio) Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any, Tuple logger = logging.getLogger() # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def download_audio_from_s3(audio_url: str) -\u0026gt; Tuple[bytes, str]: \u0026#34;\u0026#34;\u0026#34;Download audio file from S3 and determine MIME type.\u0026#34;\u0026#34;\u0026#34; s3_client = boto3.client(\u0026#39;s3\u0026#39;) # Parse S3 URL: s3://bucket-name/path/to/file.mp3 parts = audio_url.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) bucket, key = parts[0], parts[1] response = s3_client.get_object(Bucket=bucket, Key=key) audio_bytes = response[\u0026#39;Body\u0026#39;].read() # Determine MIME type from extension mime_types = {\u0026#39;.mp3\u0026#39;: \u0026#39;audio/mp3\u0026#39;, \u0026#39;.wav\u0026#39;: \u0026#39;audio/wav\u0026#39;, \u0026#39;.m4a\u0026#39;: \u0026#39;audio/m4a\u0026#39;} ext = \u0026#39;.\u0026#39; + key.split(\u0026#39;.\u0026#39;)[-1].lower() mime_type = mime_types.get(ext, \u0026#39;audio/mp3\u0026#39;) return audio_bytes, mime_type def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Speaking using Gemini native audio.\u0026#34;\u0026#34;\u0026#34; # Parse request request_data = parse_request(event) # Get API key from Secrets Manager gemini_api_key = get_gemini_api_key() gemini_client = GeminiClient(api_key=gemini_api_key) # Extract parameters audio_url = request_data.get(\u0026#39;audio_url\u0026#39;) part = request_data.get(\u0026#39;part\u0026#39;, \u0026#39;PART_1\u0026#39;) questions = request_data.get(\u0026#39;questions\u0026#39;, []) # Step 1: Download audio from S3 audio_bytes, mime_type = download_audio_from_s3(audio_url) logger.info(f\u0026#34;Downloaded {len(audio_bytes)} bytes, MIME: {mime_type}\u0026#34;) # Step 2: Send audio directly to Gemini (ONE API call) # No AWS Transcribe needed - Gemini processes audio natively evaluation = gemini_client.evaluate_audio( audio_bytes=audio_bytes, part=part, questions=questions, mime_type=mime_type, max_retries=3, timeout=120 ) # Step 3: Build response with IELTS Speaking criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;transcript\u0026#39;: evaluation.get(\u0026#39;transcript\u0026#39;), \u0026#39;duration\u0026#39;: evaluation.get(\u0026#39;duration_seconds\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;fluency_band\u0026#39;: evaluation[\u0026#39;fluency_coherence\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;pronunciation_band\u0026#39;: evaluation[\u0026#39;pronunciation\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;model_used\u0026#39;: \u0026#39;gemini-2.5-flash-audio\u0026#39;, \u0026#39;estimated_cost\u0026#39;: evaluation[\u0026#39;usage\u0026#39;][\u0026#39;cost\u0026#39;] } # Save to DynamoDB save_evaluation(result, request_data.get(\u0026#39;user_id\u0026#39;)) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Cost Comparison:\nApproach Cost per 3-min Audio Latency Gemini Native Audio ~$0.021 30-45s AWS Transcribe + LLM ~$0.076 60-90s Savings 72% 2x faster Lambda Function 3: Flashcard Generator (RAG) Generates flashcards from PDF documents using lightweight RAG pipeline with Titan Embeddings (in-memory vector store, optimized for \u0026lt;50MB Lambda package).\nSetting Value Function name bandup-flashcard-generator Runtime Python 3.11 Memory 1024 MB Timeout 10 minutes Trigger SQS (bandup-flashcard-queue) AI Model Gemini + Amazon Titan Embeddings V2 RAG Pipeline Flow:\n┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ PDF Upload │ ──▶ │ Chunking │ ──▶ │ Titan Embeddings│ │ (S3) │ │ (3000 chars) │ │ (Bedrock) │ └─────────────┘ └──────────────┘ └────────┬────────┘ │ ▼ ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ Flashcards │ ◀── │ Gemini │ ◀── │ In-Memory Store │ │ (JSON) │ │ Generation │ │ (Cosine Sim) │ └─────────────┘ └──────────────┘ └─────────────────┘ Core Implementation:\nimport json import os import boto3 import time import google.generativeai as genai from typing import Dict, Any, List from concurrent.futures import ThreadPoolExecutor, as_completed logger = logging.getLogger() # Global instance for warm starts (Lambda optimization) _rag_instance = None _s3_client = None def get_s3_client(): \u0026#34;\u0026#34;\u0026#34;Get cached S3 client.\u0026#34;\u0026#34;\u0026#34; global _s3_client if _s3_client is None: _s3_client = boto3.client(\u0026#39;s3\u0026#39;) return _s3_client def get_rag_instance(api_key: str): \u0026#34;\u0026#34;\u0026#34;Get cached RAG instance for warm starts.\u0026#34;\u0026#34;\u0026#34; global _rag_instance if _rag_instance is None: _rag_instance = RAG( api_key=api_key, chunk_size=int(os.environ.get(\u0026#39;RAG_CHUNK_SIZE\u0026#39;, \u0026#39;500\u0026#39;)), chunk_overlap=int(os.environ.get(\u0026#39;RAG_CHUNK_OVERLAP\u0026#39;, \u0026#39;100\u0026#39;)) ) logger.info(\u0026#34;Cold start: RAG instance created\u0026#34;) else: logger.info(\u0026#34;Warm start: Reusing RAG instance\u0026#34;) return _rag_instance def download_pdf_from_s3(bucket: str, key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Download PDF from S3 to /tmp.\u0026#34;\u0026#34;\u0026#34; s3 = get_s3_client() local_path = f\u0026#34;/tmp/{key.split(\u0026#39;/\u0026#39;)[-1]}\u0026#34; s3.download_file(bucket, key, local_path) return local_path def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;RAG-based flashcard generation .\u0026#34;\u0026#34;\u0026#34; start_time = time.time() is_async = is_sqs_event(event) # Parse request if is_async: request, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;) else: request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) if isinstance(event.get(\u0026#39;body\u0026#39;), str) else event # Get S3 location pdf_url = request.get(\u0026#39;pdf_url\u0026#39;) s3_bucket, s3_key = parse_s3_url(pdf_url) # Get API key from Secrets Manager secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) secrets_client = boto3.client(\u0026#39;secretsmanager\u0026#39;) api_key = secrets_client.get_secret_value(SecretId=secret_arn)[\u0026#39;SecretString\u0026#39;] # Get parameters num_cards = int(request.get(\u0026#39;num_cards\u0026#39;, 10)) difficulty = request.get(\u0026#39;difficulty\u0026#39;, \u0026#39;MEDIUM\u0026#39;) question_types = request.get(\u0026#39;question_types\u0026#39;, [\u0026#39;DEFINITION\u0026#39;, \u0026#39;VOCABULARY\u0026#39;, \u0026#39;COMPREHENSION\u0026#39;]) # Step 1: Download PDF from S3 local_pdf = download_pdf_from_s3(s3_bucket, s3_key) # Step 2: Index document with RAG (Titan Embeddings + in-memory store) rag = get_rag_instance(api_key) rag._vector_store = None # Reset for new document rag._chunks = [] index_result = rag.index_document(local_pdf, document_id=s3_key) logger.info(f\u0026#34;Indexed {index_result[\u0026#39;chunk_count\u0026#39;]} chunks from {index_result[\u0026#39;page_count\u0026#39;]} pages\u0026#34;) # Step 3: Retrieve relevant chunks (hybrid approach) if index_result[\u0026#39;chunk_count\u0026#39;] \u0026lt;= 15: # Small document: use representative chunks chunks = rag.get_representative_chunks(num_chunks=min(10, index_result[\u0026#39;chunk_count\u0026#39;])) retrieval_method = \u0026#34;representative\u0026#34; else: # Large document: use smart keyword-based queries chunks = rag.retrieve_with_smart_queries(top_k_per_query=3) retrieval_method = \u0026#34;smart_queries\u0026#34; # Step 4: Generate flashcards with Gemini prompt = generate_flashcards_prompt(chunks, num_cards, difficulty, question_types) flashcard_result = call_gemini(prompt, api_key) # Clean up os.remove(local_pdf) # Build response total_time = time.time() - start_time response_body = { \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document\u0026#39;: { \u0026#39;s3_bucket\u0026#39;: s3_bucket, \u0026#39;s3_key\u0026#39;: s3_key, \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;] }, \u0026#39;retrieval\u0026#39;: { \u0026#39;method\u0026#39;: retrieval_method, \u0026#39;chunks_used\u0026#39;: len(chunks), \u0026#39;keywords\u0026#39;: index_result.get(\u0026#39;keywords\u0026#39;, [])[:5] }, \u0026#39;flashcards\u0026#39;: flashcard_result.get(\u0026#39;flashcards\u0026#39;, []), \u0026#39;total_cards\u0026#39;: len(flashcard_result.get(\u0026#39;flashcards\u0026#39;, [])), \u0026#39;metrics\u0026#39;: { \u0026#39;total_time_ms\u0026#39;: round(total_time * 1000) } } # Save to DynamoDB (bandup-flashcard-sets table) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_FLASHCARD_SETS\u0026#39;)) table.put_item(Item={ \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document_id\u0026#39;: s3_key, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(response_body[\u0026#39;flashcards\u0026#39;]), \u0026#39;total_cards\u0026#39;: response_body[\u0026#39;total_cards\u0026#39;], \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;], \u0026#39;created_at\u0026#39;: int(time.time()) }) if is_async: return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;OK\u0026#39;} return create_response(200, response_body) Titan Embeddings with Parallel Processing:\nclass TitanEmbeddings: \u0026#34;\u0026#34;\u0026#34;Amazon Titan Text Embeddings V2 via Bedrock with parallel processing.\u0026#34;\u0026#34;\u0026#34; MODEL_ID = \u0026#34;amazon.titan-embed-text-v2:0\u0026#34; def __init__(self, region: str = None): self.region = region or os.environ.get(\u0026#39;BEDROCK_REGION\u0026#39;, \u0026#39;us-east-1\u0026#39;) self._client = None @property def client(self): if self._client is None: self._client = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=self.region) return self._client def embed(self, text: str) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34;Get embedding for single text using Titan V2.\u0026#34;\u0026#34;\u0026#34; response = self.client.invoke_model( modelId=self.MODEL_ID, body=json.dumps({ \u0026#34;inputText\u0026#34;: text[:8000], # Max input length \u0026#34;dimensions\u0026#34;: 512, \u0026#34;normalize\u0026#34;: True }), contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34; ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;embedding\u0026#39;] def embed_batch_parallel(self, texts: List[str], max_workers: int = 10) -\u0026gt; List[List[float]]: \u0026#34;\u0026#34;\u0026#34;Embed multiple texts in PARALLEL using ThreadPoolExecutor.\u0026#34;\u0026#34;\u0026#34; embeddings = [None] * len(texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(self.embed, t): i for i, t in enumerate(texts)} for future in as_completed(futures): idx = futures[future] embeddings[idx] = future.result() return embeddings RAG Pipeline (In-Memory):\nimport math import fitz # PyMuPDF class RAG: \u0026#34;\u0026#34;\u0026#34;Lightweight RAG using Titan Embeddings + in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; def __init__(self, api_key: str, chunk_size: int = 3000, chunk_overlap: int = 300): self.api_key = api_key self.chunk_size = chunk_size self.chunk_overlap = chunk_overlap self._chunks = [] self._embeddings = [] self._titan = TitanEmbeddings() self._keywords = [] def index_document(self, pdf_path: str, document_id: str = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Index PDF with Titan V2 embeddings (parallel processing).\u0026#34;\u0026#34;\u0026#34; # Load PDF pages pages = [] with fitz.open(pdf_path) as doc: for page_num, page in enumerate(doc): text = page.get_text() if text.strip(): pages.append({\u0026#39;content\u0026#39;: text, \u0026#39;page\u0026#39;: page_num + 1}) # Chunk text with overlap self._chunks = [] for page in pages: chunks = self._chunk_text(page[\u0026#39;content\u0026#39;]) for chunk in chunks: self._chunks.append({ \u0026#39;text\u0026#39;: chunk, \u0026#39;page\u0026#39;: page[\u0026#39;page\u0026#39;] }) # Extract keywords for smart query generation all_text = \u0026#34; \u0026#34;.join([c[\u0026#39;text\u0026#39;] for c in self._chunks]) self._keywords = self._extract_keywords(all_text, top_n=20) # Generate embeddings in parallel (10 concurrent Bedrock calls) texts = [c[\u0026#39;text\u0026#39;] for c in self._chunks] self._embeddings = self._titan.embed_batch_parallel(texts, max_workers=10) return { \u0026#39;page_count\u0026#39;: len(pages), \u0026#39;chunk_count\u0026#39;: len(self._chunks), \u0026#39;keywords\u0026#39;: self._keywords[:10] } def _cosine_similarity(self, a: List[float], b: List[float]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Calculate cosine similarity between two vectors.\u0026#34;\u0026#34;\u0026#34; dot_product = sum(x * y for x, y in zip(a, b)) norm_a = math.sqrt(sum(x * x for x in a)) norm_b = math.sqrt(sum(x * x for x in b)) if norm_a == 0 or norm_b == 0: return 0.0 return dot_product / (norm_a * norm_b) def similarity_search(self, query: str, top_k: int = 5) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for similar chunks using in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; query_embedding = self._titan.embed(query) # Calculate similarities similarities = [] for i, embedding in enumerate(self._embeddings): score = self._cosine_similarity(query_embedding, embedding) similarities.append((i, score)) # Sort by similarity (descending) and return top-k similarities.sort(key=lambda x: x[1], reverse=True) results = [] for rank, (idx, score) in enumerate(similarities[:top_k]): chunk = self._chunks[idx] results.append({ \u0026#39;text\u0026#39;: chunk[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: chunk[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: score, \u0026#39;rank\u0026#39;: rank + 1 }) return results def generate_smart_queries(self, num_queries: int = 5) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Generate document-specific queries using extracted keywords.\u0026#34;\u0026#34;\u0026#34; kw = self._keywords queries = [] if len(kw) \u0026gt;= 2: queries.append(f\u0026#34;definition and explanation of {kw[0]} and {kw[1]}\u0026#34;) if len(kw) \u0026gt;= 4: queries.append(f\u0026#34;key concepts about {kw[2]} {kw[3]}\u0026#34;) if len(kw) \u0026gt;= 6: queries.append(f\u0026#34;important information regarding {kw[4]} {kw[5]}\u0026#34;) return queries[:num_queries] def retrieve_with_smart_queries(self, top_k_per_query: int = 3) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Retrieve chunks using multiple smart queries for better coverage.\u0026#34;\u0026#34;\u0026#34; queries = self.generate_smart_queries() seen_texts = set() all_results = [] for query in queries: results = self.similarity_search(query, top_k=top_k_per_query) for r in results: if r[\u0026#39;text\u0026#39;] not in seen_texts: seen_texts.add(r[\u0026#39;text\u0026#39;]) all_results.append(r) return sorted(all_results, key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) def get_representative_chunks(self, num_chunks: int = 10) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Get evenly distributed chunks across document.\u0026#34;\u0026#34;\u0026#34; if len(self._chunks) \u0026lt;= num_chunks: return [{\u0026#39;text\u0026#39;: c[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: c[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for c in self._chunks] step = len(self._chunks) // num_chunks return [{\u0026#39;text\u0026#39;: self._chunks[i * step][\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: self._chunks[i * step][\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for i in range(num_chunks)] Flashcard Generation Prompt:\ndef generate_flashcards_prompt(chunks: List[Dict], num_cards: int, difficulty: str, question_types: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Build prompt for Gemini flashcard generation.\u0026#34;\u0026#34;\u0026#34; context = \u0026#34;\\n\\n\u0026#34;.join([ f\u0026#34;[Chunk {i+1}] (Page {c.get(\u0026#39;page\u0026#39;, \u0026#39;?\u0026#39;)}):\\n{c[\u0026#39;text\u0026#39;]}\u0026#34; for i, c in enumerate(chunks) ]) return f\u0026#34;\u0026#34;\u0026#34;Based on the following document excerpts, generate {num_cards} flashcards. CONTEXT: {context} REQUIREMENTS: - Difficulty: {difficulty} - Generate exactly {num_cards} flashcards - Each flashcard should have a clear question and concise answer - Focus on key concepts, definitions, and important facts - Use these question types: {\u0026#34;, \u0026#34;.join(question_types)} OUTPUT FORMAT (JSON): {{ \u0026#34;flashcards\u0026#34;: [ {{ \u0026#34;question\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DEFINITION\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;{difficulty}\u0026#34;, \u0026#34;source_chunk\u0026#34;: 1 }} ] }} Return ONLY valid JSON.\u0026#34;\u0026#34;\u0026#34; def call_gemini(prompt: str, api_key: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Call Gemini API for flashcard generation.\u0026#34;\u0026#34;\u0026#34; import google.generativeai as genai genai.configure(api_key=api_key) model = genai.GenerativeModel( model_name=os.environ.get(\u0026#39;GEMINI_MODEL\u0026#39;, \u0026#39;gemini-2.0-flash\u0026#39;), generation_config={ \u0026#39;temperature\u0026#39;: 0.3, \u0026#39;max_output_tokens\u0026#39;: 4096 } ) response = model.generate_content(prompt) text = response.text # Extract JSON if wrapped in markdown if \u0026#39;```json\u0026#39; in text: text = text.split(\u0026#39;```json\u0026#39;)[1].split(\u0026#39;```\u0026#39;)[0] return json.loads(text.strip()) Lambda Function 4: S3 Upload Handler Generates presigned URLs for secure file uploads to S3.\nSetting Value Function name bandup-s3-upload Runtime Python 3.11 Memory 256 MB Timeout 30 seconds Trigger API Gateway (sync) Core Implementation:\nimport json import os import boto3 from datetime import datetime from typing import Dict, Any s3_client = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Generate presigned URL for S3 upload.\u0026#34;\u0026#34;\u0026#34; request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) user_id = request.get(\u0026#39;user_id\u0026#39;) filename = request.get(\u0026#39;filename\u0026#39;) content_type = request.get(\u0026#39;content_type\u0026#39;, \u0026#39;application/octet-stream\u0026#39;) upload_type = request.get(\u0026#39;upload_type\u0026#39;, \u0026#39;general\u0026#39;) # Determine bucket based on upload type bucket_map = { \u0026#39;speaking_audio\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_AUDIO\u0026#39;), \u0026#39;flashcard_pdf\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), \u0026#39;writing_essay\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), } bucket = bucket_map.get(upload_type) # Generate organized S3 key timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) key = f\u0026#34;uploads/{upload_type}/{user_id}/{timestamp}_{filename}\u0026#34; # Generate presigned PUT URL (15 min expiry) upload_url = s3_client.generate_presigned_url( \u0026#39;put_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key, \u0026#39;ContentType\u0026#39;: content_type}, ExpiresIn=900 ) # Generate presigned GET URL (1 hour expiry) get_url = s3_client.generate_presigned_url( \u0026#39;get_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key}, ExpiresIn=3600 ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;upload_url\u0026#39;: upload_url, \u0026#39;get_url\u0026#39;: get_url, \u0026#39;file_url\u0026#39;: f\u0026#34;s3://{bucket}/{key}\u0026#34;, \u0026#39;expires_in\u0026#39;: 900 }) } Secure Secrets Management All Lambda functions use AWS Secrets Manager to retrieve API keys:\n# secrets_helper.py (in Lambda Layer) import boto3 import os from functools import lru_cache @lru_cache(maxsize=1) def get_gemini_api_key() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve Gemini API key from Secrets Manager (cached).\u0026#34;\u0026#34;\u0026#34; client = boto3.client(\u0026#39;secretsmanager\u0026#39;) secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) response = client.get_secret_value(SecretId=secret_arn) return response[\u0026#39;SecretString\u0026#39;] Security Best Practices:\nNever hardcode API keys in Lambda code Use AWS Secrets Manager for all sensitive credentials Rotate secrets regularly using automatic rotation Use IAM roles with least-privilege permissions IAM Role for Lambda Functions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;BedrockAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;bedrock:InvokeModel\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:*:*:foundation-model/amazon.titan-embed-text-v2*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:*:*:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:*:*:table/bandup-flashcard-sets\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3Access\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::bandup-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SQSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sqs:ReceiveMessage\u0026#34;, \u0026#34;sqs:DeleteMessage\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:*:*:bandup-*-queue\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;secretsmanager:GetSecretValue\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:*:*:secret:bandup/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:log-group:/aws/lambda/bandup-*\u0026#34; } ] } DynamoDB Tables Lambda functions store results in two DynamoDB tables:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Evaluations Table Schema (Writing \u0026amp; Speaking):\n# Used by Writing Evaluator table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: session_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, # \u0026#39;writing\u0026#39; or \u0026#39;speaking\u0026#39; \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;overall_band\u0026#39;: \u0026#39;7.0\u0026#39;, \u0026#39;task_achievement_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Writing only \u0026#39;fluency_band\u0026#39;: \u0026#39;6.5\u0026#39;, # Speaking only \u0026#39;pronunciation_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Speaking only \u0026#39;transcript\u0026#39;: \u0026#39;...\u0026#39;, # Speaking only \u0026#39;feedback\u0026#39;: json.dumps(feedback), \u0026#39;created_at\u0026#39;: timestamp }) Flashcard Sets Table Schema:\n# Used by Flashcard Generator table.put_item(Item={ \u0026#39;set_id\u0026#39;: set_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;document_id\u0026#39;: document_id, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(flashcards), \u0026#39;total_cards\u0026#39;: 10, \u0026#39;page_count\u0026#39;: 5, \u0026#39;chunk_count\u0026#39;: 12, \u0026#39;created_at\u0026#39;: timestamp }) Environment Variables Variable Description Example GEMINI_API_KEY_SECRET_ARN Secrets Manager ARN arn:aws:secretsmanager:...:secret:bandup/gemini-api-key DYNAMODB_EVALUATIONS Evaluations table (Writing + Speaking) bandup-evaluations DYNAMODB_FLASHCARD_SETS Flashcard sets table bandup-flashcard-sets S3_BUCKET_AUDIO Audio bucket bandup-audio-bucket S3_BUCKET_DOCUMENTS Documents bucket bandup-documents-bucket BEDROCK_REGION Bedrock region for Titan us-east-1 RAG_CHUNK_SIZE Chunk size for RAG 3000 RAG_CHUNK_OVERLAP Chunk overlap 300 GEMINI_MODEL Gemini model name gemini-2.0-flash Deploy Lambda Functions cd rag_flashcard pip install -r requirements.txt -t package/ cp lambda_handler.py rag_pipeline.py package/ cd package \u0026amp;\u0026amp; zip -r ../function.zip . \u0026amp;\u0026amp; cd .. # Create Lambda function aws lambda create-function \\ --function-name bandup-flashcard-generator \\ --runtime python3.11 \\ --handler lambda_handler.lambda_handler \\ --role arn:aws:iam::${AWS_ACCOUNT_ID}:role/bandup-lambda-role \\ --timeout 600 \\ --memory-size 1024 \\ --zip-file fileb://function.zip \\ --environment Variables=\u0026#34;{ GEMINI_API_KEY_SECRET_ARN=arn:aws:secretsmanager:${AWS_REGION}:${AWS_ACCOUNT_ID}:secret:bandup/gemini-api-key, BEDROCK_REGION=us-east-1, RAG_CHUNK_SIZE=3000 }\u0026#34; # Add SQS trigger aws lambda create-event-source-mapping \\ --function-name bandup-flashcard-generator \\ --event-source-arn arn:aws:sqs:${AWS_REGION}:${AWS_ACCOUNT_ID}:bandup-flashcard-queue \\ --batch-size 1 Next Steps Proceed to DynamoDB to configure the database tables.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerate benefits claims processing with Amazon Bedrock Data Automation The blog post introduces a solution to accelerate insurance benefits claims processing by leveraging Amazon Bedrock Data Automation and Generative AI. It explains how benefits administration companies can significantly enhance operational efficiency by automating the reimbursement claims process, which is currently challenged by reliance on legacy systems, slow manual processes, and high fraud rates. The proposed solution utilizes Amazon Bedrock Data Automation to automatically extract and classify data from unstructured documents such as check images and receipts, thereby reducing manual errors and accelerating processing time. Simultaneously, the architecture integrates the large language model Amazon Nova Lite and Knowledge Bases to automatically validate and make approve/deny decisions based on flexible business rules, allowing for quick adaptation to compliance requirements like HIPAA and ERISA, ultimately lowering operational costs and improving employee satisfaction.\nBlog 2 - Enabling AI adoption at scale through enterprise risk management framework – Part 2 This blog introduces practical strategies for adjusting the Enterprise Risk Management Framework (ERMF) to enable safe and responsible large-scale adoption of Generative AI. The article explores how to integrate Generative AI governance factors into the ERMF, building upon existing risk management processes while addressing the unique characteristics of Generative AI. Key control areas are detailed, including Fairness, Explainability, Security and Privacy, Safety, Controllability, Accuracy and Robustness, Governance, and Transparency. By the end of the article, readers will have a roadmap to balance innovation with rigorous controls, helping organizations protect against critical risks like data leakage and model hallucinations, which is especially important for organizations in the financial services sector.\nBlog 3 - How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock The article describes how PropHero, a leading real estate asset management service in Spain and Australia, collaborated with the AWS Generative AI Innovation Center to build a smart real estate investment advisor. This solution uses a multi-agent conversational AI system based on Amazon Bedrock to provide personalized advice, while integrating a continuous evaluation mechanism to measure quality metrics such as Context Relevance, Response Groundedness, and Agent Goal Accuracy. In terms of business impact, the AI advisor achieved a 90% goal accuracy rate, helped reduce customer support workload by 30%, and optimized AI costs by 60% compared to using only premium models.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AWS Cloud Day Vietnam - AI Edition 2025\nDate: September 18, 2025\nLocation: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in the Event: Participant\nEvent Overview and Key Activities The AWS Cloud Day Vietnam - AI Edition 2025 served as a pivotal forum designed to accelerate Vietnam\u0026rsquo;s digital transformation, harnessing the power of Cloud Computing and Artificial Intelligence. The event explored four core themes:\nDemocratizing Generative AI for Enterprises Bridging the Gap Between Business and IT in Finance Accelerating Industry Modernization Enhancing Security Frameworks The day’s activities featured high-level plenary sessions with government officials and industry leaders, followed by in-depth technical tracks focused on Data Strategy, DevOps, and Cloud Migration Pathways.\nKey Takeaways and Outcomes Strategic Insights: Gained a deeper understanding of the critical interplay between Generative AI and a robust data strategy, identified as the key driver for success in modern enterprises. Migration to Operate Mindset: Developed an appreciation for the \u0026ldquo;Migrate to Operate\u0026rdquo; framework, which emphasizes using AI to streamline operations and optimize costs post-cloud migration. Technical Knowledge: Acquired insights into the integration of Generative AI within the DevOps lifecycle, particularly in automating code generation and testing. Security Innovations: Learned about the \u0026ldquo;Security by Design\u0026rdquo; approach, which focuses on embedding security measures throughout the application lifecycle rather than relying solely on perimeter defenses. This event provided invaluable knowledge and practical takeaways, further enhancing my understanding of the intersection between AI, cloud computing, and security in the context of modern enterprise solutions.\nEvent 2 Event Name: Discover Agentic AI – Amazon QuickSuite Workshop\nDate: November 7, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City\nRole in the Event: Participant\nEvent Overview and Key Activities The \u0026ldquo;Discover Agentic AI – Amazon QuickSuite Workshop,\u0026rdquo; organized in collaboration with Cloud Kinetics, served as a strategic technical session marking the evolution from passive Generative AI to autonomous Agentic AI. The event featured the first-ever live demonstration of Amazon QuickSuite in Vietnam. The workshop focused on four key pillars: Defining the \u0026ldquo;Agentic\u0026rdquo; paradigm: Autonomy, Reasoning, and Execution. Integrating Data and AI through the Amazon QuickSuite ecosystem. Hands-on building of AI concepts with AWS technical experts. Financial enablement for innovation through the AWS LIFT Program. The agenda combined theoretical architectural sessions with practical, hands-on workshops using Amazon QuickSight and Quick Suite Q, allowing participants to build functional AI concepts in real-time.\nKey Takeaways and Outcomes - Paradigm Shift: Gained a clear understanding of the transition from Generative AI (content creation) to Agentic AI (autonomous task execution), where systems can perceive environments and act independently to solve business problems. Unified Ecosystem: Acquired practical insights into Amazon QuickSuite, learning how to integrate business intelligence (QuickSight) with generative capabilities to create \u0026ldquo;Analyst Agents\u0026rdquo; that streamline operations. Operational Agility: Recognized the strategic value of the \u0026ldquo;Quick\u0026rdquo; framework, which emphasizes rapid deployment and \u0026ldquo;Time-to-Value,\u0026rdquo; allowing enterprises to implement complex AI solutions with speed. Strategic Enablement: Learned about the AWS LIFT Program (offering up to $80,000 USD in credit), identifying it as a critical mechanism to de-risk R\u0026amp;D and accelerate the adoption of high-performance computing. This workshop provided a concrete roadmap for building autonomous enterprise systems, combining theoretical knowledge with hands-on technical skills and strategic financial insights to accelerate digital transformation.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives Keep pace with the team\u0026rsquo;s learning progress on AWS services. Master AWS Transit Gateway setup and configuration. Deepen understanding of Amazon EC2 and related compute services. Learn Git fundamentals for effective team collaboration. Workshop: Begin VPC \u0026amp; Network Setup for the Bandup IELTS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Explore AWS Transit Gateway: concepts, setup process, and required resources.\n- Compare differences between VPC Peering and Transit Gateway.\n- Complete: Centralized Network Management with AWS Transit Gateway. 29/09/2025 30/09/2025 AWS Transit Gateway 3 - Deep dive into Amazon EC2 through Module 3 lectures.\n- Study EC2 Auto Scaling for automated resource management.\n- Complete: Scaling Applications with EC2 Auto Scaling. 01/10/2025 02/10/2025 FCJ Playlist 4 - Learn and practice Git commands (commit, push, pull) for team collaboration.\n- Explore Amazon Lightsail for simplified compute solutions.\n- Complete: Simplified Computing with Amazon Lightsail. 03/10/2025 04/10/2025 Git Tutorial 5 - Propose ideas and assign tasks to team members for project proposal.\n- Study migration strategies for AWS.\n- Complete: VM Migration with AWS VM Import/Export.\n- Workshop Activity: Create VPC with CIDR 10.0.0.0/16 and configure DNS support. 05/10/2025 06/10/2025 Team Meeting, Workshop 5.3 AWS Skill Builder Courses Completed Course Category Status Centralized Network Management with AWS Transit Gateway Networking ✅ Scaling Applications with EC2 Auto Scaling Compute ✅ Simplified Computing with Amazon Lightsail Compute ✅ Container Deployment with Amazon Lightsail Containers Containers ✅ VM Migration with AWS VM Import/Export Migration ✅ Database Migration with AWS DMS and SCT Migration ✅ Disaster Recovery with AWS Elastic Disaster Recovery Reliability ✅ Monitoring with Amazon CloudWatch Operations ✅ Week 4 Achievements Technical Skills Acquired:\nAWS Transit Gateway:\nMastered Transit Gateway setup and configuration Understood key advantages over VPC Peering: Supports complex multi-VPC topologies (hub-and-spoke model) Enables transitive routing between connected networks Simplifies network management at scale Supports VPN and Direct Connect attachments Learned Transit Gateway route table management Amazon EC2 Deep Dive:\nComprehensive understanding of EC2 key features: Elasticity: Scale resources up/down based on demand Flexible configurations: Multiple instance types for various workloads Cost optimization: On-Demand, Reserved, Spot instance pricing models Mastered EC2 Auto Scaling for automated resource adjustment Understood Instance Store as ephemeral block storage for EC2 Explored Amazon Lightsail as a simplified solution for small-scale applications Learned about Lightsail Containers for easy container deployment Migration Services:\nUnderstood AWS Application Migration Service (MGN) for server migration Learned VM Import/Export for virtual machine migration to AWS Explored Database Migration Service (DMS) and Schema Conversion Tool (SCT) Studied disaster recovery strategies with AWS Elastic Disaster Recovery DevOps and Monitoring:\nProficient in Git commands (commit, push, pull) and team workflows Learned CloudWatch fundamentals for monitoring AWS resources Team Collaboration:\nSuccessfully proposed ideas and assigned tasks for project proposal Team is prepared to begin implementation phase Established clear roles and responsibilities for each team member Workshop Progress - VPC \u0026amp; Network Setup:\nCreated VPC with CIDR 10.0.0.0/16 in ap-southeast-1 region Designed subnet architecture: Public subnets (10.0.1.0/24, 10.0.2.0/24) and Private subnets for App (10.0.11.0/24, 10.0.12.0/24) and DB (10.0.21.0/24, 10.0.22.0/24) across two AZs Configured Internet Gateway for public subnet internet access Set up route tables for proper traffic routing Began security group configuration for multi-tier architecture Key Takeaways:\nTransit Gateway is essential for managing complex multi-VPC architectures EC2 Auto Scaling ensures applications can handle variable load efficiently Lightsail is perfect for simple workloads without AWS complexity Migration services provide multiple paths for moving workloads to AWS VPC design with separate public/private subnets provides security isolation Multi-AZ deployment ensures high availability from the network layer "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.5-setup-be/5.5.4-task/","title":"Create Service &amp; Task","tags":[],"description":"","content":"In the final step of the backend deployment, we define the runtime configuration for the Spring Boot application and launch it as a stable ECS Service.\n1. Create Task Definition Navigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Family: bandup-backend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: 1 vCPU and 2 GB Memory. Note: Java applications (Spring Boot) generally require more memory than Node.js apps to handle the JVM heap effectively. Task Role \u0026amp; Execution Role: Select ecsTaskExecutionRole. Container Details:\nName: bandup-be-container. Image URI: Enter the ECR URI (.../band-up-backend:v1.0.0). Container Port: 8080 (Default Spring Boot port). Environment Configuration (Best Practice): Instead of manually entering sensitive variables (Database URL, User, Password) in plain text, we load them from a secure file stored in S3.\nEnvironment files: Add the S3 ARN of your .env file (e.g., arn:aws:s3:::bandup2025-fcj/.env). Requirement: Ensure your ecsTaskExecutionRole has permission to read this S3 object. 2. Create ECS Service Deploy the task definition to the cluster.\nNavigate to bandup-cluster \u0026gt; Services \u0026gt; Create. Deployment configuration: Compute options: FARGATE. Family: bandup-backend (Revision 7 or latest). Service name: bandup-backend-service. Desired tasks: 1. Networking: VPC: band-up-vpc. Subnets: Select Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-backend-sg (Created in step 5.5.2). Load Balancing: Load balancer: Select bandup-public-alb. Listener: Use existing listener 80:HTTP. Target group: Create a new target group target-bandup-be. Container info: Ensure traffic is routed to port 8080. Click Create. The service will provision the Fargate tasks, pull the image, load the environment variables from S3, and register with the ALB. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.4-setup-fe/5.4.4-task/","title":"Create Task Definition &amp; Service","tags":[],"description":"","content":"In this final step for the frontend, we define how our application container should run (Task Definition) and deploy it as a scalable service (ECS Service) connected to our Load Balancer.\n1. Create Task Definition The Task Definition serves as a blueprint for our application.\nNavigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Task definition family: bandup-frontend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: .5 vCPU and 1 GB Memory (Sufficient for our Next.js frontend). Task Role \u0026amp; Task Execution Role: Select ecsTaskExecutionRole (Created in section 5.3.3). Container details: Name: bandup-fe-container. Image URI: Enter the ECR URI we pushed earlier (e.g., .../band-up-frontend:v1.0.0). Container port: 3000. Click Create. 2. Create ECS Service Now we deploy this blueprint into our Cluster.\nNavigate to Clusters \u0026gt; Select bandup-cluster. In the Services tab, click Create. Step 1: Environment\nCompute options: Launch type -\u0026gt; FARGATE. Task definition: bandup-frontend (Revision 1). Service name: bandup-frontend-service. Desired tasks: 1. Step 2: Networking\nVPC: band-up-vpc. Subnets: Select the Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-private-sg (This allows traffic from ALB). Step 3: Load Balancing\nLoad balancer type: Application Load Balancer. Load balancer: Select bandup-public-alb. Container to load balance: bandup-fe-container 3000:3000. Listener: Create a new listener on Port 80 (HTTP). Target group: Use an existing target group -\u0026gt; target-bandup-fe. Click Create. The service will start deploying your container. Wait until the status changes to Active and the Task status is Running. 3. Verify Deployment Once the service is stable, open your web browser and navigate to the DNS name of your Application Load Balancer.\nYou should see the IELTS BandUp landing page loading successfully, served from your container in the private subnet.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.4-setup-fe/","title":"Frontend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Frontend (Next.js application) to the AWS Cloud.\nWe will utilize Amazon Elastic Container Service (ECS) with the Fargate launch type. This serverless approach allows us to run containers without managing the underlying EC2 instances. The frontend service will be placed in Private Subnets for security but will be accessible to users via the Application Load Balancer (ALB) we configured in the previous section.\nImplementation Steps To successfully deploy the frontend, we will follow this workflow:\nContainer Registry (ECR): Create a repository to store our Docker images and push the local application code to AWS. Security Configuration: Define specific security group rules allowing the Frontend container to receive traffic only from the ALB. ECS Task \u0026amp; Service: Define the blueprint (Task Definition) for our container (CPU, Memory, Environment Variables) and launch it as a stable Service. Content Dockerize Application Setup ECR \u0026amp; Push Image Configure Security Group Create Task Definition \u0026amp; Service "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.3-network/5.3.4-endpoints/","title":"VPC Endpoints Setup","tags":[],"description":"","content":"To ensure security, our backend services running in Private Subnets should not access key AWS services over the public internet. Instead, we use AWS PrivateLink (VPC Endpoints) to keep this traffic within the AWS network.\nWe will create 4 Endpoints:\nInterface Endpoints: For ECR (Docker \u0026amp; API) and CloudWatch Logs. Gateway Endpoint: For Amazon S3. 1. Create Interface Endpoints (ECR \u0026amp; CloudWatch) We will start by creating the endpoint for ECR Docker (ecr.dkr). The process is identical for ECR API (ecr.api) and CloudWatch (logs).\nStep 1: Service Selection\nNavigate to VPC Dashboard \u0026gt; Endpoints \u0026gt; Create endpoint. Name tag: ecr-endpoint (for Docker). Service category: Select AWS services. Services: Search for ecr.dkr and select com.amazonaws.ap-southeast-1.ecr.dkr. Step 2: VPC \u0026amp; Subnets\nVPC: Select band-up-vpc. Subnets: Select the Availability Zones and choose the Private Subnets (private-app-subnet-1 and private-app-subnet-2). Note: This creates Elastic Network Interfaces (ENIs) in your private subnets to serve as entry points. Step 3: Security Group\nSecurity groups: Select the Security Group that allows HTTPS (Port 443) traffic from your VPC. For this workshop, you can use the default security group if it allows inbound traffic from within the VPC. Click Create endpoint. Step 4: Repeat for ECR API and CloudWatch Repeat the steps above to create two more Interface Endpoints:\nECR API: Search for ecr.api -\u0026gt; Name: ecr-api-endpoint. CloudWatch Logs: Search for logs -\u0026gt; Name: cloudwatch-endpoint. 2. Create Gateway Endpoint (S3) For Amazon S3, we use a Gateway Endpoint, which is cost-effective and uses routing tables instead of network interfaces.\nClick Create endpoint. Name tag: s3-endpoint. Services: Search for s3 and select com.amazonaws.ap-southeast-1.s3 (Type: Gateway). VPC: Select band-up-vpc. Route tables: Select the Route Tables associated with your Private Subnets. Click Create endpoint. 3. Verify All Endpoints Once completed, navigate to the Endpoints list. You should see 4 active endpoints ensuring secure connectivity for your infrastructure.\necr-endpoint (Interface) ecr-api-endpoint (Interface) cloudwatch-endpoint (Interface) s3-endpoint (Gateway) "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/5.6.4-dynamodb/","title":"DynamoDB","tags":[],"description":"","content":"Overview Create two DynamoDB tables to store Lambda function results:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Table 1: Evaluations Table Stores results from both Writing Evaluator and Speaking Evaluator Lambda functions.\nSetting Value Table name bandup-evaluations Partition key evaluation_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description evaluation_id String Unique session ID (PK) user_id String User identifier (SK) evaluation_type String writing or speaking status String processing, completed, failed overall_band String Overall IELTS band score (e.g., \u0026ldquo;7.0\u0026rdquo;) task_achievement_band String Writing only coherence_band String Writing only lexical_band String Both Writing and Speaking grammar_band String Both Writing and Speaking fluency_band String Speaking only pronunciation_band String Speaking only transcript String Speaking only - transcribed audio feedback String JSON-encoded detailed feedback model_used String AI model used for evaluation created_at Number Unix timestamp Example Item (Writing):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;eval-abc123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;writing\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;task_achievement_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;coherence_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;strengths\\\u0026#34;:[...],\\\u0026#34;weaknesses\\\u0026#34;:[...]}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-writing_task2\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Example Item (Speaking):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;speak-xyz789\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;speaking\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;fluency_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;pronunciation_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;transcript\u0026#34;: \u0026#34;Well, I\u0026#39;d like to talk about...\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;120.5\u0026#34;, \u0026#34;word_count\u0026#34;: 185, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;fluency\\\u0026#34;:{...},\\\u0026#34;pronunciation\\\u0026#34;:{...}}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-2.5-flash-audio\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Table 2: Flashcard Sets Table Stores results from Flashcard Generator Lambda function.\nSetting Value Table name bandup-flashcard-sets Partition key set_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description set_id String Unique flashcard set ID (PK) user_id String User identifier (SK) document_id String Source document S3 key status String processing, completed, failed flashcards String JSON-encoded array of flashcards total_cards Number Number of flashcards generated page_count Number Number of pages in source PDF chunk_count Number Number of text chunks indexed created_at Number Unix timestamp Example Item:\n{ \u0026#34;set_id\u0026#34;: \u0026#34;flashcard-set-123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;document_id\u0026#34;: \u0026#34;uploads/documents/user-456/vocab-guide.pdf\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;flashcards\u0026#34;: \u0026#34;[{\\\u0026#34;question\\\u0026#34;:\\\u0026#34;What is...\\\u0026#34;,\\\u0026#34;answer\\\u0026#34;:\\\u0026#34;...\\\u0026#34;}]\u0026#34;, \u0026#34;total_cards\u0026#34;: 15, \u0026#34;page_count\u0026#34;: 8, \u0026#34;chunk_count\u0026#34;: 24, \u0026#34;created_at\u0026#34;: 1733644800 } Create Tables with AWS CLI # Create evaluations table (Writing + Speaking) aws dynamodb create-table \\ --table-name bandup-evaluations \\ --attribute-definitions \\ AttributeName=evaluation_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=evaluation_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production # Create flashcard sets table aws dynamodb create-table \\ --table-name bandup-flashcard-sets \\ --attribute-definitions \\ AttributeName=set_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=set_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production Enable Point-in-Time Recovery Enable PITR for data protection:\n# Enable PITR for evaluations table aws dynamodb update-continuous-backups \\ --table-name bandup-evaluations \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true # Enable PITR for flashcard sets table aws dynamodb update-continuous-backups \\ --table-name bandup-flashcard-sets \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true Query Patterns Get user\u0026rsquo;s evaluation history:\nresponse = table.query( IndexName=\u0026#39;user_id-created_at-index\u0026#39;, # If GSI exists KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), ScanIndexForward=False, # Most recent first Limit=10 ) Get specific evaluation by ID:\nresponse = table.get_item( Key={ \u0026#39;evaluation_id\u0026#39;: \u0026#39;eval-abc123\u0026#39;, \u0026#39;user_id\u0026#39;: \u0026#39;user-456\u0026#39; } ) Get user\u0026rsquo;s flashcard sets:\nresponse = table.query( KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), FilterExpression=Attr(\u0026#39;status\u0026#39;).eq(\u0026#39;completed\u0026#39;) ) Lambda Environment Variables Configure Lambda functions to use these tables:\nLambda Function Environment Variable Value Writing Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Speaking Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Flashcard Generator DYNAMODB_FLASHCARD_SETS bandup-flashcard-sets IAM Policy for Lambda Access { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-flashcard-sets\u0026#34; ] } ] } Next Steps Proceed to Bedrock Integration to configure Amazon Titan Embeddings.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives Identify and resolve abnormal AWS costs on the account. Design and partition infrastructure architecture for the project. Begin initial project configuration and assign team roles. Explore AWS Skill Builder and advance learning on optimization topics. Workshop: Complete VPC Network Setup and begin Database \u0026amp; Storage Setup. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Analyze and identify causes of abnormal costs on AWS account.\n- Complete: Cost and Usage Management and Managing Quotas with Service Quotas. 07/10/2025 08/10/2025 AWS Cost Explorer 3 - Design and partition project infrastructure architecture.\n- Propose basic architecture templates for team reference.\n- Complete: Building Highly Available Web Applications.\n- Workshop Activity: Configure NAT Gateways in public subnets and complete Security Groups setup. 09/10/2025 10/10/2025 FCJ Community, Workshop 5.3 4 - Build code skeleton and configure initial project files.\n- Set up development environment.\n- Complete: Development Environment with AWS Toolkit for VS Code.\n- Workshop Activity: Begin Database \u0026amp; Storage Setup - plan RDS PostgreSQL and ElastiCache Redis configurations. 11/10/2025 13/10/2025 VS Code + AWS Toolkit, Workshop 5.6 5 - Register for AWS Skill Builder and explore courses.\n- Study EC2 optimization techniques.\n- Complete: Right-Sizing with EC2 Resource Optimization. 11/10/2025 12/10/2025 AWS Skill Builder AWS Skill Builder Courses Completed Course Category Status Cost and Usage Management Cost Optimization ✅ Managing Quotas with Service Quotas Operations ✅ Billing Console Delegation Cost Management ✅ Right-Sizing with EC2 Resource Optimization Cost Optimization ✅ Development Environment with AWS Toolkit for VS Code Development ✅ Building Highly Available Web Applications Architecture ✅ Database Essentials with Amazon RDS Database ✅ NoSQL Database Essentials with Amazon DynamoDB Database ✅ In-Memory Caching with Amazon ElastiCache Database ✅ Command Line Operations with AWS CLI Operations ✅ Week 5 Achievements Technical Skills Acquired:\nCost Optimization:\nIdentified causes of abnormal AWS costs: Incomplete deletion of EC2 resources (EBS volumes, Elastic IPs) Lack of control over user accounts and IAM permissions Resources left running in unused regions Learned AWS cost management best practices: AWS Budgets for proactive cost alerts Cost Explorer for analyzing spending patterns Service Quotas for managing account limits Billing Console Delegation for team cost visibility Proposed cost optimization measures for the team Architecture Design:\nSuccessfully designed project infrastructure architecture Created reference architecture templates for team adoption Applied High Availability principles: Multi-AZ deployments Load balancing strategies Database replication patterns Fault-tolerant design patterns Development Environment:\nSet up AWS Toolkit for VS Code for streamlined development Mastered AWS CLI for command-line operations Built robust code skeleton with initial configuration files Established project foundation for team collaboration Database Services:\nUnderstood Amazon RDS for relational database needs Learned DynamoDB for NoSQL workloads Explored ElastiCache for in-memory caching (Redis/Memcached) Applied database selection criteria based on use cases Project Progress:\nRegistered and activated AWS Skill Builder account Began exploring advanced courses and learning paths Infrastructure architecture finalized and documented Development environment configured and ready for coding Workshop Progress - Network \u0026amp; Database Setup:\nCompleted NAT Gateway deployment in both public subnets for private subnet internet access Configured Security Groups for ALB, ECS, RDS, and ElastiCache tiers with least-privilege access Set up route tables: Public routes to Internet Gateway, Private routes to NAT Gateways Designed RDS PostgreSQL Multi-AZ configuration for high availability Planned ElastiCache Redis cluster for session management and caching Configured S3 buckets for static assets and document storage Key Takeaways:\nCost optimization starts with visibility - use Cost Explorer daily Right-sizing EC2 instances can reduce costs by 30-50% High availability requires planning across multiple AZs AWS Toolkit for VS Code significantly improves developer productivity Database selection depends on data model, scale, and access patterns Service Quotas prevent unexpected capacity limitations NAT Gateways enable private subnet resources to access internet securely Security Groups provide defense-in-depth with multiple layers of protection "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.5-setup-be/","title":"Backend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Backend, a Spring Boot application that serves as the core logic layer of the platform.\nUnlike the Frontend, the Backend requires persistent storage and caching mechanisms to function effectively. Therefore, before launching the application containers on ECS Fargate, we must provision the data infrastructure (PostgreSQL and Redis). The Backend service will reside in Private Subnets, strictly protected by Security Groups, and will communicate with the AI services via AWS SDK.\nImplementation Steps To deploy the fully functional backend system, we will follow this sequence:\nContainer Registry (ECR): Build the Spring Boot application and push the Docker image to a private ECR repository. Relational Database (RDS): Provision an Amazon RDS for PostgreSQL instance to store user data, test results, and content. In-Memory Cache (ElastiCache): Set up an Amazon ElastiCache (Redis) cluster for session management and high-speed data retrieval. ECS Task \u0026amp; Service: Define the backend task configuration (including environment variables for DB connections) and launch the service. Content Setup ECR \u0026amp; Push Image Create PostgreSQL RDS Create ElastiCache (Redis) Create Service \u0026amp; Task "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/","title":"Workshop","tags":[],"description":"","content":"IELTS Self-Learning Web System - AWS Infrastructure Workshop Overview This comprehensive workshop guides you through building a production-ready AWS infrastructure for the IELTS Self-Learning Web System. You will learn how to deploy a highly available, scalable, and secure web application using modern AWS services and best practices.\nThe architecture implements an active-passive Multi-AZ deployment pattern on Amazon ECS, with a serverless AI service layer for intelligent assessment and content generation.\nWhat You Will Build By completing this workshop, you will have deployed:\nComponent AWS Service Purpose Network Layer VPC, Subnets, NAT Gateway Isolated, secure network infrastructure Container Platform ECS Fargate, ECR Serverless container orchestration Load Balancing ALB, Route 53, ACM Traffic distribution and SSL termination Data Layer RDS PostgreSQL, ElastiCache, S3 Relational database, caching, object storage AI Services API Gateway, SQS, Lambda, DynamoDB Serverless AI processing pipeline CI/CD CodePipeline, CodeBuild Automated deployment pipeline Security IAM, Secrets Manager, WAF Identity management and protection Monitoring CloudWatch Logs, Alarms Observability and alerting Architecture Highlights High Availability Design:\nMulti-AZ deployment across two Availability Zones Active-passive failover for ECS services RDS Multi-AZ with automatic failover Application Load Balancer with health checks Serverless AI Architecture:\nAPI Gateway for RESTful AI endpoints SQS for asynchronous message processing Lambda functions for Writing Assessment, Speaking Assessment, and RAG-based Flashcard Generation DynamoDB for storing AI results Amazon Bedrock integration for AI models (Gemma 3 12B, Titan Embeddings) Google Gemini API for smart query generation Security Best Practices:\nPrivate subnets for application and database tiers Security groups with least-privilege access AWS WAF for application-level protection Secrets Manager for credential management IAM roles with minimal required permissions Prerequisites Before starting this workshop, ensure you have:\nAn AWS account with appropriate permissions AWS CLI installed and configured Basic understanding of AWS services (VPC, EC2, ECS) Docker installed locally for container builds Git for version control Time to Complete Section Estimated Time Prerequisites 15 minutes VPC \u0026amp; Network Setup 30 minutes ECS \u0026amp; Container Setup 45 minutes Load Balancer Configuration 30 minutes Database \u0026amp; Storage Setup 45 minutes AI Service Architecture 60 minutes CI/CD Pipeline 30 minutes Security \u0026amp; IAM 30 minutes Monitoring Setup 20 minutes Total ~5 hours Content Workshop Overview Prerequisites VPC \u0026amp; Network Setup ECS \u0026amp; Container Setup Load Balancer Configuration Database \u0026amp; Storage Setup AI Service Architecture CI/CD Pipeline Security \u0026amp; IAM Monitoring \u0026amp; Logging Clean Up "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/5.6.5-bedrock-integration/","title":"Bedrock Integration","tags":[],"description":"","content":"Overview Configure Amazon Bedrock for AI model access including Gemma 3 12B and Titan Embeddings.\nEnable Model Access Navigate to Amazon Bedrock → Model access Request access to: Amazon Titan Text Express (for assessments) Amazon Titan Embeddings V2 (for RAG) Meta Llama 3 or Anthropic Claude (optional) Test Bedrock API import boto3 import json bedrock = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # Test Titan Text response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-text-express-v1\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Hello, how are you?\u0026#39;, \u0026#39;textGenerationConfig\u0026#39;: { \u0026#39;maxTokenCount\u0026#39;: 100, \u0026#39;temperature\u0026#39;: 0.7 } }) ) print(json.loads(response[\u0026#39;body\u0026#39;].read())) Titan Embeddings for RAG # Generate embeddings response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-embed-text-v2:0\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Document chunk text here\u0026#39; }) ) embedding = json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embedding\u0026#39;] # Store in OpenSearch or use for similarity search Google Gemini Integration For smart query generation:\nimport google.generativeai as genai genai.configure(api_key=os.environ[\u0026#39;GEMINI_API_KEY\u0026#39;]) model = genai.GenerativeModel(\u0026#39;gemini-2.5-flash\u0026#39;) response = model.generate_content( f\u0026#34;\u0026#34;\u0026#34;Analyze this document and generate 10 intelligent questions: {document_text} \u0026#34;\u0026#34;\u0026#34; ) Cost Optimization Use Titan Text Express for assessments (lower cost) Batch embeddings generation where possible Implement caching for repeated queries Use Google Gemini free tier for query generation Next Steps Proceed to CI/CD Pipeline.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives Master fundamental AWS storage services and their use cases. Enhance Python programming skills through practical exercises. Design and refine the project\u0026rsquo;s infrastructure architecture. Attend the \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar to explore DevSecOps practices and Amazon Q Developer. Workshop: Complete Database \u0026amp; Storage Setup and configure S3 for static assets. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study Amazon S3 fundamentals: Bucket architecture, durability guarantees, and static website hosting capabilities.\n- Explore S3 Storage Classes (Standard, Standard-IA) and Amazon Glacier for cold storage solutions.\n- Complete: Static Website Hosting with Amazon S3. 14/10/2025 15/10/2025 AWS S3 Documentation 3 - Learn AWS Storage Gateway types (File, Volume, Tape Gateway) and their integration patterns.\n- Understand Object Lifecycle Management policies for cost optimization.\n- Practice Python fundamentals: data structures, functions, and error handling.\n- Attend webinar: \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; featuring Hoàng Kha. 16/10/2025 17/10/2025 AWS Storage Gateway\nAWS Events 4 - Study disaster recovery concepts: RTO, RPO, and Backup \u0026amp; Restore strategies.\n- Explore AWS Backup service for centralized backup management.\n- Hands-on: Create S3 buckets, upload files, configure static website hosting, and test lifecycle policies.\n- Research DevSecOps methodologies: CI/CD pipelines, SAST/DAST tools, Infrastructure as Code.\n- Workshop Activity: Deploy RDS PostgreSQL Multi-AZ instance and ElastiCache Redis cluster. 18/10/2025 19/10/2025 AWS Backup, Workshop 5.6 5 - Finalize project infrastructure architecture diagram with detailed component relationships.\n- Restructure code skeleton to align with the updated architecture design.\n- Standardize programming language and framework selection for team consistency.\n- Explore Amazon Q Developer capabilities: AI-powered code generation, testing, and vulnerability scanning. 20/10/2025 21/10/2025 Amazon Q Developer AWS Skill Builder Courses Completed Course Category Status Static Website Hosting with Amazon S3 Storage ✅ Data Protection with AWS Backup Reliability ✅ Content Delivery with Amazon CloudFront Networking ✅ Week 6 Achievements Storage Services Mastery:\nComprehensive understanding of Amazon S3 architecture: Buckets, durability (99.999999999%), and static website hosting Mastered S3 Storage Classes: Standard, Standard-IA, Glacier for different access patterns Learned AWS Storage Gateway integration patterns for hybrid cloud storage Understood Object Lifecycle Management for automated data tiering and cost optimization Disaster Recovery \u0026amp; Backup:\nGrasped disaster recovery fundamentals: RTO (Recovery Time Objective) and RPO (Recovery Point Objective) Learned AWS Backup service for centralized backup management across services Understood Backup \u0026amp; Restore strategies for business continuity Development Skills:\nEnhanced Python programming through practical exercises Successfully created S3 buckets, configured static websites, and tested lifecycle policies Improved understanding of data structures and error handling Project Planning:\nFinalized comprehensive infrastructure architecture diagram Restructured code skeleton with proper directory structure Standardized technology stack for team collaboration DevSecOps Insights:\nAttended \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar (October 16, 2025) Learned DevSecOps integration: Security in SDLC using Jenkins (CI/CD), SonarQube (SAST), OWASP ZAP (DAST), Terraform (IaC) Explored Amazon Q Developer: AI assistant for code generation, testing, vulnerability scanning, and AWS optimization Workshop Progress - Database \u0026amp; Storage:\nDeployed RDS PostgreSQL Multi-AZ instance in private DB subnets for high availability Configured ElastiCache Redis cluster for session management and application caching Set up S3 buckets for static website assets, user uploads, and document storage Configured S3 lifecycle policies for cost optimization (transitioning to Glacier) Established database connection strings and caching endpoints for application integration Implemented database backup strategies using AWS Backup service Key Takeaways:\nS3 is the foundation for object storage in AWS - understanding storage classes is crucial for cost optimization Lifecycle policies automate data management and reduce storage costs significantly AWS Backup provides unified backup management across multiple AWS services DevSecOps integrates security throughout the development lifecycle, not as an afterthought RDS Multi-AZ provides automatic failover for database high availability ElastiCache significantly improves application performance through in-memory caching Private subnets for databases provide additional security layer "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Amazon Web Services] from [8/9/2025] to [28/11/2025], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ☐ ✅ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.6-ai-service/","title":"AI Service Architecture","tags":[],"description":"","content":"Overview This section covers the serverless AI service architecture using API Gateway, SQS, Lambda, DynamoDB, and Amazon Bedrock for intelligent assessment and content generation.\nAI Service Architecture The AI service implements a fully serverless pattern:\nUser → API Gateway → SQS Queue → Lambda → AI Model → DynamoDB → Response Three Lambda Functions:\nFunction Purpose AI Model Writing Evaluate IELTS writing assessment Gemma 3 12B / Gemini Speaking Evaluate Audio transcription + assessment Transcribe + Gemma 3 12B Flashcard Generate RAG-based flashcard creation Titan Embeddings + Gemini Content API Gateway SQS Queues Lambda Functions DynamoDB Bedrock Integration Request Flow User submits request (writing sample, audio, document) API Gateway validates and enqueues message to SQS SQS triggers appropriate Lambda function Lambda processes with AI model (Bedrock/Gemini) Results stored in DynamoDB User retrieves results via API Estimated Time: ~60 minutes Cost Estimate Monthly Cost Summary:\nUpfront Cost Monthly Cost Total 12 Months Cost Currency $0.00 $23.61 $283.32 USD Note: Includes upfront cost\nDetailed Cost Breakdown:\nService Description Region Monthly Cost (USD) Annual Cost (USD) AWS Lambda Writing Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Speaking Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Evaluation Status Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda S3 Upload Asia Pacific (Singapore) $0.00 $0.00 Amazon API Gateway HTTP API Asia Pacific (Singapore) $0.42 $5.04 S3 Standard Audio bucket Asia Pacific (Singapore) $0.51 $6.12 S3 Standard Documents Bucket Asia Pacific (Singapore) $0.53 $6.36 DynamoDB Evaluations Table Asia Pacific (Singapore) $0.37 $4.44 DynamoDB Flashcard Sets Table Asia Pacific (Singapore) $0.52 $6.24 Amazon SQS Writing/Speaking/Flashcard queues Asia Pacific (Singapore) $0.00 $0.00 AWS Secrets Manager Secrets management Asia Pacific (Singapore) $0.45 $5.40 Amazon CloudWatch RAG Lambda logs Asia Pacific (Singapore) $0.01 $0.08 Amazon Bedrock Bedrock inference US East (N. Virginia) $0.50 $6.00 OpenAI GPT inference US East (N. Virginia) $20.30 $243.65 Total $23.61 $283.32 AWS Pricing Calculator provides only an estimate of your AWS fees and doesn\u0026rsquo;t include any taxes that might apply. Your actual fees depend on a variety of factors, including your actual usage of AWS services.\nView detailed cost breakdown: AWS Pricing Calculator\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives Focus on comprehensive review and knowledge consolidation in preparation for the mid-term exam. Practice hands-on labs and multiple-choice questions on AWS Builders and AWSboy platforms to familiarize with exam format. Systematize fundamental AWS services learned: EC2, S3, VPC, IAM, RDS, Lambda, DynamoDB. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Systematize knowledge of Compute services (EC2, Lambda). - Hands-on: Complete exercises/labs on creating, configuring, and managing the lifecycle of EC2 Instances. - Review Lambda function creation, triggers, and execution models. 22/10/2024 22/10/2024 AWS Builders, AWSboy Tuesday - Review knowledge of Storage services (S3, EBS, EFS). - Hands-on: Complete exercises on S3 storage classes (Standard, IA, Glacier), EBS volume types, and EFS use cases. - Practice S3 bucket policies and access control. 23/10/2024 23/10/2024 AWS Builders, AWSboy Wednesday - Consolidate knowledge of Networking (VPC, Subnets, Route Tables, Internet Gateway, Security Groups, NACLs). - Hands-on: Practice questions on VPC configuration, Security Group rules vs NACL rules, and routing principles. - Review VPC Peering and Transit Gateway concepts. 24/10/2024 24/10/2024 AWS Builders, AWSboy Thursday - Review Database services (RDS, DynamoDB) and Security/Identity (IAM). - Hands-on: Focus on fundamental IAM Policies, IAM Roles, and IAM Users concepts. - Practice DynamoDB table design and RDS instance configuration. 25/10/2024 25/10/2024 AWS Builders, AWSboy Friday - Summary and Mock Exams: Take comprehensive practice tests on AWS Builders and AWSboy platforms. - Review weak areas identified during practice tests for further study. - Create summary notes for quick reference before exam. 26/10/2024 26/10/2024 AWS Builders, AWSboy Week 7 Achievements Successfully completed comprehensive review of core AWS service groups: Compute, Storage, Networking, Database, Security (IAM). Successfully practiced numerous labs and multiple-choice questions on AWS Builders and AWSboy platforms. Mastered fundamental parameters of EC2 (Instance Types, AMI, EBS volumes) and S3 operations (Storage Classes, Object/Bucket management). Clearly understood relationships and configuration of components within a VPC (Public/Private Subnets, Routing, Security Groups vs NACLs). Gained confidence in knowledge acquired, ready for the upcoming mid-term exam. Created comprehensive study notes covering all major AWS service categories. Identified and addressed knowledge gaps through targeted practice sessions. Key Takeaways:\nSecurity Groups are stateful (return traffic automatically allowed), NACLs are stateless (bidirectional rules required) EC2 instance types are optimized for different workloads (compute, memory, storage, GPU) S3 storage classes balance cost vs. access frequency requirements IAM policies follow explicit deny principle - most restrictive policy wins VPC routing follows most specific route match principle "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.7-cicd-pipeline/","title":"CI/CD with CodeBuild &amp; CodePipeline","tags":[],"description":"","content":"CI/CD Pipeline with AWS CodeBuild \u0026amp; CodePipeline This guideline describes how to implement a production-ready CI/CD pipeline with AWS CodePipeline and CodeBuild using GitLab as SCM. When a new Release is created in the GitLab repository, CodePipeline is triggered, CodeBuild runs the frontend and backend projects using the existing frontend-buildspec.yml and backend-buildspec.yml, and then CodePipeline deploys to ECS.\nWhat you’ll do 5.3.1 – Configure CodeBuild projects (frontend/backend) and trigger on GitLab Release 5.3.2 – Design CodePipeline for ECS deploy and integrate post-build artifacts Prerequisites An IAM user/role with permissions for CodeBuild, CodePipeline, S3, ECR (if needed for GitLab token), and IAM pass role. An S3 bucket for pipeline artifacts (will be created by CodePipeline wizard or you can pre-create). ECR repository created. Architecture Overview "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives Complete the mid-term exam (October 31st) with strong performance. Begin implementing foundational CRUD (Create, Read, Update, Delete) functionalities for the Bandup IELTS project. Research and plan integration of AWS Serverless services (Lambda, API Gateway, DynamoDB) for the project architecture. Set up development environment and establish project structure. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Final comprehensive review of knowledge in preparation for the mid-term exam. - Review challenging questions and commonly confused concepts (IAM Policies vs Roles, Security Groups vs NACLs, VPC routing). - Practice time management for exam completion. 28/10/2024 28/10/2024 Personal notes, AWS Builders Tuesday - Mental preparation and tool setup for the exam. - Hands-on: Begin setting up development environment for the Bandup IELTS project. - Install and configure Python development tools, AWS CLI, and IDE setup. 29/10/2024 30/10/2024 AWS CLI Documentation Wednesday - Mid-term Exam (October 31st) - Completion of the most important objective. - Post-exam reflection on performance and areas for improvement. 31/10/2024 31/10/2024 Exam Venue Thursday - Begin implementing first basic CRUD functionalities (Create operation: generating flashcard sets). - Research and trial deployment of AWS Lambda functions for serverless compute. - Study DynamoDB table design for storing flashcard data. 01/11/2024 01/11/2024 AWS Lambda \u0026amp; DynamoDB documentation Friday - Plan Serverless architecture integration: + Research API Gateway for RESTful API endpoints. + Design data flow: Frontend → API Gateway → Lambda → DynamoDB. + Define Lambda function structure and event handling patterns. - Implement basic Read functionality to retrieve flashcard sets from DynamoDB. 02/11/2024 02/11/2024 API Gateway documentation, Serverless patterns Week 8 Achievements Completed the mid-term exam (October 31st) successfully. Successfully set up basic development environment for the project with Python, AWS CLI, and IDE configuration. Started building initial Create/Read functionalities for the Bandup IELTS project using AWS Lambda and DynamoDB. Researched and designed serverless architecture pattern: API Gateway for HTTP endpoints Lambda functions for business logic DynamoDB for NoSQL data storage Reinforced knowledge of essential Serverless services (Lambda, DynamoDB, API Gateway) critical for project development. Created initial project structure with proper directory organization. Implemented first Lambda function handler for Create operation. Key Takeaways:\nServerless architecture eliminates server management overhead Lambda functions are event-driven and scale automatically DynamoDB provides single-digit millisecond latency for NoSQL workloads API Gateway acts as the entry point for serverless APIs Proper project structure from the start simplifies future development "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/5-workshop/5.8-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Overview This section guides you through cleaning up all AWS resources created during this workshop to avoid ongoing charges. Follow the steps in order as some resources depend on others.\nImportant: Deleting resources is irreversible. Make sure you have backed up any data you need before proceeding.\nEstimated Time: ~30 minutes Step 1: Delete CI/CD Pipeline Resources First, delete the CI/CD pipeline to stop any automated deployments.\nNavigate to AWS CodePipeline console Select your pipeline (e.g., ielts-pipeline) Click Delete pipeline → Confirm deletion Navigate to AWS CodeBuild console Delete all build projects associated with the workshop Delete any S3 buckets used for pipeline artifacts Step 2: Delete ECS Services and Cluster Stop and delete all ECS services before deleting the cluster.\nNavigate to Amazon ECS console Select your cluster (e.g., ielts-cluster) Go to Services tab For each service: Select the service Click Update → Set Desired tasks to 0 → Update Wait for running tasks to stop Click Delete service → Confirm After all services are deleted, go back to Clusters Select your cluster → Delete cluster → Confirm Step 3: Delete ECR Repositories Delete container images and repositories.\nNavigate to Amazon ECR console Select each repository: ielts-frontend ielts-backend Click Delete → Type the repository name to confirm Step 4: Delete AI Service Resources Delete serverless AI components in this order:\n4.1 Delete Lambda Functions Navigate to AWS Lambda console Delete each function: writing-evaluator speaking-evaluator flashcard-generator evaluation-status s3-upload Select function → Actions → Delete → Confirm 4.2 Delete API Gateway Navigate to Amazon API Gateway console Select your API (e.g., ielts-ai-api) Click Actions → Delete API → Confirm 4.3 Delete SQS Queues Navigate to Amazon SQS console Delete each queue: writing-evaluation-queue writing-evaluation-dlq speaking-evaluation-queue speaking-evaluation-dlq flashcard-generation-queue flashcard-generation-dlq Select queue → Delete → Confirm 4.4 Delete DynamoDB Tables Navigate to Amazon DynamoDB console Delete each table: evaluations flashcard-sets Select table → Delete → Confirm deletion Step 5: Delete Load Balancer Resources Navigate to EC2 console → Load Balancers Select your ALB (e.g., ielts-alb) Click Actions → Delete → Confirm Go to Target Groups Delete all target groups associated with the workshop Go to Listeners and delete any remaining listeners Step 6: Delete Database Resources 6.1 Delete RDS Instance Navigate to Amazon RDS console Select your database instance Click Actions → Delete Uncheck Create final snapshot (if not needed) Check I acknowledge\u0026hellip; → Delete RDS deletion may take 5-10 minutes to complete.\n6.2 Delete ElastiCache (Redis) Navigate to Amazon ElastiCache console Select your Redis cluster Click Delete → Confirm 6.3 Delete RDS Subnet Group In RDS console, go to Subnet groups Select your subnet group → Delete Step 7: Delete S3 Buckets Navigate to Amazon S3 console For each bucket created in the workshop: ielts-audio-bucket ielts-documents-bucket Any pipeline artifact buckets Select bucket → Empty → Confirm After emptying, select bucket → Delete → Confirm S3 buckets must be emptied before they can be deleted.\nStep 8: Delete Secrets Manager Secrets Navigate to AWS Secrets Manager console Select each secret created for the workshop Click Actions → Delete secret Set recovery window to 7 days (minimum) or choose immediate deletion Confirm deletion Step 9: Delete CloudWatch Resources Navigate to Amazon CloudWatch console Go to Log groups Delete log groups: /aws/lambda/writing-evaluator /aws/lambda/speaking-evaluator /aws/lambda/flashcard-generator /ecs/ielts-frontend /ecs/ielts-backend Go to Alarms and delete any created alarms Step 10: Delete VPC and Network Resources Delete network resources in this specific order:\n10.1 Delete NAT Gateway Navigate to VPC console → NAT Gateways Select your NAT Gateway Click Actions → Delete NAT gateway → Confirm Wait for status to change to Deleted 10.2 Release Elastic IPs Go to Elastic IPs Select any Elastic IPs associated with NAT Gateway Click Actions → Release Elastic IP addresses → Confirm 10.3 Delete VPC Endpoints Go to Endpoints Select all VPC endpoints created for the workshop Click Actions → Delete VPC endpoints → Confirm 10.4 Delete Security Groups Go to Security Groups Delete security groups in this order (due to dependencies): Application security groups first Database security groups Load balancer security groups Do not delete the default security group 10.5 Delete Subnets Go to Subnets Select all subnets in your workshop VPC Click Actions → Delete subnet → Confirm 10.6 Delete Route Tables Go to Route Tables Delete custom route tables (not the main route table) Select route table → Actions → Delete route table 10.7 Delete Internet Gateway Go to Internet Gateways Select your IGW → Actions → Detach from VPC → Confirm Select IGW again → Actions → Delete internet gateway → Confirm 10.8 Delete VPC Go to Your VPCs Select your workshop VPC Click Actions → Delete VPC → Confirm Step 11: Delete IAM Resources Navigate to IAM console Go to Roles and delete: ecsTaskExecutionRole (if created for this workshop) ielts-lambda-execution-role Any other workshop-specific roles Go to Policies and delete custom policies created for the workshop Be careful not to delete IAM resources used by other applications.\nVerification Checklist After completing the cleanup, verify all resources are deleted:\nResource Service Console Status CodePipeline CodePipeline ☐ Deleted ECS Cluster ECS ☐ Deleted ECR Repositories ECR ☐ Deleted Lambda Functions Lambda ☐ Deleted API Gateway API Gateway ☐ Deleted SQS Queues SQS ☐ Deleted DynamoDB Tables DynamoDB ☐ Deleted Load Balancer EC2 ☐ Deleted RDS Instance RDS ☐ Deleted ElastiCache ElastiCache ☐ Deleted S3 Buckets S3 ☐ Deleted Secrets Secrets Manager ☐ Deleted CloudWatch Logs CloudWatch ☐ Deleted NAT Gateway VPC ☐ Deleted VPC VPC ☐ Deleted IAM Roles IAM ☐ Deleted Cost Verification To ensure no unexpected charges:\nGo to AWS Billing console Check Bills for the current month Review Cost Explorer to verify no active resources Set up a Budget alert if you plan to continue using AWS Wait 24-48 hours and check your billing dashboard again to confirm all resources have been cleaned up and no charges are accruing.\n"},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives Complete transition to AWS SAM (Serverless Application Model) development framework. Refactor and re-implement CRUD functionalities following SAM architecture patterns. Resolve environment-related issues to achieve successful deployment status on AWS. Integrate Docker for standardized build environment and dependency management. Workshop: ECS \u0026amp; Container Setup - Deploy containerized Frontend and Backend services. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - In-depth research on AWS SAM: Understand template.yaml structure, SAM CLI commands, and how Serverless resources (Lambda, API Gateway) operate within SAM model. - Plan detailed migration strategy: Convert existing Lambda functions to SAM-compatible structure. - Study SAM local testing capabilities (sam local invoke, sam local start-api). 04/11/2024 04/11/2024 AWS SAM Documentation, AWS Study Group Tuesday - Source Code Refactoring: Rewrite CRUD functionalities (Create/Read operations) using SAM patterns (Lambda handlers and API Gateway events). - Docker Integration: Install and configure Docker to ensure consistent Python runtime environment for sam build process. - Create Dockerfile for Lambda layer dependencies. - Workshop Activity: Create ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images. 05/11/2024 06/11/2024 Docker Documentation, SAM CLI, Workshop 5.4 Wednesday - Local Debugging and Testing: Execute sam local invoke to test individual Lambda functions. - Encountered critical issues in Local environment: Dependency conflicts, Python version mismatches, DynamoDB local connection problems. - Attempt to resolve local testing barriers through configuration adjustments. 06/11/2024 07/11/2024 SAM CLI Error Reports, Stack Overflow Thursday - Strategic Decision: Backend Team decided to adopt deploy-then-test strategy on actual AWS environment to overcome local debugging limitations, accepting calculated risk. - Focus on fixing configuration errors in template.yaml (resource definitions, IAM permissions, environment variables). - Validate SAM template syntax and resource dependencies. 07/11/2024 08/11/2024 CloudFormation Template Validator Friday - Successful Deployment: Executed sam deploy --guided and successfully deployed project to AWS environment. - Basic Verification: Tested created API endpoints using Postman/curl, confirming CRUD functionality is operational. - Document deployment process and configuration for team reference. - Workshop Activity: Build and push Docker images to ECR, create ECS Task Definitions and deploy ECS Services with Fargate. 08/11/2024 08/11/2024 AWS CloudFormation Deployment Logs, Workshop 5.4 Week 9 Achievements Completed technology transition to AWS SAM development model for entire project. Successfully refactored CRUD functionalities into SAM Serverless structure with proper handler organization. Resolved environment issues by using Docker to ensure sam build process uses correct Python version and dependencies. Achieved critical milestone: Successfully deployed project to AWS environment, overcoming local debugging hurdles. The Bandup IELTS project now has working API version on real Cloud environment (though deeper testing still required). Established deployment workflow and best practices for team collaboration. Created comprehensive template.yaml with proper resource definitions and IAM permissions. Workshop Progress - ECS \u0026amp; Container Setup:\nCreated ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images Built and pushed Docker images to ECR with proper tagging strategy Created ECS Task Definitions with CPU/memory specifications (Frontend: 512 CPU/1024MB, Backend: 1024 CPU/2048MB) Set up ECS Cluster with Fargate capacity providers Deployed ECS Services in active-passive Multi-AZ pattern (2 replicas active, 1 standby) Configured Service Connect for internal service discovery between Frontend and Backend Implemented health checks for automatic task recovery Key Takeaways:\nSAM simplifies serverless application development with infrastructure as code Docker ensures consistent build environments across different development machines Deploy-then-test strategy can be viable when local testing is problematic SAM templates provide single source of truth for serverless infrastructure Proper IAM permissions in SAM templates are critical for Lambda function execution ECS Fargate eliminates server management overhead for containerized applications Multi-AZ deployment ensures high availability for container services Service Connect simplifies internal service communication without load balancers "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives Stabilize AWS SAM/Serverless deployment environment and resolve critical issues. Focus on debugging core problems: CORS configuration, template validation errors, and API response formatting. Integrate Frontend/Backend to enable end-to-end testing on user interface. Complete basic Read and Delete functionalities with proper error handling. Participate in AWS Cloud Mastery Series event to receive expert guidance and address project challenges. Workshop: Load Balancer Configuration - Set up Application Load Balancer for traffic distribution. Tasks Completed This Week Day Task Start Date Completion Date Resources Mon - Debug CORS: Analyze CORS configuration in API Gateway (CORS headers, preflight OPTIONS requests) and Lambda response headers to allow Frontend access. - Fix template validation errors: Review and optimize template.yaml file to prevent deployment loop errors and resource dependency issues during sam deploy. 11/11/2024 11/11/2024 API Gateway/CORS Documentation Tue - Strengthen Read function (Retrieving flashcard sets): Ensure data is queried from DynamoDB correctly and returned in proper JSON format for Frontend consumption. - Implement error handling for missing records and invalid queries. - Add logging for debugging purposes. 12/11/2024 12/11/2024 DynamoDB Query Documentation Wed - Frontend Integration: Begin combining Frontend codebase with project and test deployed API endpoints. - Successfully display flashcard sets list on user interface. - Test API connectivity and data rendering in React/Vue components. - Workshop Activity: Create Application Load Balancer (ALB) in public subnets and configure target groups for ECS services. 13/11/2024 13/11/2024 Frontend Framework Documentation, Workshop 5.5 Thu - Deploy and test Delete function (Removing flashcard sets). - Encountered Error: Identified authorization issue with Cognito User Sub ID when executing Delete function - Lambda unable to extract/process Sub ID from Cognito token correctly. - Begin troubleshooting authentication flow. 14/11/2024 14/11/2024 AWS Cognito Documentation Fri - Participation in AWS Cloud Mastery Series: + Received expert guidance and clarified questions regarding Serverless architecture, Lambda best practices, and authentication patterns. - Analyze Update/Delete errors: Apply mentor guidance to resolve authorization issues and Cognito token parsing problems. - Document solutions for future reference. 15/11/2024 15/11/2024 Mentor, AWS Cloud Mastery Series Week 10 Achievements Successfully fixed CORS error and stabilized SAM deployment process (mitigated template validation errors). Participated in AWS Cloud Mastery Series event and gathered essential information to solve major project blockers. Completed Frontend and Backend integration, achieving first functional user interface for end-to-end testing. Successfully deployed Read (Retrieving flashcard sets) and Delete (Removing flashcard sets) functionalities, operational on web interface. Identified and gained direction to solve critical bottlenecks: Authorization error: Lambda fails to retrieve/incorrectly process Cognito Sub ID from JWT token, affecting privileged operations Update function dependency: Requires proper authentication flow and token validation The project has transitioned to basic user testing phase with working CRUD operations. Established debugging workflow and error handling patterns for team. Workshop Progress - Load Balancer Configuration:\nCreated Application Load Balancer (ALB) in public subnets across two AZs Configured target groups for Frontend (port 3000) and Backend (port 8080) ECS services Set up health checks for automatic unhealthy target removal Configured SSL/TLS termination using ACM certificates Integrated ALB with Route 53 for DNS routing Implemented listener rules for path-based routing (Frontend: /, Backend: /api/*) Configured security groups to allow ALB traffic to ECS tasks Key Takeaways:\nCORS requires proper configuration in both API Gateway and Lambda response headers Cognito JWT tokens must be properly decoded to extract user identity (Sub ID) Frontend-Backend integration requires careful attention to API contracts and data formats Error handling and logging are essential for debugging production issues AWS Cloud Mastery Series provides valuable real-world insights from experienced practitioners ALB provides intelligent traffic distribution and automatic failover Health checks ensure only healthy targets receive traffic SSL/TLS termination at ALB reduces computational load on backend services "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Participate in AWS Cloud Mastery Series #2 to continue resolving specialized technical issues. Refactor and standardize the Frontend structure for improved stability and maintainability. Implement a Multi-Stack architecture to optimize deployment speed and Serverless project management. Integrate basic CRUD functionalities with AI Image Processing (using Rekognition) into the website. Completely resolve deployment errors (especially CORS issues) to stabilize the system. Workshop: AI Service Architecture, Security \u0026amp; IAM, Monitoring - Complete serverless AI pipeline setup. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Sun - Participate in AWS Cloud Mastery Series #2 (Nov 17th): Continue receiving guidance and addressing deeper technical questions about authorization errors and the AI workflow. 17/11/2024 17/11/2024 Mentor, AWS Cloud Mastery Series Mon - Frontend Structure Unification and Refactor: Hold team meeting to standardize the Frontend code structure for maintainability. - Research Multi-Stack Solution: Begin analyzing how to split the template.yaml file into smaller Stacks (Multi-Stack) to optimize the sam deploy process. 18/11/2024 18/11/2024 Serverless Architecture Docs Tue - Implement Multi-Stack Architecture: Start splitting and configuring separate Stacks (e.g., API Backend Stack, Frontend Hosting Stack). - Proceed with AI Image Processing Integration: Combine basic CRUD functions with image processing logic (e.g., calling Rekognition API/S3 trigger) in preparation for the Update function. - Workshop Activity: Set up API Gateway REST endpoints and SQS queues for asynchronous AI processing. 19/11/2024 19/11/2024 Backend Codebase, AWS Rekognition, Workshop 5.7 Wed - Error Encountered after AI Integration: The system faced errors after combining AI functionality, necessitating a full Stack deletion and redeployment. - Leader Develops Backup Stack: The team leader created a separate, optimized Multi-Stack as a contingency and reference for future optimal deployments. 20/11/2024 20/11/2024 Leader\u0026rsquo;s Backup Stack Thu - Persistent CORS Error: After redeploying, the CORS issue re-emerged. - In-depth CORS Debugging: Spent time thoroughly analyzing the root cause and permanently fixing the CORS error, ensuring correct header configuration on both API Gateway and Lambda. - Workshop Activity: Configure IAM roles and policies for Lambda functions, set up Secrets Manager for API keys, and implement WAF rules. 21/11/2024 21/11/2024 API Gateway/Lambda Configuration, Workshop 5.9 Fri - Team Meeting and Project Stabilization: Held a team meeting to review the new Frontend structure, stabilize the main project Stack, and synchronize the fixes for CORS and basic template errors. - Optimization for Maintenance: Finalized the solution to use a separate stack (developed by the leader) for flexibility and easier optimization in future development. 22/11/2024 22/11/2024 New Structure Report Week 11 Achievements: Participated in AWS Cloud Mastery Series #2, gaining deeper knowledge of Serverless, Rekognition, and solutions for authorization errors. Successfully refactored the Frontend and standardized the overall project structure, improving maintainability. Implemented the Multi-Stack architecture (or at least established a reliable solution/backup stack), which speeds up deployment and simplifies resource management. Completely resolved the persistent CORS error after identifying the root cause, ensuring stable communication between Frontend and Backend. Acquired knowledge on fixing basic Template errors and gained a clearer understanding of AWS SAM deployment issues. Developed a separate stack for backup/optimization, enhancing project flexibility and safety during future major updates. The project has moved into the AI functionality testing phase, and although errors were encountered, a clear path for troubleshooting has been established. Workshop Progress - AI Services, Security \u0026amp; Monitoring:\nConfigured API Gateway REST API with three endpoints: /writing/evaluate, /speaking/evaluate, /flashcard/generate Set up SQS queues for asynchronous message processing (writing-queue, speaking-queue, flashcard-queue) Deployed Lambda functions: writing_evaluator, speaking_evaluator, rag_flashcard with proper IAM roles Configured DynamoDB tables: bandup-evaluations and bandup-flashcard-sets for storing AI results Integrated Amazon Bedrock (Titan Embeddings V2) and Google Gemini API for AI processing Set up AWS Secrets Manager for secure API key storage Configured IAM roles with least-privilege permissions for Lambda functions Implemented AWS WAF rules for application-level protection Set up CloudWatch Logs and Alarms for monitoring Lambda execution and errors Configured CloudWatch Insights for log analysis and troubleshooting "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Complete 100% of the basic CRUD functionalities and AI image processing (including the Update function). Upgrade the image processing architecture by integrating SQS for asynchronous processing and clear flow segmentation. Finalize essential missing project features: Security, Map Pinning, and SNS. Complete the main interfaces of the Frontend, prepare for domain name purchase, and attend the final AWS Cloud Mastery Series event. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Mon - Finalize Update Function and AI: Resolve remaining bugs (Sub ID, Rekognition) to ensure CRUD and image processing are fully functional. 25/11/2024 25/11/2024 Mentor Guidance, Backend Codebase Tue - Upgrade AI Flow with SQS: Implement AWS SQS to create an asynchronous image processing queue, enhancing flow segmentation and performance under high load. - Define Clear Processing Flows: Redefine the data flow (Upload -\u0026gt; S3 -\u0026gt; SQS -\u0026gt; Lambda (AI) -\u0026gt; DynamoDB). 26/11/2024 26/11/2024 AWS SQS Documentation, Lambda Architecture Wed - Frontend Interface Finalization: Complete interfaces for the main pages (Homepage, Article Detail, Personal Management Page). - Implement Map Pinning: Integrate Map Pinning functionality for posts, utilizing geo data in DynamoDB or an appropriate map service. 27/11/2024 27/11/2024 Frontend Codebase, DynamoDB Geo Thu - Finalize Security (Authorization): Optimize authentication and permissions (IAM Policy/Cognito), especially accurate Sub retrieval for user operations. - Implement SNS: Integrate AWS SNS for basic notification features (e.g., notification when a post is successfully processed/uploaded). 28/11/2024 28/11/2024 AWS SNS, Cognito/IAM Documentation Fri - Attend the Final AWS Cloud Mastery Series Event: Receive overall project guidance, review and finalize missing parts (domain, security, SNS) before the demo. - Domain Name: Conduct research and prepare for purchasing the website domain, configuring basic DNS (Route 53) as necessary (based on mentor guidance). 29/11/2024 29/11/2024 Mentor, AWS Cloud Mastery Series, Route 53 Week 12 Achievements: Completed 100% of basic CRUD functionalities and AI image processing, ensuring system stability. Upgraded the image processing architecture by integrating SQS and defining clear asynchronous processing flows, improving performance and reliability. Finalized essential features: Implemented Security, Map Pinning, and SNS notifications. The basic Frontend interface is complete, ready for presentation. Successfully participated in the final AWS Cloud Mastery Series event, receiving comprehensive guidance for project completion. Research for domain name purchase has been conducted and planned. The project has reached Demo Readiness status. "},{"uri":"https://hoangworthy.github.io/AWS-Worklog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://hoangworthy.github.io/AWS-Worklog/tags/","title":"Tags","tags":[],"description":"","content":""}]